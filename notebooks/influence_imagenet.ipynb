{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence functions for Computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores the use of influences functions for convolutional neural networks. In the first part we will fine-tune Resnet18 on the [tiny-imagenet dataset](https://huggingface.co/datasets/Maysee/tiny-imagenet). This dataset was first created for the [Stanford Deep Learning for Computer Vision](http://cs231n.stanford.edu/) course, and it contains a subset of the [famous ImageNet dataset](https://image-net.org/challenges/LSVRC/2012/index), (200 classes vs 1000, and images are downsampled to a lower-resolution, from 64x64 pixels to 256x256). \n",
    "\n",
    "After training the last layers of the network, we will use pyDVL to find the most and least influential points on the evaluation images. This can be used e.g. to explain errors in the inference of new images or to direct efforts for collecting new data. In the last part of the notebook we will also see that influence functions are an effective tool for finding anomalous or corrupted data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to know more about the mathematical foundations of influence functions for neural networks, you can find a primer in the appendix to this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now proceed with the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from .notebook_support import (\n",
    "    load_model,\n",
    "    save_model,\n",
    "    load_results,\n",
    "    save_results,\n",
    "    plot_sample_images,\n",
    "    plot_top_bottom_if_images,\n",
    "    plot_train_val_loss,\n",
    "    get_corrupted_imagenet,\n",
    "    plot_influence_distribution,\n",
    ")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from pydvl.utils.dataset import load_preprocess_imagenet\n",
    "from pydvl.influence.model_wrappers import TorchModel\n",
    "from pydvl.influence.general import compute_influences\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"xtick.labelsize\"] = 12\n",
    "plt.rcParams[\"ytick.labelsize\"] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "is_CI = os.environ.get(\"CI\")\n",
    "\n",
    "run_model_trainings = True\n",
    "calculate_influences = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is loaded through the load_preprocess_imagenet method. We will load the images related to the classes 90 and 100. This is an arbitrary choice and any other class would have worked. You can try selecting any other set of numbers, even more than just two (you could even select all 200 classes, though this will require longer training times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_keep = [90, 100]\n",
    "train_ds, val_ds, test_ds = load_preprocess_imagenet(\n",
    "    train_size=0.8,\n",
    "    test_size=0.1,\n",
    "    keep_labels=labels_to_keep,\n",
    "    is_CI=is_CI,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded the data, let's take a look at a sample of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_images(train_ds, labels_to_keep, n_images_per_class=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first class is related to dining tables, the second to boats and to Venice! Let's now further pre-process the data and prepare for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_label_to_model_label = {ds_label: idx for idx, ds_label in enumerate(labels_to_keep)}\n",
    "model_label_to_ds_label = {idx: ds_label for idx, ds_label in enumerate(labels_to_keep)}\n",
    "\n",
    "\n",
    "def process_io(x, y):\n",
    "    x_nn = torch.stack(x.tolist())\n",
    "    y_nn = [ds_label_to_model_label[yi] for yi in y]\n",
    "    return x_nn, y_nn\n",
    "\n",
    "\n",
    "train_x, train_y = process_io(train_ds[\"normalized_images\"], train_ds[\"labels\"])\n",
    "val_x, val_y = process_io(val_ds[\"normalized_images\"], val_ds[\"labels\"])\n",
    "test_x, test_y = process_io(test_ds[\"normalized_images\"], test_ds[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will proceed with the initialization of the model and of some helper methods for training and evaluation. \n",
    "\n",
    "The model is defined by loading resnet18 and then switching the last few layers so that we can do binary classification on our selected classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(output_size):\n",
    "    model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Finetune final few layers\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, output_size)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "model_ft = initialize_model(output_size=len(labels_to_keep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is done through some pytorch convenience wrappers (TorchModel) which are part of pyDVL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs, training_data, lr=0.001):\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    process_io(training_data[\"normalized_images\"], training_data[\"labels\"])\n",
    "    train_x, train_y = process_io(\n",
    "        training_data[\"normalized_images\"], training_data[\"labels\"]\n",
    "    )\n",
    "\n",
    "    train_loss, val_loss = TorchModel(model=model).fit(\n",
    "        x_train=train_x,\n",
    "        y_train=train_y,\n",
    "        x_val=val_x,\n",
    "        y_val=val_y,\n",
    "        loss=ce_loss,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=1000,\n",
    "    )\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and influence computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the model for 50 epochs and save the results. Then we will plot the train and validation loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_model_trainings:\n",
    "    num_epochs = 50\n",
    "    train_loss, val_loss = train_model(\n",
    "        model_ft, num_epochs=num_epochs, training_data=train_ds\n",
    "    )\n",
    "    save_model(model_ft, train_loss, val_loss, model_name=\"model_ft\")\n",
    "else:\n",
    "    train_loss, val_loss = load_model(model_ft, model_name=\"model_ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_loss(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix and f1 score are good, especially considering the low resolution of the images and their complexity (large diversity of objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_test = np.argmax(model_ft(test_x).detach(), axis=1)\n",
    "\n",
    "cm = confusion_matrix(test_y, pred_y_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_to_keep)\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(test_y, pred_y_test, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's calculate influences. The method compute_influences will take the trained model, a loss (which typically is the training loss, but not necessarily), some input dataset with labels (which typically are the training data, or a subset of it) and some test data (which in this case will be the validation set). \n",
    "\n",
    "Other important parameters are the hessian regularization term, which should be chosen as small as possible for the computation to converge. Details on why this is important can be found in the pyDVL documentation or in the [original paper](https://arxiv.org/pdf/1703.04730.pdf). \n",
    "\n",
    "Since the Resnet18 is quite big, the inversion methods that should be preferred is conjugate gradient (\"cg\"). The direct method would require a lot of memory. Finally, the influence type will be \"up\" (the other option, \"perturbation\", is beyond the scope of this notebook, but more info can be found in the influence_wine notebook or on the pyDVL documentation).\n",
    "\n",
    "The output of calculate_influences is a matrix of size (validation_set_length, training_set_length). Each row represents a validation data-point, and each column a training data-point. Each entry (i,j) represents the influence of training point j on the validation point i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_influences:\n",
    "    influences = compute_influences(\n",
    "        model=model_ft,\n",
    "        loss=ce_loss,\n",
    "        x=train_x,\n",
    "        y=train_y,\n",
    "        x_test=val_x,\n",
    "        y_test=val_y,\n",
    "        hessian_regularization=1e-3,\n",
    "        inversion_method=\"cg\",\n",
    "        influence_type=\"up\",\n",
    "    )\n",
    "    save_results(influences, file_name=\"influences.pkl\")\n",
    "else:\n",
    "    influences = load_results(file_name=\"influences.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the influence on validation images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an image in the validation set. Among the images in the training set, we will take those that have the same label and visualize those that have the highest and lowest influence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_image_idx = 118\n",
    "plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "plt.imshow(val_ds[\"images\"][val_image_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Predicted label:\",\n",
    "    model_label_to_ds_label[\n",
    "        np.argmax(model_ft(val_x[val_image_idx].unsqueeze(0)).detach(), axis=1).item()\n",
    "    ],\n",
    ")\n",
    "print(\"Real label:\", val_ds[\"labels\"][val_image_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "for label in labels_to_keep:\n",
    "    plt.hist(\n",
    "        influences[val_image_idx][train_ds[\"labels\"] == label], label=label, alpha=0.7\n",
    "    )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_with_same_label = train_ds[\"labels\"] == val_ds[\"labels\"][val_image_idx]\n",
    "if_same_label = influences[val_image_idx][images_with_same_label]\n",
    "imges_same_label = train_ds[\"images\"][images_with_same_label].values\n",
    "plot_top_bottom_if_images(if_same_label, subset_images=imges_same_label, num_to_plot=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_influences = np.mean(influences, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "for label in labels_to_keep:\n",
    "    plt.hist(avg_influences[train_ds[\"labels\"] == label], label=label, alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 90\n",
    "img_with_selected_label = train_ds[\"labels\"] == label\n",
    "if_selected_label = avg_influences[img_with_selected_label]\n",
    "imges_same_label = train_ds[\"images\"][img_with_selected_label].values\n",
    "plot_top_bottom_if_images(if_selected_label, imges_same_label, num_to_plot=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the influence of corrupted training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_corrupted = initialize_model(output_size=len(labels_to_keep))\n",
    "corrupted_dataset, corrupted_indices = get_corrupted_imagenet(\n",
    "    dataset=train_ds,\n",
    "    labels_to_keep=labels_to_keep,\n",
    "    fraction_to_corrupt=0.1,\n",
    "    avg_influences=avg_influences,\n",
    ")\n",
    "\n",
    "if run_model_trainings:\n",
    "    num_epochs = 50\n",
    "    train_loss, val_loss = train_model(\n",
    "        model_corrupted,\n",
    "        num_epochs=num_epochs,\n",
    "        training_data=corrupted_dataset,\n",
    "    )\n",
    "    save_model(model_corrupted, train_loss, val_loss, model_name=\"model_corrupted\")\n",
    "else:\n",
    "    train_loss, val_loss = load_model(model_corrupted, model_name=\"model_corrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_loss(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_test = np.argmax(model_corrupted(test_x).detach(), axis=1)\n",
    "model_score = f1_score(test_y, pred_y_test, average=\"weighted\")\n",
    "print(model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_influences:\n",
    "    corrupted_train_x, corrupted_train_y = process_io(\n",
    "        corrupted_dataset[\"normalized_images\"],\n",
    "        corrupted_dataset[\"labels\"],\n",
    "    )\n",
    "    influences = compute_influences(\n",
    "        model=model_corrupted,\n",
    "        loss=ce_loss,\n",
    "        x=corrupted_train_x,\n",
    "        y=corrupted_train_y,\n",
    "        x_test=val_x,\n",
    "        y_test=val_y,\n",
    "        hessian_regularization=1e-3,\n",
    "        inversion_method=\"cg\",\n",
    "        influence_type=\"up\",\n",
    "    )\n",
    "    save_results(influences, file_name=\"influences_corrupted.pkl\")\n",
    "else:\n",
    "    influences = load_results(file_name=\"influences_corrupted.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_corrupted_influences = np.mean(influences, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 100\n",
    "img_with_selected_label = corrupted_dataset[\"labels\"] == label\n",
    "if_selected_label = avg_corrupted_influences[img_with_selected_label]\n",
    "imges_same_label = corrupted_dataset[\"images\"][img_with_selected_label].values\n",
    "plot_top_bottom_if_images(if_selected_label, imges_same_label, num_to_plot=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_label_influence = plot_influence_distribution(\n",
    "    corrupted_dataset, labels_to_keep, corrupted_indices, avg_corrupted_influences\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_label_influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Theory of Influence functions for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this appendix we will briefly go through the essential formulas that lay the foundations of influnce functions for neural networks. A more in-depth and expanded analysis can be found on the original paper: [\"Understanding Black-box Predictions via Influence Functions\"](https://arxiv.org/pdf/1703.04730.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upweighting points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by considering some input space $\\mathcal{X}$ to a model (e.g. images) and an output space $\\mathcal{Y}$ (e.g. labels). Let's take $z_i = (x_i, y_i)$ to be the $i$-th training point, and $\\theta$ to be the (potentially highly) multi-dimensional parameters of the neural network (i.e. $\\theta$ is a big array with very many parameters). We will indicate with $L(z, \\theta)$ the loss of the model for point $z$ and parameters $\\theta$. When training the model, we want to minimize some sort of \"empirical risk\". The optimal parameters are calculated through minimization (i.e. typically through gradient descent) of the following formula:\n",
    "$$\n",
    "\\hat{\\theta} = \\arg \\min_\\theta \\frac{1}{n}\\sum_{i=1}^n L(z_i, \\theta)\n",
    "$$\n",
    "where $n$ is the total number of training data-points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's thus define\n",
    " $$\n",
    "\\hat{\\theta}_{-z} = \\arg \\min_\\theta \\frac{1}{n}\\sum_{z_i \\ne z} L(z_i, \\theta) \\ ,\n",
    "$$\n",
    "i.e. $\\hat{\\theta}_{-z}$ are the model parameters that minimize the total loss when $z$ is not in the training dataset. \n",
    "\n",
    "In order to check the impact of each training point on the model, we would need to calculate $\\hat{\\theta}_{-z}$ for each $z$ in the training dataset, thus re-training the model ~$n$ times. This is computationally very expensive, especially for big neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To circumvent this problem, we will calculate a first order approximation of $\\hat{\\theta}$, which can be computed without re-training the full model. \n",
    "\n",
    "Let's define\n",
    "$$\n",
    "\\hat{\\theta}_{\\epsilon, z} = \\arg \\min_\\theta \\frac{1}{n}\\sum_{i=1}^n L(z_i, \\theta) + \\epsilon L(z_i, \\theta) \\ ,\n",
    "$$\n",
    "which is the optimal $\\hat{\\theta}$ if we were to up-weigh $z$ by an amount $\\epsilon$. \n",
    "\n",
    "From a classical result (details at *Cook, R. D. and Weisberg, S. [Residuals and influence in\n",
    "regression](https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.4710270110). New York: Chapman and Hall, 1982*), we know that:\n",
    "$$\n",
    "\\frac{d \\ \\hat{\\theta}_{\\epsilon, z}}{d \\epsilon} = -H_{\\hat{\\theta}}^{-1} \\nabla_\\theta L(z, \\hat{\\theta})\n",
    "$$\n",
    "where $H_{\\hat{\\theta}} = \\frac{1}{n} \\sum_{i=1}^n \\nabla_\\theta^2 L(z_i, \\hat{\\theta})$ is the Hessian of $L$. Importantly, notice that this expression is only valid when $\\hat{\\theta}$ is a minimum of $L$, or otherwise $H_{\\hat{\\theta}}$ cannot be inverted!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximating points' influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the influence of training point $z$ on test point $z_{\\text{test}}$ as $\\mathcal{I}(z, z_{\\text{test}}) =  L(z_{\\text{test}}, \\hat{\\theta}_{-z}) - L(z_{\\text{test}}, \\hat{\\theta})$, which is higher for points which positively impact the model score (i.e. if they are excluded, the loss is higher). In practice, however, we will always use the infinitesimal approximation $\\mathcal{I}_{up}(z, z_{\\text{test}})$, defined as\n",
    "$$\n",
    " \\mathcal{I}_{up}(z, z_{\\text{test}}) = - \\frac{d L(z_{\\text{test}}, \\hat{\\theta}_{\\epsilon, z})}{d \\epsilon} \\Big|_{\\epsilon=0}\n",
    "$$\n",
    "\n",
    "Using the chain rule and the results calculated above, we thus have:\n",
    "$$\n",
    " \\mathcal{I}_{up}(z, z_{\\text{test}}) = - \\nabla_\\theta L(z_{\\text{test}}, \\hat{\\theta})^\\top \\ \\frac{d \\hat{\\theta}_{\\epsilon, z}}{d \\epsilon} \\Big|_{\\epsilon=0} = \\nabla_\\theta L(z_{\\text{test}}, \\hat{\\theta})^\\top \\ H_{\\hat{\\theta}}^{-1} \\ \\nabla_\\theta L(z, \\hat{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate this expression we need the gradient and the Hessian of the loss wrt. the model parameters $\\hat{\\theta}$. This can be easily done through a single backpropagation pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularizing the Hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very important assumption that we make when approximating influence is that $\\hat{\\theta}$ is a (at least local) minimum of the loss. However, when dealing with neural networks' training, many factors, such as the noise in SGD or a non-small enough learning rate, may interfere with us reaching the actual minimum. In this scenario, calculating and inverting the Hessian may become infeasible (the computation diverges and returns random values).\n",
    "\n",
    "To prevent this from happening, instead of inverting the true Hessian $H_{\\hat{\\theta}}$, in our computation with invert $H_{\\hat{\\theta}} + \\lambda \\mathbb{I}$, with $\\mathbb{I}$ the identity matrix with same shape as $H$. Therefore, the regularization parameter $\\lambda$ should be chosen to be as small as possible, but big enough so that the inversion of $H_{\\hat{\\theta}} + \\lambda \\mathbb{I}$ is stable. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dval_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3369ace3ad477f5e763d9fa7767e0177027059e92a8b1ded9e92b707c0b1513"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence functions in Computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from pydvl.utils.dataset import load_preprocess_imagenet\n",
    "from pydvl.influence.model_wrappers import TorchModel\n",
    "from pydvl.influence.general import compute_influences\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (16, 8)\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "plt.rcParams[\"xtick.labelsize\"] = 12\n",
    "plt.rcParams[\"ytick.labelsize\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from cloudpickle import pickle as pkl\n",
    "\n",
    "imgnet_model_data_path = Path().resolve().parent / \"data/imgnet_model\"\n",
    "\n",
    "\n",
    "def save_model(model, train_loss, val_loss, model_name):\n",
    "    torch.save(model.state_dict(), imgnet_model_data_path / f\"{model_name}_weights.pth\")\n",
    "    with open(\n",
    "        imgnet_model_data_path / f\"{model_name}_train_val_loss.pkl\", \"wb\"\n",
    "    ) as file:\n",
    "        pkl.dump([train_loss, val_loss], file)\n",
    "\n",
    "\n",
    "def load_model(model, model_name):\n",
    "    model.load_state_dict(\n",
    "        torch.load(imgnet_model_data_path / f\"{model_name}_weights.pth\")\n",
    "    )\n",
    "    with open(\n",
    "        imgnet_model_data_path / f\"{model_name}_train_val_loss.pkl\", \"rb\"\n",
    "    ) as file:\n",
    "        train_loss, val_loss = pkl.load(file)\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def save_results(results, file_name):\n",
    "    with open(imgnet_model_data_path / f\"{file_name}\", \"wb\") as file:\n",
    "        pkl.dump(results, file)\n",
    "\n",
    "\n",
    "def load_results(file_name):\n",
    "    with open(imgnet_model_data_path / f\"{file_name}\", \"rb\") as file:\n",
    "        results = pkl.load(file)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_keep = np.random.choice(list(range(200)), 2)\n",
    "# labels_to_keep = [90, 100, 110]\n",
    "train_ds, val_ds, test_ds = load_preprocess_imagenet(\n",
    "    train_size=0.8, test_size=0.1, keep_labels=labels_to_keep\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images_per_class = 4\n",
    "fig, axes = plt.subplots(nrows=n_images_per_class, ncols=len(labels_to_keep))\n",
    "fig.suptitle(\"Examples of training images\")\n",
    "for class_idx, class_label in enumerate(labels_to_keep):\n",
    "    for img_idx, (_, img_data) in enumerate(\n",
    "        train_ds[train_ds[\"labels\"] == class_label].iterrows()\n",
    "    ):\n",
    "        axes[img_idx, class_idx].imshow(img_data[\"images\"])\n",
    "        axes[img_idx, class_idx].axis(\"off\")\n",
    "        axes[img_idx, class_idx].set_title(f\"img label: {class_label}\")\n",
    "        if img_idx + 1 >= n_images_per_class:\n",
    "            break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_model(output_size):\n",
    "    model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Finetune Final few layers to adjust for tiny imagenet input\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, output_size)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "model_ft = initialise_model(output_size=len(labels_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_label_to_model_label = {ds_label: idx for idx, ds_label in enumerate(labels_to_keep)}\n",
    "model_label_to_ds_label = {idx: ds_label for idx, ds_label in enumerate(labels_to_keep)}\n",
    "\n",
    "\n",
    "def get_model_io(x, y):\n",
    "    x_nn = torch.stack(x.tolist())\n",
    "    y_nn = [ds_label_to_model_label[yi] for yi in y]\n",
    "    return x_nn, y_nn\n",
    "\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train_x, train_y = get_model_io(train_ds[\"normalized_images\"], train_ds[\"labels\"])\n",
    "val_x, val_y = get_model_io(val_ds[\"normalized_images\"], val_ds[\"labels\"])\n",
    "test_x, test_y = get_model_io(test_ds[\"normalized_images\"], test_ds[\"labels\"])\n",
    "\n",
    "\n",
    "def get_f1_score_on_test_set(model):\n",
    "    pred_y_test = np.argmax(model(test_x).detach(), axis=1)\n",
    "    return f1_score(test_y, pred_y_test, average=\"weighted\")\n",
    "\n",
    "\n",
    "def plot_train_val_loss(train_loss, val_loss):\n",
    "    plt.rcParams[\"figure.figsize\"] = (10, 8)\n",
    "    plt.plot(train_loss, label=\"Train\")\n",
    "    plt.plot(val_loss, label=\"Val\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_model(model, num_epochs, training_data, lr=0.001):\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    get_model_io(training_data[\"normalized_images\"], training_data[\"labels\"])\n",
    "    train_x, train_y = get_model_io(\n",
    "        training_data[\"normalized_images\"], training_data[\"labels\"]\n",
    "    )\n",
    "\n",
    "    train_loss, val_loss = TorchModel(model=model).fit(\n",
    "        x_train=train_x,\n",
    "        y_train=train_y,\n",
    "        x_val=val_x,\n",
    "        y_val=val_y,\n",
    "        loss=ce_loss,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=1000,\n",
    "    )\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model_training = True\n",
    "\n",
    "if run_model_training:\n",
    "    num_epochs = 30\n",
    "    train_loss, val_loss = train_model(\n",
    "        model_ft, num_epochs=num_epochs, training_data=train_ds\n",
    "    )\n",
    "    save_model(model_ft, train_loss, val_loss, model_name=\"model_ft\")\n",
    "else:\n",
    "    train_loss, val_loss = load_model(model_ft, model_name=\"model_ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_loss(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_test = np.argmax(model_ft(test_x).detach(), axis=1)\n",
    "\n",
    "cm = confusion_matrix(test_y, pred_y_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_to_keep)\n",
    "disp.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(test_y, pred_y_test, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_influences = True\n",
    "\n",
    "if calculate_influences:\n",
    "    influences = compute_influences(\n",
    "        model=model_ft,\n",
    "        loss=ce_loss,\n",
    "        x=train_x,\n",
    "        y=train_y,\n",
    "        x_test=val_x,\n",
    "        y_test=val_y,\n",
    "        hessian_regularization=1e-3,\n",
    "        inversion_method=\"cg\",\n",
    "        influence_type=\"up\",\n",
    "    )\n",
    "    save_results(influences, file_name=\"influences.pkl\")\n",
    "else:\n",
    "    influences = load_results(file_name=\"influences.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_image_idx = 2\n",
    "plt.rcParams[\"figure.figsize\"] = (5, 5)\n",
    "plt.imshow(val_ds[\"images\"][val_image_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Predicted label:\",\n",
    "    model_label_to_ds_label[\n",
    "        np.argmax(model_ft(val_x[val_image_idx].unsqueeze(0)).detach(), axis=1).item()\n",
    "    ],\n",
    ")\n",
    "print(\"Real label:\", val_ds[\"labels\"][val_image_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_bottom_if_images(\n",
    "    subset_influences, subset_images, num_to_plot, figsize=(8, 8)\n",
    "):\n",
    "    top_if_idxs = np.argsort(subset_influences)[-num_to_plot:]\n",
    "    bottom_if_idxs = np.argsort(subset_influences)[:num_to_plot]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=num_to_plot, ncols=2)\n",
    "    plt.rcParams[\"figure.figsize\"] = figsize\n",
    "    fig.suptitle(\"Botton (left) and top (right) influences\")\n",
    "\n",
    "    for plt_idx, img_idx in enumerate(bottom_if_idxs):\n",
    "        axes[plt_idx, 0].set_title(f\"img influence: {subset_influences[img_idx]:0f}\")\n",
    "        axes[plt_idx, 0].imshow(subset_images[img_idx])\n",
    "        axes[plt_idx, 0].axis(\"off\")\n",
    "\n",
    "    for plt_idx, img_idx in enumerate(top_if_idxs):\n",
    "        axes[plt_idx, 1].set_title(f\"img influence: {subset_influences[img_idx]:0f}\")\n",
    "        axes[plt_idx, 1].imshow(subset_images[img_idx])\n",
    "        axes[plt_idx, 1].axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "for label in labels_to_keep:\n",
    "    plt.hist(influences[val_image_idx][train_ds[\"labels\"] == label], label=label)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_with_same_label = train_ds[\"labels\"] == val_ds[\"labels\"][val_image_idx]\n",
    "if_same_label = influences[val_image_idx][images_with_same_label]\n",
    "imges_same_label = train_ds[\"images\"][images_with_same_label].values\n",
    "plot_top_bottom_if_images(if_same_label, subset_images=imges_same_label, num_to_plot=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_influences = np.mean(influences, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8, 8)\n",
    "for label in labels_to_keep:\n",
    "    plt.hist(avg_influences[train_ds[\"labels\"] == label], label=label)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 90\n",
    "img_with_selected_label = train_ds[\"labels\"] == label\n",
    "if_selected_label = avg_influences[img_with_selected_label]\n",
    "imges_same_label = train_ds[\"images\"][img_with_selected_label].values\n",
    "plot_top_bottom_if_images(if_selected_label, imges_same_label, num_to_plot=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_PIL(arr):\n",
    "    return Image.fromarray(np.uint8(arr))\n",
    "\n",
    "\n",
    "def get_corrupted_dataset(dataset, fraction_to_corrupt, avg_influences):\n",
    "    indices_to_corrupt = []\n",
    "    corrupted_dataset = deepcopy(dataset)\n",
    "    corrupted_indices = {l: [] for l in labels_to_keep}\n",
    "\n",
    "    avg_influences_series = pd.DataFrame()\n",
    "    avg_influences_series[\"avg_influences\"] = avg_influences\n",
    "    avg_influences_series[\"labels\"] = dataset[\"labels\"]\n",
    "\n",
    "    for label in labels_to_keep:\n",
    "        class_data = avg_influences_series[avg_influences_series[\"labels\"] == label]\n",
    "        num_corrupt = int(fraction_to_corrupt * len(class_data))\n",
    "        indices_to_corrupt = class_data.nlargest(\n",
    "            num_corrupt, \"avg_influences\"\n",
    "        ).index.tolist()\n",
    "        wrong_labels = [l for l in labels_to_keep if l != label]\n",
    "        for img_idx in indices_to_corrupt:\n",
    "            sample_label = np.random.choice(wrong_labels)\n",
    "            corrupted_dataset.at[img_idx, \"labels\"] = sample_label\n",
    "            corrupted_indices[sample_label].append(img_idx)\n",
    "    return corrupted_dataset, corrupted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model_training = True\n",
    "model_corrupted = initialise_model(output_size=len(labels_to_keep))\n",
    "corrupted_dataset, corrupted_indices = get_corrupted_dataset(\n",
    "    train_ds, 0.1, avg_influences\n",
    ")\n",
    "\n",
    "if run_model_training:\n",
    "    num_epochs = 30\n",
    "    train_loss, val_loss = train_model(\n",
    "        model_corrupted,\n",
    "        num_epochs=num_epochs,\n",
    "        training_data=corrupted_dataset,\n",
    "        lr=0.001,\n",
    "    )\n",
    "    save_model(model_corrupted, train_loss, val_loss, model_name=\"model_corrupted\")\n",
    "else:\n",
    "    train_loss, val_loss = load_model(model_corrupted, model_name=\"model_corrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_val_loss(train_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score = get_f1_score_on_test_set(model_corrupted)\n",
    "print(model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_influences = True\n",
    "\n",
    "if calculate_influences:\n",
    "    corrupted_train_x, corrupted_train_y = get_model_io(\n",
    "        corrupted_dataset[\"normalized_images\"], corrupted_dataset[\"labels\"]\n",
    "    )\n",
    "    influences = compute_influences(\n",
    "        model=model_corrupted,\n",
    "        loss=ce_loss,\n",
    "        x=corrupted_train_x,\n",
    "        y=corrupted_train_y,\n",
    "        x_test=val_x,\n",
    "        y_test=val_y,\n",
    "        hessian_regularization=1e-3,\n",
    "        inversion_method=\"cg\",\n",
    "        influence_type=\"up\",\n",
    "    )\n",
    "    save_results(influences, file_name=\"influences_corrupted.pkl\")\n",
    "else:\n",
    "    influences = load_results(file_name=\"influences_corrupted.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 100\n",
    "avg_corrupted_influences = np.mean(influences, axis=0)\n",
    "img_with_selected_label = corrupted_dataset[\"labels\"] == label\n",
    "if_selected_label = avg_corrupted_influences[img_with_selected_label]\n",
    "imges_same_label = corrupted_dataset[\"images\"][img_with_selected_label].values\n",
    "plot_top_bottom_if_images(if_selected_label, imges_same_label, num_to_plot=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in labels_to_keep:\n",
    "    avg_influences_series = pd.Series(avg_corrupted_influences)\n",
    "    class_influences = avg_influences_series[corrupted_dataset[\"labels\"] == label]\n",
    "    corrupted_infl = class_influences[\n",
    "        class_influences.index.isin(corrupted_indices[label])\n",
    "    ]\n",
    "    non_corrupted_infl = class_influences[\n",
    "        ~class_influences.index.isin(corrupted_indices[label])\n",
    "    ]\n",
    "    plt.hist(non_corrupted_infl, label=\"non corrupted data\", density=True, alpha=0.7)\n",
    "    plt.hist(corrupted_infl, label=\"corrupted data\", density=True, alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\n",
    "        f\"Average influence of corrupted points for {label=}: \",\n",
    "        np.mean(corrupted_infl),\n",
    "    )\n",
    "    print(\n",
    "        f\"Average influence of other points for {label=}: \",\n",
    "        np.mean(non_corrupted_infl),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dval_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b3369ace3ad477f5e763d9fa7767e0177027059e92a8b1ded9e92b707c0b1513"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

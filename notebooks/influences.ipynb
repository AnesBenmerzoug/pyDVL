{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Influence functions for data mis-labelling\n",
    "\n",
    "Both data mis-labelling and outlier detection target the same problem, they are operating on discrete and continuous values, respectively. A influence function $I(x_1, x_2) \\colon \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R} $ measures the influence of the data point $x_1$ onto $x_2$ conditioned on the training targets $y_1$ and $y_2$. As long as the loss function $L(x, y)$ is differentiable (or can be approximated by a surrogate objective). Imagine a simple classification problem, where $y_i \\in \\{1, \\dots, K\\}$, the goal is now to find labels which are mislabelled. In our case we further simplify the problem to set $K=2$.\n",
    "\n",
    "## Artificial data generation\n",
    "\n",
    "First we generate a K-class, M-dimensional dataset using a Gaussian mixture model (GMM). This can be done by sampling $N$ data points from a discrete distribution with $K$ different values. For each of these different discrete states a mean is specified and the observation $x$ is sampled accordingly."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from valuation.utils import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "N = 1000\n",
    "K = 2\n",
    "M = 2\n",
    "sigma = 0.2\n",
    "\n",
    "gaussian_mean = np.asarray([\n",
    "    [0.0, 0.0],\n",
    "    [1.0, 1.0]\n",
    "])\n",
    "gaussian_cov = sigma * np.eye(M)\n",
    "gaussian_chol = np.linalg.cholesky(gaussian_cov)\n",
    "y = np.random.randint(K, size=N)\n",
    "x = np.einsum('ij,kj->ki', gaussian_chol, np.random.normal(size=[N, M])) + gaussian_mean[y]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.70)\n",
    "dataset = Dataset(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inspecting the dataset\n",
    "\n",
    "It is always helpful to visualize the dataset, in the 2-dimensional case it is rather straight forward and we just choose one color for each class. Note that both the train and test set are plotted separately."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datasets = {\n",
    "    'train': (dataset.x_train, dataset.y_train),\n",
    "    'test': (dataset.x_test, dataset.y_test)\n",
    "}\n",
    "num_datasets = len(datasets)\n",
    "fig, ax = plt.subplots(1, num_datasets, figsize=(12, 4))\n",
    "\n",
    "for i, dataset_name in enumerate(datasets.keys()):\n",
    "    x, y = datasets[dataset_name]\n",
    "    ax[i].set_title(dataset_name)\n",
    "\n",
    "    for v in np.unique(y):\n",
    "        idx = np.argwhere(y == v)\n",
    "        ax[i].scatter(x[idx, 0], x[idx, 1], label=str(v))\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As one can see the two classes slighlty overlap and we expect sampples from this region to have higher influence on the remaining region. We are going to verify this in the next section."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculating linear influences using logistic regression\n",
    "\n",
    "For calculating the influences his section we utilize a logistic regression model to identify wrongly labelled data samples. As a first step one needs to define a model, fit it to the data and then insert it in the influence function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from valuation.models.pytorch_model import PyTorchSupervisedModel, PyTorchOptimizer\n",
    "from valuation.influence.general import influences\n",
    "from valuation.influence.types import InfluenceTypes\n",
    "from valuation.models.binary_logistic_regression import BinaryLogisticRegressionTorchModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "objective = F.binary_cross_entropy\n",
    "model = PyTorchSupervisedModel(\n",
    "    model=BinaryLogisticRegressionTorchModel(M),\n",
    "    objective=F.binary_cross_entropy,\n",
    "    num_epochs=1000,\n",
    "    batch_size=128,\n",
    "    optimizer=PyTorchOptimizer.ADAM_W,\n",
    "    optimizer_kwargs={\"lr\": 0.01, \"weight_decay\": 0.005},\n",
    ")\n",
    "model.fit(\n",
    "    dataset.x_train,\n",
    "    dataset.y_train\n",
    ")\n",
    "\n",
    "train_influences = influences(\n",
    "    model,\n",
    "    dataset.x_train,\n",
    "    dataset.y_train,\n",
    "    dataset.x_test,\n",
    "    dataset.y_test,\n",
    "    influence_type=InfluenceTypes.Up\n",
    ")\n",
    "test_influences = influences(\n",
    "    model,\n",
    "    dataset.x_test,\n",
    "    dataset.y_test,\n",
    "    influence_type=InfluenceTypes.Up\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plotting the mean influences of the training samples\n",
    "\n",
    "Next we are going to calculate the mean absolute influences of the training samples and plot them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_influences = lambda arr: np.mean(np.abs(arr), axis=0)\n",
    "mean_train_influences = mean_influences(train_influences)\n",
    "mean_test_influences = mean_influences(test_influences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "fig, ax = plt.subplots(1, num_datasets, figsize=(12, 4))\n",
    "mean_influences = {\n",
    "    'train': mean_train_influences,\n",
    "    'test': mean_test_influences\n",
    "}\n",
    "v_max = max(np.max(mean_train_influences), np.max(mean_test_influences))\n",
    "for i, dataset_name in enumerate(datasets.keys()):\n",
    "    x = datasets[dataset_name][0].copy()\n",
    "    ax[i].set_title(dataset_name)\n",
    "    points = ax[i].scatter(x[:, 0], x[:, 1], c=mean_influences[dataset_name], vmin=0, vmax=v_max, cmap=\"plasma\")\n",
    "\n",
    "plt.suptitle(\"Influences of training and test set.\")\n",
    "plt.colorbar(points)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flipping 5% of the labels of the train set\n",
    "\n",
    "We assume that our reference test set is not flipped and was checked, e.g. by human inspection. In comparison the test set gets flipped 5% at random positions. We want to show how to identify flipped examples "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flip_percentage = 0.05\n",
    "flip_num_samples = int(flip_percentage * len(dataset.x_train))\n",
    "idx = np.random.choice(len(dataset.x_train), replace=False, size=flip_num_samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flipped_dataset = Dataset(\n",
    "    dataset.x_train.copy(),\n",
    "    dataset.y_train.copy(),\n",
    "    dataset.x_test.copy(),\n",
    "    dataset.y_test.copy()\n",
    ")\n",
    "flipped_dataset.y_train[idx] = 1 - flipped_dataset.y_train[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Identifiying data points with absurd high gradients"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flipped_train_test_influences = influences(\n",
    "    model,\n",
    "    flipped_dataset.x_train,\n",
    "    flipped_dataset.y_train,\n",
    "    flipped_dataset.x_test,\n",
    "    flipped_dataset.y_test,\n",
    "    influence_type=InfluenceTypes.Up\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_flipped_train_test_influences = np.abs(np.mean(flipped_train_test_influences, axis=0))\n",
    "estimated_idx = np.flip(np.argsort(mean_flipped_train_test_influences))[:len(idx)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "found_elements = set(estimated_idx).intersection(set(idx))\n",
    "remaining_element = set(idx).difference(set(estimated_idx))\n",
    "display(f\"Around {100* len(found_elements) / len(idx):.2f} could be identified. But there are {100* len(remaining_element) / len(idx):.2f}% remaining samples\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see it was very straightforward to identify 82% of the 5% flipped data samples. This accounts for at most 1% remaining flipped samples. We now want to take a look at these cases."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "remaining_idx = np.asarray(list(remaining_element), dtype=int)\n",
    "plt.figure()\n",
    "y = flipped_dataset.y_train[remaining_idx]\n",
    "for v in np.unique(y):\n",
    "    sub_idx = np.argwhere(y == v)\n",
    "    c_idx = remaining_idx[sub_idx]\n",
    "    plt.scatter(flipped_dataset.x_train[c_idx, 0], flipped_dataset.x_train[c_idx, 1], label=str(v))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "48623f3f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mean_flipped_train_test_influences = np.abs(np.mean(flipped_train_test_influences, axis=0))\n",
    "estimated_idx = np.flip(np.argsort(mean_flipped_train_test_influences))[:len(idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f6acf1c6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Around 82.86 could be identified. But there are 17.14% remaining elements'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "found_elements = set(estimated_idx).intersection(set(idx))\n",
    "remaining_element = set(idx).difference(set(estimated_idx))\n",
    "display(f\"Around {100* len(found_elements) / len(idx):.2f} could be identified. But there are {100* len(remaining_element) / len(idx):.2f}% remaining samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fb15ab",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we can see it was very straightforward to identify 82% of the 5% flipped data samples. This accounts for at most 1% remaining flipped samples. We now want to take a look at these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "54b627b6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "remaining_idx = np.asarray(list(remaining_element), dtype=int)\n",
    "plt.figure()\n",
    "y = flipped_dataset.y_train[remaining_idx]\n",
    "for v in np.unique(y):\n",
    "    sub_idx = np.argwhere(y == v)\n",
    "    c_idx = remaining_idx[sub_idx]\n",
    "    plt.scatter(flipped_dataset.x_train[c_idx, 0], flipped_dataset.x_train[c_idx, 1], label=str(v))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429af1a7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
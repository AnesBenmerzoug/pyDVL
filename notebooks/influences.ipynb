{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd4dc3a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Influence functions for data mis-labelling\n",
    "\n",
    "Both data mis-labelling and outlier detection target the same problem, they are operating on discrete and continuous values, respectively. Imagine a simple classification problem, where $y_i \\in \\{1, \\dots, K\\}$, the goal is now to find labels which are mislabelled. In our case we further simplify the problem to set $K=2$.\n",
    "\n",
    "## 1. Using a GMM to generate artificial data with unique decision boundary\n",
    "\n",
    "A Gaussian mixture model (GMM) is used\n",
    "\n",
    "$$\n",
    "y_i \\sim \\text{Cat}\\left ( K, p=\\frac{1}{K} \\right) \\\\\n",
    "x_i \\sim \\mathcal{N}\\left (\\cdot |\\mu_{y_i}, \\sigma^2 I \\right),\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23cb0e79",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sigma = 0.2\n",
    "mus = np.asarray([\n",
    "    [0.0, 0.0],\n",
    "    [1.0, 1.0]\n",
    "])\n",
    "\n",
    "\n",
    "num_classes = len(mus)\n",
    "num_samples = 10000\n",
    "num_features = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bb0a11",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "for generating the data with fixed means $\\mu_1, \\dots, \\mu_k$ and standard deviation $\\sigma$. Sampling is fairly easy and can be done by sampling $N$ target data points from a categorical distribution. Afterwards the features can be obtained by using the previously sampled target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398cca1d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from valuation.utils import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "gaussian_cov = sigma * np.eye(num_features)\n",
    "gaussian_chol = np.linalg.cholesky(gaussian_cov)\n",
    "y = np.random.randint(num_classes, size=num_samples)\n",
    "x = np.einsum('ij,kj->ki', gaussian_chol, np.random.normal(size=[num_samples, num_features])) + mus[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using this model has the advantage, that the unique decision boundary is easy inferrable,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\| x - \\mu_1 \\|^2 &= \\| x - \\mu_2 \\|^2 \\\\\n",
    "\\| \\mu_1 \\|^2 -2 x^\\mathsf{T} \\mu_1 &= \\| \\mu_2 \\|^2 -2 x^\\mathsf{T} \\mu_2 \\\\\n",
    "\\implies 0 &= 2 (\\mu_2 - \\mu_1)^\\mathsf{T} x + \\| \\mu_1 \\|^2 - \\| \\mu_2 \\|^2 \\\\\n",
    "0 &= \\mu_1^\\mathsf{T}x - \\mu_2^\\mathsf{T}x - \\frac{1}{2} \\mu_1^\\mathsf{T} \\mu_1 + \\frac{1}{2} \\mu_2^\\mathsf{T} \\mu_2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "by using Bayesian decision theory. Further enforcing a functional form $f(z) = x = a z + b$ with $z \\in \\mathbf{R}$ onto $x \\in \\mathbf{R}^d$ yields the implicit decision function in terms of\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "0 &= (\\mu_2 - \\mu_1)^\\mathsf{T} (at + b) + \\frac{1}{2} \\| \\mu_1 \\|^2 - \\| \\mu_2 \\|^2 \\\\\n",
    "\\implies f(t) &= \\underbrace{\\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix} (\\mu_2 - \\mu_1)}_a t + \\underbrace{\\frac{\\mu_1 + \\mu_2}{2}}_b\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "with vectors $a, b \\in \\mathbf{R}^2$. Now it is possible to obtain"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_min = np.asarray([-2, -2])\n",
    "x_max = np.asarray([3, 3])\n",
    "z_linspace = np.linspace(-1.5, 1.5, 100).reshape([-1, 1])\n",
    "\n",
    "a = np.asarray([[0, 1], [-1, 0]]) @ (mus[1] - mus[0])\n",
    "b = np.sum(mus, axis=0) / 2\n",
    "a = a.reshape([1, -1])\n",
    "decision_boundary = z_linspace * a + b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " the decision boundary. The next step is to wrap the previously generated data into a dataset with separate training and test set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "arg_flipper = lambda x1, x2, y1, y2: (x1, y1, x2, y2)\n",
    "dataset = Dataset(*arg_flipper(*train_test_split(x, y, train_size=0.70)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is always a good idea to visualize the dataset. In the 2-dimensional case it is rather straight forward. Each class is represented by a unique color."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "datasets = {\n",
    "    'train': (dataset.x_train, dataset.y_train),\n",
    "    'test': (dataset.x_test, dataset.y_test)\n",
    "}\n",
    "num_datasets = len(datasets)\n",
    "fig, ax = plt.subplots(1, num_datasets, figsize=(12, 4))\n",
    "\n",
    "for i, dataset_name in enumerate(datasets.keys()):\n",
    "    x, y = datasets[dataset_name]\n",
    "    ax[i].set_title(dataset_name)\n",
    "    ax[i].set_xlim(x_min[0], x_max[0])\n",
    "    ax[i].set_ylim(x_min[1], x_max[1])\n",
    "    ax[i].plot(decision_boundary[:, 0], decision_boundary[:, 1], color=\"black\")\n",
    "\n",
    "    for v in np.unique(y):\n",
    "        idx = np.argwhere(y == v)\n",
    "        ax[i].scatter(x[idx, 0], x[idx, 1], label=str(v))\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that both the train and test set are plotted side by side as well as the previously determined decision boundary. The next section cares about how to calculate influences for this dataset under the assumption of using a logistic regression model for inferring the right labels."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Calculating influences using a (differentiable) logistic regression model\n",
    "\n",
    "Using the pyDVL valuation library a model can be formalized and fitted by using just a few lines of code."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from valuation.models.pytorch_model import PyTorchSupervisedModel, PyTorchOptimizer\n",
    "from valuation.models.binary_logistic_regression import BinaryLogisticRegressionTorchModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model = PyTorchSupervisedModel(\n",
    "    model=BinaryLogisticRegressionTorchModel(num_features),\n",
    "    objective=F.binary_cross_entropy,\n",
    "    num_epochs=100,\n",
    "    batch_size=128,\n",
    "    optimizer=PyTorchOptimizer.ADAM_W,\n",
    "    optimizer_kwargs={\n",
    "        \"lr\": 0.005,\n",
    "        \"weight_decay\": 0.005\n",
    "    },\n",
    ")\n",
    "model.fit(\n",
    "    dataset.x_train,\n",
    "    dataset.y_train\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note how the objective is specified in the model class, because the Hessian and scores calculated throughout are with respect to this loss function. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next the influences with respect to the previously fitted logistic regression model are calculated. A influence function $I(x_1, x_2) \\colon \\mathbb{R}^n \\times \\mathbb{R}^n \\to \\mathbb{R} $ measures the influence of the data point $x_1$ onto $x_2$ conditioned on the training targets $y_1$ and $y_2$. As long as the loss function $L(x, y)$ is differentiable (or can be approximated by a surrogate objective)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from valuation.influence.general import influences\n",
    "from valuation.influence.types import InfluenceTypes\n",
    "train_influences = influences(\n",
    "    model,\n",
    "    dataset.x_train,\n",
    "    dataset.y_train,\n",
    "    dataset.x_test,\n",
    "    dataset.y_test,\n",
    "    influence_type=InfluenceTypes.Up\n",
    ")\n",
    "test_influences = influences(\n",
    "    model,\n",
    "    dataset.x_test,\n",
    "    dataset.y_test,\n",
    "    influence_type=InfluenceTypes.Up\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Afterwards the average absolute influence (MAI) is calculated, the formula is given by\n",
    "$$\\text{MAI}(x) = \\frac{1}{N} \\sum_{i=1}^N | I(x, x_i) |.$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_influences = lambda arr: np.mean(np.abs(arr), axis=0)\n",
    "mean_train_influences = mean_influences(train_influences)\n",
    "mean_test_influences = mean_influences(test_influences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And afterwards visualized by"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "fig, ax = plt.subplots(1, num_datasets, figsize=(12, 4))\n",
    "mean_influences = {\n",
    "    'train': mean_train_influences,\n",
    "    'test': mean_test_influences\n",
    "}\n",
    "v_max = max(np.max(mean_train_influences), np.max(mean_test_influences))\n",
    "for i, dataset_name in enumerate(datasets.keys()):\n",
    "    x = datasets[dataset_name][0].copy()\n",
    "    ax[i].set_title(dataset_name)\n",
    "    ax[i].set_xlim(x_min[0], x_max[0])\n",
    "    ax[i].set_ylim(x_min[1], x_max[1])\n",
    "    ax[i].plot(decision_boundary[:, 0], decision_boundary[:, 1], color=\"black\")\n",
    "    points = ax[i].scatter(x[:, 0], x[:, 1], c=mean_influences[dataset_name], vmin=0, vmax=v_max, cmap=\"plasma\")\n",
    "\n",
    "plt.suptitle(\"Influences of training and test set.\")\n",
    "plt.colorbar(points)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Flipping 5% of the labels of the training set and identify them\n",
    "\n",
    "It is assumed that our reference test set is not flipped and was checked. As the test set is much smalled then the train set, this is a viable solution. In comparison to the correct test set, 5% of the training set get flipped at random positions. Next it is shown how to identify flipped examples"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "flip_percentage = 0.05\n",
    "flipped_dataset = copy(dataset)\n",
    "flip_num_samples = int(flip_percentage * len(dataset.x_train))\n",
    "idx = np.random.choice(len(dataset.x_train), replace=False, size=flip_num_samples)\n",
    "flipped_dataset.y_train[idx] = 1 - flipped_dataset.y_train[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Start by fitting a new model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from valuation.models.pytorch_model import PyTorchSupervisedModel, PyTorchOptimizer\n",
    "from valuation.models.binary_logistic_regression import BinaryLogisticRegressionTorchModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "flipped_model = PyTorchSupervisedModel(\n",
    "    model=BinaryLogisticRegressionTorchModel(num_features),\n",
    "    objective=F.binary_cross_entropy,\n",
    "    num_epochs=100,\n",
    "    batch_size=128,\n",
    "    optimizer=PyTorchOptimizer.ADAM_W,\n",
    "    optimizer_kwargs={\n",
    "        \"lr\": 0.005,\n",
    "        \"weight_decay\": 0.005\n",
    "    },\n",
    ")\n",
    "flipped_model.fit(\n",
    "    flipped_dataset.x_train,\n",
    "    flipped_dataset.y_train\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "to the flipped dataset and use it to calculate the influences and derive the absolute mean influences for all samples again."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flipped_train_test_influences = influences(\n",
    "    flipped_model,\n",
    "    flipped_dataset.x_train,\n",
    "    flipped_dataset.y_train,\n",
    "    flipped_dataset.x_test,\n",
    "    flipped_dataset.y_test,\n",
    "    influence_type=InfluenceTypes.Up\n",
    ")\n",
    "mean_flipped_train_test_influences = np.mean(np.abs(flipped_train_test_influences), axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make an comparison how good it approximates the randomly flipped samples, we take the 5% training samples with the highest absolute mean influence and measure the accuracy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "estimated_idx = np.flip(np.argsort(mean_flipped_train_test_influences))[:len(idx)]\n",
    "found_elements = set(estimated_idx).intersection(set(idx))\n",
    "remaining_element = set(idx).difference(set(estimated_idx))\n",
    "f\"Around {100* len(found_elements) / len(idx):.2f}% could be identified. But there are {100* len(remaining_element) / len(idx):.2f}% remaining samples\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Depending on the sampled dataset a detection of up to 76.43% percent of the training data could be achieved. One might further inspect the selection method for the indices as it only selects the 200 highest influence points."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "304358af",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Depending on the sampled dataset a detection of up to 76.43% percent of the training data could be achieved. One might further inspect the selection method for the indices as it only selects the 200 highest influence points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
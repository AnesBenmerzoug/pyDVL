{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a75acfec",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Clean dataset using influence functions and neural networks\n",
    "\n",
    "A dataset usually consists of a lot of data points, while not all data points are equally important for achieving the desired accuracy. Sometimes it is necessary to speed up the gradient calculations or optimize the required RAM. Example applications are reinforcement learning or continuous optimal control tasks. This notebook\n",
    "\n",
    "- shows how to calculate influences using the pyDVL library of an arbitrary (usually called training) set of data samples $X_\\text{train} \\subseteq \\mathbb{R}^d$ onto a test set $X_\\text{test} \\subseteq \\mathbb{R}^d$.\n",
    "- shows a plot with the weighted F1-score on the y-axis and the number of samples on the x-axis.\n",
    "- selects the optimal number of samples for the dataset as induced by the influence functions.\n",
    "\n",
    "First, an arbitrary random dataset can be loaded or created using the ```utils.dataset.Dataset``` class. For showcase purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be813151",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from valuation.utils.dataset import Dataset\n",
    "\n",
    "wine_bunch = load_wine(as_frame=True)\n",
    "dataset = Dataset.from_sklearn(wine_bunch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7204cfba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "the wine dataset is used. It can be loaded (as any other compatible dataset) directly from sklearn.\n",
    "\n",
    "## Visualizing the multi-dimensional data using a TSNE embedding and class colors\n",
    "\n",
    "As usual a closer inspection of the data is helpful. Hence, the data is plotted by first encoding it using a TSNE embedding. This can be achieved by the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d09e2c4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from valuation.utils.dataset import dataset_tsne_encode, dataset_to_json\n",
    "from valuation.utils.plotting import plot_datasets\n",
    "\n",
    "tsne_dataset = dataset_tsne_encode(dataset)\n",
    "tsne_dateset_json = dataset_to_json(tsne_dataset)\n",
    "plot_datasets(tsne_dateset_json, s=30, suptitle=\"TSNE-encoded dataset with labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a018e72c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Fit a neural network to the data\n",
    "\n",
    "First, a 2-layer neural network is created and fitted with pyDVL. This can be achieved by the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc59af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import torch\n",
    "from valuation.models import TorchModule, TorchOptimizer, NeuralNetworkTorchModel, TorchObjective\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "x_transformer = MinMaxScaler()\n",
    "\n",
    "transformed_dataset = copy(dataset)\n",
    "transformed_dataset.x_train = x_transformer.fit_transform(transformed_dataset.x_train)\n",
    "transformed_dataset.x_test = x_transformer.transform(transformed_dataset.x_test)\n",
    "feature_dimension = transformed_dataset.x_train.shape[1]\n",
    "unique_classes = np.unique(np.concatenate((dataset.y_train, dataset.y_test)))\n",
    "num_classes = len(unique_classes)\n",
    "\n",
    "network_size = [16, 16]\n",
    "model = TorchModule(\n",
    "    model=NeuralNetworkTorchModel(feature_dimension, num_classes, network_size),\n",
    "    objective=TorchObjective(F.cross_entropy, \"long\"),\n",
    "    num_epochs=300,\n",
    "    batch_size=32,\n",
    "    optimizer=TorchOptimizer.ADAM,\n",
    "    optimizer_kwargs={\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 0.001,\n",
    "        \"cosine_annealing\": True,\n",
    "    }\n",
    ")\n",
    "model.fit(\n",
    "    transformed_dataset.x_train,\n",
    "    transformed_dataset.y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da68f7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "code snippet. It creates a 2-layer neural network with 16 neurons in each hidden layer. After fitting the data, the TSNE embedded data along with the predicted values get displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbc35e4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pred_y_train = np.argmax(model.predict(transformed_dataset.x_train), axis=1)\n",
    "pred_y_test = np.argmax(model.predict(transformed_dataset.x_test), axis=1)\n",
    "\n",
    "datasets = {\n",
    "    'train': (tsne_dataset.x_train, dataset.y_train),\n",
    "    'train_predicted': (tsne_dataset.x_train, pred_y_train),\n",
    "    'test': (tsne_dataset.x_test, dataset.y_test),\n",
    "    'test_predicted': (tsne_dataset.x_test, pred_y_test)\n",
    "}\n",
    "plot_datasets(datasets, s=20, suptitle=\"TSNE embedded samples of train and test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3345522",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A short visual inspection yields that all labels of the test set are now classified correctly. This can be verified by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f1cba4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(dataset.y_test, pred_y_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dbdd1e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "plotting a confusion matrix using ```sklearn``` library. As one can see the accuracy is 100% for the selected model.\n",
    "\n",
    "## Calculating influences for small neural networks\n",
    "\n",
    "The following section elaborates the calculation of influences through a neural network. It is noteworthy that the full Hessian matrix is constructed and used for inverting the gradients. This can only be achieved for small networks. In the case of a big network additionally conjugate gradient has to be used to perform approximate inversion. The influences for both the train and test set are calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d0983",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from valuation.influence.general import influences\n",
    "\n",
    "inversion_method = \"direct\" # cg for big networks\n",
    "test_influences = influences(model, transformed_dataset.x_test, transformed_dataset.y_test, inversion_method=inversion_method)\n",
    "train_influences = influences(model, transformed_dataset.x_train, transformed_dataset.y_train, transformed_dataset.x_test, transformed_dataset.y_test, inversion_method=inversion_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48f40ba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "by the preceding code snippet. Subsequently, the mean absolute influence of one train sample onto all test samples is calculated by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9169c2dc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mean_influences = lambda arr: np.mean(np.abs(arr), axis=0)\n",
    "mean_train_influences = mean_influences(train_influences)\n",
    "mean_test_influences = mean_influences(test_influences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca7fa21",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "and plotted by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2577a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "influence_datasets = {\n",
    "    'train': (tsne_dataset.x_train, mean_train_influences),\n",
    "    'test': (tsne_dataset.x_test, mean_test_influences),\n",
    "}\n",
    "plot_datasets(influence_datasets, s=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774b81fc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "a visual inspection yields that there are indeed different influential samples.\n",
    "\n",
    "## Keeping the most influential samples\n",
    "\n",
    "Using the pyDVL library, the next section selects the most influential samples and retrains the model using the shortened dataset. This is done over the number of samples to showcase how the weighted F1-score changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581bdbe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display_functions import display\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "num_estimates_per_samples = 5\n",
    "min_num_samples = 1\n",
    "metrics = {}\n",
    "x_range = range(min_num_samples, len(dataset), 5)\n",
    "\n",
    "for i, num_shortened_samples in enumerate(x_range):\n",
    "    display(f\"Iteration {i+1}/{len(x_range)}\")\n",
    "    summed_metric = 0\n",
    "\n",
    "    for j in range(num_estimates_per_samples):\n",
    "        idx = np.flip(np.argsort(mean_train_influences))[:num_shortened_samples]\n",
    "        shortened_dataset = copy(transformed_dataset)\n",
    "        shortened_dataset.x_train = shortened_dataset.x_train[idx]\n",
    "        shortened_dataset.y_train = shortened_dataset.y_train[idx]\n",
    "        shortened_model = TorchModule(\n",
    "            model=NeuralNetworkTorchModel(feature_dimension, num_classes, network_size),\n",
    "            objective=TorchObjective(F.cross_entropy, \"long\"),\n",
    "            num_epochs=300,\n",
    "            batch_size=32,\n",
    "            optimizer=TorchOptimizer.ADAM,\n",
    "            optimizer_kwargs={\n",
    "                \"lr\": 0.001,\n",
    "                \"weight_decay\": 0.001,\n",
    "                \"cosine_annealing\": True,\n",
    "            }\n",
    "        )\n",
    "        shortened_model.fit(\n",
    "            shortened_dataset.x_train,\n",
    "            shortened_dataset.y_train\n",
    "        )\n",
    "        pred_y_test = np.argmax(shortened_model.predict(transformed_dataset.x_test), axis=1)\n",
    "        summed_metric += f1_score(shortened_dataset.y_test, pred_y_test, average=\"weighted\")\n",
    "\n",
    "    metrics[num_shortened_samples] = summed_metric / num_estimates_per_samples\n",
    "\n",
    "metrics = pd.Series(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093bb27",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note, how the model is created completely new and is not derived from the previous model. This section showcases how to use the influence calculations The notebook finishes by plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420849a4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "metrics.plot(xlabel=\"#samples\", ylabel=\"F1 score\", title=\"Weighted F1-Score over test set for number of most influential samples.\")\n",
    "hline_kwargs = {\n",
    "    \"linestyle\": \"--\",\n",
    "    \"color\": \"black\"\n",
    "}\n",
    "plt.axhline(0.0, **hline_kwargs)\n",
    "plt.axhline(1.0, **hline_kwargs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66293554",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "the optimal number of samples on the x-axis along with the weighted F1-score on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdab62f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimal_number_of_samples = metrics.index[metrics.argmax()]\n",
    "f\"The optimal number of samples is {optimal_number_of_samples}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e26cf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Over multiple runs with different the optimal number of samples canges, but it can be said to be safe to remove around 30% of the samples of the wine dataset to easily achieve the same data accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
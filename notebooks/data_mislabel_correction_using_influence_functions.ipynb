{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd4dc3a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Influence functions for data mislabeling\n",
    "\n",
    "\n",
    "Data mislabeling occurs whenever some examples from a usually big dataset are wrongly-labeled. This hardly violates the assumption that the data distribution of the data equals the hidden distribution of the real world. Hence, it is important to be able to restore such datasets automatically. In real-life scenarios, this happens fairly often and can have various reasons, e.g. problems with identifiability, human error, or noise in the data. Imagine a simple classification problem\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "x_i &\\in \\mathbb{R}^d \\\\\n",
    "y_i &\\in \\{0, 1\\} \\\\\n",
    "\\forall i &\\in [ N ]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "containing $N$ samples. A classical example is whether a patient has a disease or not based on some feature representation $x$ of that person. Furthermore, this formalism applies to all datasets, which can be transformed to a vector of size $d$ by either linearization or other embedding techniques. For the described model the optimal decision boundary can be derived manually. This showed to be advantageous for educational purposes. Using a Bernoulli distribution on the classes $y$ and for the features $x$ a Gaussian distribution conditioned on the previous samples class label $y$. This can be formalized as a graphical model\n",
    "\n",
    "$$\n",
    "y_i \\sim \\text{Ber}\\left (0.5 \\right) \\\\\n",
    "x_i \\sim \\mathcal{N}\\left (y_i \\mu_1 + (1 - y_i) \\mu_2, \\sigma^2 I \\right),\n",
    "$$\n",
    "\n",
    "with fixed means and diagonal covariance. Implementing the sampling scheme in python is straightforward and can be achieved by first sampling $y$ and afterward $x$. More formally $x$ is a function of $y$ and not vice versa. The following code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23cb0e79",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from valuation.utils.numeric import sample_classification_dataset_using_gaussians\n",
    "\n",
    "num_samples = 10000\n",
    "num_features = 2\n",
    "sigma = 0.2\n",
    "mus = np.asarray([\n",
    "    [0.0, 0.0],\n",
    "    [1.0, 1.0]\n",
    "])\n",
    "\n",
    "num_classes = len(mus)\n",
    "x, y = sample_classification_dataset_using_gaussians(mus, sigma, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bb0a11",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "generates the aforementioned dataset. After the dataset was generated, a closer inspection of the data is performed.\n",
    "\n",
    "#### [*] Calculating the decision boundary\n",
    "\n",
    "For obtaining the optimal discriminator one has to solve the equation\n",
    "\n",
    "$$p(x|y=0)=p(x|y=1)$$\n",
    "\n",
    "and determine the solution set $X$. A closed-form solution can be found, if the pdf of both classes can be written in closed form. In the general case, this decision boundary must not be linear. However, in the case of two Gaussians\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(x|y=0)&=\\mathcal{N}\\left (\\mu_1, \\sigma^2 I \\right) \\\\\n",
    "p(x|y=1)&=\\mathcal{N}\\left (\\mu_2, \\sigma^2 I \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "the boundary is linear and is straightforward to derive it. The complete case with different full covariances is left to the reader as an exercise. For a single fixed diagonal variance parameterized by $\\sigma$ the equation can be directly rewritten as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\| x - \\mu_1 \\|^2 &= \\| x - \\mu_2 \\|^2 \\\\\n",
    "\\| \\mu_1 \\|^2 -2 x^\\mathsf{T} \\mu_1 &= \\| \\mu_2 \\|^2 -2 x^\\mathsf{T} \\mu_2 \\\\\n",
    "\\implies 0 &= 2 (\\mu_2 - \\mu_1)^\\mathsf{T} x + \\| \\mu_1 \\|^2 - \\| \\mu_2 \\|^2 \\\\\n",
    "0 &= \\mu_1^\\mathsf{T}x - \\mu_2^\\mathsf{T}x - \\frac{1}{2} \\mu_1^\\mathsf{T} \\mu_1 + \\frac{1}{2} \\mu_2^\\mathsf{T} \\mu_2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "by using linear decision theory (see [Bishop C. 2006] for more details). However, this implicit description has to be transferred to an explicit one to effectively use it in python. Solving for the explicit form can be achieved by enforcing a functional form $f(z) = x = a z + b$ with $z \\in \\mathbb{R}$ onto $x$. After the term is inserted in the previous equation\n",
    "\n",
    "$$\n",
    "0 = (\\mu_2 - \\mu_1)^\\mathsf{T} (az + b) + \\frac{1}{2} \\| \\mu_1 \\|^2 - \\| \\mu_2 \\|^2\n",
    "$$\n",
    "\n",
    "by setting $a$ to be explicitly orthogonal to $\\mu_2 - \\mu_1$ and then solving for $b$ the solution\n",
    "\n",
    "$$\n",
    "f(z) = \\underbrace{\\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix} (\\mu_2 - \\mu_1)}_a z + \\underbrace{\\frac{\\mu_1 + \\mu_2}{2}}_b\n",
    "$$\n",
    "\n",
    "for the functional form $f(z) = a z + b$ can be obtained. The following code snippet explicitly calculates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "789b066b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from valuation.utils import decision_boundary_fixed_variance_2d\n",
    "\n",
    "decision_boundary_fn = decision_boundary_fixed_variance_2d(mus[0], mus[1])\n",
    "decision_boundary = decision_boundary_fn(np.linspace(-1.5, 1.5, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb167bfe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " the decision boundary by mapping a continuous line of z values to a 2-dimensional vector in feature space.\n",
    "\n",
    "#### Plotting the dataset\n",
    "Next step consists in wrapping the previously generated data into a dataset with separate training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f290148d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from valuation.utils import Dataset\n",
    "\n",
    "arg_flipper = lambda x1, x2, y1, y2: (x1, y1, x2, y2) # hacky\n",
    "dataset = Dataset(*arg_flipper(*train_test_split(x, y, train_size=0.70)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2548ea8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Subsequently, the data is plotted with their respective class labels 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f22a0c94",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 864x288 with 2 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsQAAAEICAYAAABYl+LRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABwvElEQVR4nO29eXxTZdr//7mTJm3aYhfAUQoKAqIiBaQoDvzGEQVUFCsqjriOIiIqriA4frWoM1ZQcdRxQXGbB9QqUEF0UHF7wLVYFnFkfBwdoUVla0fatE2T+/fHyUlPTs59luRkv96vF0uTs9xJk+tc51o+F+OcgyAIgiAIgiCyFUeyF0AQBEEQBEEQyYQcYoIgCIIgCCKrIYeYIAiCIAiCyGrIISYIgiAIgiCyGnKICYIgCIIgiKyGHGKCIAiCIAgiqyGHmMhoGGNPMsb+X7LXQRAEQRBE6kIOMZHSMMZ+YIydGu3+nPMZnPN77FwTQRAEYUys9jt4jMsZY+vtWhNBiCCHmEhbGGM5yV4DQRAEQRDpDznERMrCGPs7gMMArGaMHWCMzWGMccbYlYyxHwG8F9zuVcbYT4yxZsbYR4yxwYpjPM8Yuzf4/98zxnYyxm5hjP3CGNvFGPtjUl4cQRBEBiOw36MYYx8zxpoYY5sZY79XbH85Y+zfjLFfGWPfM8YuYowdDeBJACcGj9GUlBdDZAXkEBMpC+f8EgA/AjiLc14IoCb41EkAjgYwIfjzWwAGAjgYwJcAluoc9hAARQDKAFwJ4G+MsRL7V08QBJG9aNjvpQDWALgXQCmAWwEsZ4z1ZIwVAHgEwOmc824AfgtgE+f8nwBmAPiEc17IOS9OwkshsgRyiIl0pIpz3sI59wIA5/xZzvmvnPN2AFUAhjLGigT7+gDczTn3cc7fBHAAwKCErJogCCJ7uRjAm5zzNznnAc75OwDqAJwRfD4A4FjGmIdzvotzvi1pKyWyEnKIiXRkh/wfxpiTMVbNGPuOMfZfAD8En+oh2Hcv57xT8XMrgML4LJMgCIIIcjiA84PlEk3B8ocxAA7lnLcAuABSNHgXY2wNY+yoJK6VyELIISZSHW7w2FQAZwM4FVIpRN/g4yy+yyIIgiAMUNrqHQD+zjkvVvwp4JxXAwDnfC3nfByAQwF8A+BpjWMQRNwgh5hIdX4GcITO890AtAPYCyAfwF8SsSiCIAjCEKX9/h8AZzHGJgQze3nBRufejLHfMMbODtYSt0MqZQsojtGbMeZO/PKJbIIcYiLVuQ/AHcH02nkaz78I4D8AGgB8DeDTxC2NIAiC0EFpvy+AlM27HcBuSBHj2ZD8EAeAmwE0AtgHqXH6muAx3gOwDcBPjLE9iVw8kV0wzikbQRAEQRAEQWQvFCEmCIIgCIIgspqYHeJgHdDnQZHtbYyx+XYsjCAIgogPZLcJgiDCiblkgjHGABRwzg8wxlwA1gO4gXNOtZwEQRApCNltgiCIcHJiPQCXPOoDwR9dwT9UmEwQBJGikN0mCIIIJ2aHGJCGIwDYCGAAgL9xzj/T2GY6gOkAUFBQMOKoo0hzmyCI9GPjxo17OOc9k72OWDGy22SzCYLIFMzYbVtVJhhjxQBWAriec/6VaLuKigpeV1dn23kJgiASBWNsI+e8ItnrsAszdptsNkEQ6YwZu22rygTnvAnA+wBOs/O4BEEQRHwgu00QBGGPykTPYIQBjDEPgHGQxi4SBEEQKQjZbYIgiHDsqCE+FMALwXo0B4AazvkbNhyXIAiCiA9ktwmCIBTYoTKxBcBwG9ZCEARBJACy2wRBWMHn82Hnzp1oa2tL9lJ0ycvLQ+/eveFyuSzva4vKBEEQBEEQBJGZ7Ny5E926dUPfvn0hyZinHpxz7N27Fzt37kS/fv0s70+jmwmCIAiCIAghbW1t6N69e8o6wwDAGEP37t2jjmKTQ0wQBEEQBEHoksrOsEwsaySHmCAIgiAIgshqyCEmCIIgCIIgUp5//OMfGDRoEAYMGIDq6mpbj00OMUEQBEEQBJHS+P1+XHvttXjrrbfw9ddf46WXXsLXX39t2/FJZYIgCIIgCIKwjdr6Bixcux2NTV70KvZg9oRBqBxeFtMxP//8cwwYMABHHHEEAOAPf/gDXn/9dRxzzDF2LJkixARBEARBEIQ91NY3YN6KrWho8oIDaGjyYt6Kraitb4jpuA0NDejTp0/o5969e6OhIbZjKiGHmCAIgiAIgrCFhWu3w+vzhz3m9fmxcO32JK3IHOQQEwRBEARBELbQ2OS19LhZysrKsGPHjtDPO3fuRFlZbGUYSsghJgiCIAiCIGyhV7HH0uNmGTlyJL799lt8//336OjowMsvv4xJkybFdEwl5BATBEEQBEEQtjB7wiB4XM6wxzwuJ2ZPGBTTcXNycvDYY49hwoQJOProozFlyhQMHjw4pmOGHd+2IxEEQRAEQRBZjawmYbfKBACcccYZOOOMM2I+jhbkEBMEQRAEQRC2UTm8zBYHOJFQyQRBEARBEASR1ZBDTBAEQRAEQWQ15BATBEEQBEEQWQ05xASRSWypARYdC1QVS/9uqUn2igiCIJID2UPCAtRURxCpxpYaYN3dQPNOoKg3cMqdQPkUc/utngX4guLnzTuknwFz+xMEQWQKZA8Ji1CEmCDiQbSRCdmIN+8AwLuMuJn9193dZfxlfF7pcYIgiGyC7GHGccUVV+Dggw/GscceG5fjk0NMZBZKR/T+ftKfRKfLkuXUNu+09jhBEESmQvYw47j88svxj3/8I27HJ4eYyBzUjqh3n/THqlNqdA6jyG+ynNqi3tYeJwiCyFTIHiaXONRv/+53v0NpaWnsaxNADjGROWg5okpiTZeZjfzGw6llDmODcsqdgEs1K97lkR4nCILIJsgeJo9YsqRJhBxiIrWxcpdpxuGMJV1mNvIbS2RCy4gDAPcbG5TyKcBZjwBFfQAw6d+zHqEGEoIg0pdoI4169pDUJ+JLmtZvk8oEkbpY7RIu6h28I9UhlnSZ2cjvKXeGrxswH5mQX9fKGZITrEQ2KHoObvkUcoAJgsgMYlWK0LKHpD4Rf9K0fpsixETqYvUuUxRdlYk1XWY28htNpDYUsSjSdoZl1AaFIh0EQWQqdkYaZVu54qr4Ri9T0SYnek1pWr9NEWIidbF6lyk7nLKGr6dE+tm735qerxKlJrCnBHC4gICv63mXBxg4XjIyat1gs+faUgO8fi3g75B+FjnDQLhBoUgHQRCpSLRa6mrsijSqbaUdxzRznlhssl3vYTKuE7FkSXW48MIL8cEHH2DPnj3o3bs35s+fjyuvvDLGxXZBDjGRuohKIPTuMq2WDOgZHbUh8e4DnG7AU9rlZA8cD2xeZt7YaJ3vrdu6nGE91AZFL3pCDjFBEMnATgcsmmuAFkYN1+pjqgMhgLnAil022c73MBnXCXVwKhaHXsFLL71kw+LEUMkEkbrEu0tYqxN2xXTgjZul57UMib8DcBcAVU3ATV8B375tPv0m6rz17jNeq6cUyPFI65NTXsLoyY7USdURBJFd2FnmYNc1wKi3RHnMWOQ77Ypo2/keJquet3yKdI2Ur5VpEKQhh5hIXeKtmqAZNeBA3bMGDudO7f+LttE7n1HUApCc4U5vpFGWIxea508PmRuCIDIMOx0wK9cArTpZ+TE91MeMRb7TTO2smXpeM++h2brgNK3nTQZUMkGkNnaWQKgRGmgulTGYSddZSelFc0GQoyNajnSOR3peZLyVaTG76tEIgiD0sKvMQcbMNUCrxKB2JsCYcTma2hbGIt9pVDtrthTC6D20UlJhYz0v5xyMMcv7JRLOedT7UoSYSAzx6HJVH/ONm62JgetFWL37gNZgzbAStSERpfTkRjvl67V6QfCUSpEL737BGvcroicCmnemrUg6QRBpSDIGYmhFdQM+c70ZZnXkzWxjFNE2Wwph9B5aKamwKdOal5eHvXv3xuRwxhvOOfbu3Yu8vLyo9qcIMRF/4tHlqnXMuiWR24maB7bUAB0H9M/ha5FUJZRNdOpoglbzgKjRbuhU4MsXw1UqRHhKgdu+l/7/1m3adcaekq7oyaJjtSMKnpLoNY1lKLpMEIRZ4tRQpUss9bBmdOSVGDn3ehFts+UkRu9hNApMMb7/vXv3xs6dO7F79+6YjhNv8vLy0Lt3dNkIcoiJ+BOPLlczXcMyonpeM9GDgE9qopOdUy3UxmbRsdqv99u3gdxu5proRFFhEVpG3OkG2n81r2msBUm7EQRhlUQPCDIzlElvXyXxkO80XCeXrhvKY+u9h3aXpZjA5XKhX79+cTt+KkAOMRF/4tHlamVfT0mkTrCV/bWGYehFP+x4vUrDplcyIaMVUeho0Xe+9UpGZEjajSCIVEcrIOBwGdcQi6K9Rg690TVA9Lxe9NlKsCFOOr/ZTswOMWOsD4AXAfwGAAewmHP+11iPS2QQ8bibNRsRkKOksmOoVGgwE6kFwh1HUcT0x0+lCHDzToA5tKOynhIp2my4biaVXcjoRRWqiqTzjfgjcOZD4Ya0qtjc69MjTUdwEvqQ3SZSlmhKtEQlBurHBo7vstOyXV8xXdrGbORX6xqw4iqptO30+6XHjLJq6+7Wtulmgw3JKEvJAlisBdKMsUMBHMo5/5Ix1g3ARgCVnPOvRftUVFTwurq6mM6bTrS0tKCgoCDZy0geWlOCXJ7YJNS21EiGDDqf36I+OlFSpr+vEocLqHxcWuv9/aI/nsMJHHd5eH2xCOX7Y2bKEgBUXCk5xTKiumLlmqua9I8prE0uDTr32WeMGWMbOecVyV5HLFi129lms71eL9xuN5xOZ7KXkl1o2TqnG3AX2lOyoHce9TVJ5Jjr2VWXR1L/0bpGFPWR9Hhlqoqhfc0wYZcJy5ix2zGrTHDOd3HOvwz+/1cA/wRQFutxM4W2tjaceOKJmDFjBlpbW5O9nOSRo+iYldUTYjFq5VOAiisgOaICdEsGLNwIBnxSBEDoDJs8XsAPbHwO6H28FNXVQ90xnOMRbyuz8flw5Y2OYFOgCOYwVv3Q6naWo+6kWpG2kN3W59prr8XYsWOxY0eUdalEF1YUhkTDkJQa7LItNrI3euc1UmkQDW2qKtIPMvi84muEOqtmpA+sp6tsp1oTEcJW2TXGWF8AwwF8pvHcdMZYHWOsLtW7FO3E4XDgtNNOw1NPPYURI0Zg06ZNyV5SYpENi9JIdJpshpP3FxmAMx8CJi8GmCCKY7Ykwix2HI8HgO8/lP41onlHl5ScmXNzf+SEJcYAlyA7wf0wdGi1JHvchZFKGdFOUSKSjshuZ6vNBoCTTjoJX375JcrLy/Hqq68meznpi1XJR7ONcd59+scxOq9RKZhoaFMsqB3d5h2ICOjIdcBa66+dCbx+LQUi4ohtDjFjrBDAcgA3cs7/q36ec76Yc17BOa/o2bOnXadNedxuNxYsWIB33nkHzc3NOP744/Hggw8iEDDhEGUCZu7ERQ6vGWNaPsWcc5mu1D1rXk0D0I6u5JdK5RSiGwd5P5FDqx7BKWryo7ritEPPbmerzQaAyy67DPX19TjyyCMxZcoUXHHFFThwwECmkYjE6ghiPRulxueV6najOa9RdDZaxQpAyoCKNITDrmmA5GQHnWKlPrBZXWUKRNiKLQ4xY8wFyagu5ZyvsOOYmcapp56KLVu24IwzzsCtt96K0047DY2NjcleVvzRuxM3cnhFRm3ljNgGXqQVNoigN++Q6pZF8mvK7cxAo0AzArLb+gwYMADr16/Hn/70Jzz//PMYPnw4vvjii2QvK/VRBjlENkV0XTCyUWq8+6QsmnpAk9F59QZfbKmBbimeHi6P1FgnGoQhijzL9cVGOsN6r4mImZgdYibN8VsC4J+c84eMts9mevTogZUrV+Kpp57C+vXrUV5ejtdffz3Zy4oves6T0V28rtFUONADx0caN0IBMxdlNhudScYkKsJWyG6bw+Vy4d5778UHH3yA9vZ2/Pa3v8V9990Hv9+i45YtqIMcIphDO9VvJUIsU/dseFBFa0CTjCzBueIqoLNd8XhpuNNqORChcnzVWTXDgRoqB96MJKYMBSJsw44I8WgAlwAYyxjbFPxzhg3HzUgYY5g+fTq+/PJLHHbYYaisrMQ111yTuQ13es6TUfTYqPEM6Bp4cdYj1tblzibVD5PGnfvNNW3YNAqUSCpkty3wu9/9Dps3b8bkyZNx++2345RTTqGGOy3MDkyS+x3U9sVqhFjaydxmYc3ACC+1U/a1RFMuoXZ8RQidV9b1XginqDqk16CEAhG2YofKxHrOOeOcl3POhwX/vGnH4jKZo446Cp988gluvfVWPPnkk6ioqMjMhju18+QplRQTVkwXO7yeEslYmjWOzTul8xT1Mbd9UR/g9kZpLelA4aGJOY+n1HwDjCgCQqQFZLetU1JSgpdffhnPPfcc6urqqOFOCyvpe3X9a7ybw7SagbXWEk2U2iyn3AntcgzedX7RFFVPMXD237quc8zZtW5qrLMFW1UmCGvk5uZi4cKFoYa7E044AQ899FDmNdzJztPkxdKduCyho+XwytFkK41k8l23VjRai+YdknyO3SoU8eLArgScJGiktUpYRI0rBJFlMMZw+eWXY9OmTdRwJ6PMKpnJ6imRHWi51CJeFPUxtvfNO6TXYTVKLQdWTEuiCSLa8nshuqnw7u+adufydK1TVqC4v5/+uUmyzRByiFMAZcPdLbfcggkTJmRmw50oncacCEu9i1QMNGFdKaPyKZLGLxEFXHzB8O4j40kQCuSGuzvuuCNzGu6icZjUNcOiIIcoGycHM8yWWkSDwyVpsptBr1zCUxqp6+5wSU10mrrFKr3kN26WHhPCpfddVD+s914FfCqt5qBmslK/2Ir8XZZCDnGK0L17d6xYsQJPPfUUNmzYkJkNd8ImuUB46t1KkwBzSF/+RccCL0ySNH4J+yFpH4IIw+Vy4Z577smMhrtoHSazQY7T79dvxLVdKSGY8fKUSteXWLOBcu1uwNdVUlHUp2uC6Vu3ab8Psl7yGzfrN/vJNO+Q6pz1aoVNvVe863irZ2mvjyTbIiCHOIVI64Y7M9EFM3JdW2rM380D4YoT5AzHD5L2IQhNMqLhzqpesIyZIMePn0pSmcrjqxtxbVdK4F21tlE16inwlAJckUHj/i4HtXyKdM3Sc7h9XmmKqFkCPqneWdS0bPW9sjI9L8shhzgFkRvuZs+enR4Nd2ajC0ZyXW/cLEV706W2N5VxuCOjDLFA0j4EIURuuHv++eexcePG9Gu405MDiyXIIUdG1U7pwPHSv4uOlVL7/22Iatm6NO+I/VoiR4O1JnOuuAqYX2JQBhEkGn1lUdOy2V4ZM5BdD4Mc4hQlNzc3NOGuqakJJ5xwQupOuNOLLigjx+vuBoZODa8nywl+sbfUSHqSdgyiIIBAhxTVEI1ttoJZaR9q2iCyGMZY+k6403WMoghyDBwv2QBRmUDdEqkRTEsCLZXgfn2n2uy6o1GuENlPLeUmo+CH3vQ8IgTjPPEOSEVFBa+rq0v4edOVPXv2YNq0aXj99dcxbtw4vPDCCzj00ARJcZmhqhhCR9blCXeWHS6AsXBZGZdHcowpMmw/RX2kEpSo31sGVFwBnBmc3bClRrqxad4pXUSVacPVs1Q3Rqp9MwTG2EbOeUWy15FIyGZbw+fzYf78+fjLX/6C/v37Y+nSpTj++BRu+NX8/mogT1RT76u0CQPHS5Mx49Ukl2443cBhJ1ov6XMXSPKgZgj9DnZAqp9WXI9dni6dfi3bnSWYsdvkEKcJnHMsXrwYN910E/Lz8/Hss89i0qRJyV6WxKJjtbtzmTP2+i0iRmTNyxi+58wJnPOk9H/1RVM2tm/dJnC6mSS3l0GGlxxiwiwfffQRLr74YuzatQt333035syZA6czjjq3saB0bIX2gklpfD1E14OsReWgWmHy0/q2UytAAWS14yuCHOIM5JtvvsHUqVNRX1+PGTNm4MEHH0R+fr59JxBFAI320XKUKEKQfJgTyLNBc1kviu8p1T++VlQpjSGHmLDC/v37MWPGDNTU1OCkk07C3//+d/TpY3KIULIQObVmvst6GUPCGp5S4LbvtZ8TXXdpaqgmZuw21RCnGeoJdyNGjLCv4U7QHPfFqqcwuvo99Ju7BqOr30NtvaoBQjTK1+zkOBk7m8AICaMaOLPodSobCt5TJzORvaTlhDujBmg9qFHLPvQ04EW9OzRIKWrIIU5D4jbhTvAF67VxARqavOAAGpq8mLdiq7ZTrO6K1TKqamFzJf4Oe5rAiNSCLpBElqOccDdw4MDUb7gTBTnMZAutyGZmNQqtZD1EDq5woh0NUooWcojTGPWEu9NPPx27dsUw5lfwBTsUe8N+9vr8WLh2u/HxtIxq5eP6+/jImKYVTre+QadOZoIIMWDAAGzYsAG33357aky401OG0QpyiLbfUiNNZVtxFTVHm6Goj9RwXNRHmsyqFwgSvZ96gQYauBEV5BCnOT169AhNuPvf//1fDBkyBKtWrYruYIIvWCPvHvlYk8n6YC2jarWUgkgBmPbD7kLtKVSA5ChTPRtBhOFyufDnP/85eRPuQk5tkaT7bnY6nWg88Z97Aa9fS46wWSY/LQUJNi/rei+jCQTpBRqM9KNFZLl0JjnEGYB6wt3ZZ58d3YQ7jRIHL3KxoDPSobms8HNrXxzlF41SaukFc0LYJOPdr50JmPy01Ayidoaz3OAShExSJtyFObVAxPdabzqdaDyxryVcRpPQgUk2UTTyWgtlBk6t669bYmhhBLd87GjGd2cQ5BBnEOqGO8sT7jQcm6+OuwfvOE8K2+w898e4gz8ZfWSBIglpBJMk10RlEZ4S6V+tTIAaMrgEEYa64W7o0KF47bXX4ndCM46YVumc0XhiwiTBGxCzjcYOl5SBU5akKO1nwKfflwOYG8ENRD++O4MghzjDUDbcyRPuLDXcqRybkZOuxn2Th6Cs2AMGoKzYg7sLliPH3xa+n94Xx8rdMJFicOn319ke+6HMGFyKIBNZhrLhbsCAATj//PNx5ZVXxqfhzowjplU6l0VOUUIw22gc8EmR+doZ2jck/g4gt1tXEEuEmd+7cHx39qgEkUOcoagb7k477bSoG+4qh5dhw9yx+L56IjbMHYt8r+A4GrqVtfUNCOh9oYw6bInk07xDXONmJWpkZHApgkxkMcqGu+eeey4+DXdGjpioCTaLnKL44gTml1obXOLdBwR06su9+7uCWKL+HDMOuGibLFIJIoc4g1E23K1fvx7l5eVYvXp1dAdTRu5EqOa119Y3YN6KrWgMRDblAZCcYTfJrKU1qt+5LkYGl1J2RJaj1XBXXV1tX8OdlhSmHFkUSKvV1jfgJ/Sw5/xZj9/+6a1Ku3rKnRolFA6pb8co6xaL9nSGQA5xhiM33G3cuBG9e/fGpEmTMHPmTHMNd6JuZBGqL/rCtdvh9fmxoHMKWrl66IZDuvOlEZ/pjRXjbmRwKWVHEADCG+7mzZuHU0891Z6GO80G2MVAVbNm7b8c1PhLx/kaNjweMNKitwQDBo5XPaQunQgEM3kGWbdotaczCBrdnEW0t7fjjjvuwAMPPICjjz4ay5Ytw7Bhw6Qn1SObB46XZGGs1P7KEd/gMV7cOwhjHZvQi+1BEwrBOVDCDoADcOqUOxFpBHNKTXdmjabeaPBYxsUmEBrdTCQKzjleeOEFXHfddXC73Vi8eDHOO+88+08U/F7y5p34GT1wX8f5qDtoHFo7OrG/1QcAmORYjzk5NejF9iDAHMhBjIOgtKi4EqhbYv9xMx1PqdR8t+5uc0GmaG2qnv1OcczYbXKIs4ngh/nd+h9w6evt2OsF7qu+HzeO7QXHmhtVzi+DpXn0TjfAudQEEIQjvMy/nTvBwOBmnbG9DiK1cHnEkQQrBlSuIVZ+DvWOnSTIISYSzf/93/9h6tSp+OKLL3DFFVfgr3/9KwoLC+05uMb3LsCBv/tPxV2dV2juwgB8nzfVnvPLuAqk6wdJuEWJlWs2k2qOrZAm9lkEOcREF6oP857WAKa94cPr/2zH+EEFeH4iw6HdLFbQMCfAA5Kj09FCsjzZjFbEIRoDmgYRCHKIiWTg8/lQVVWF++67DwMGDMCyZctQUWHwMdT6PgHhjwlsN+fAPl6IEnYAjbwHFnROwarAGACS2tCGtnNse22yF0KJwwRS1MeafU2TDJ4IcoiJLjQ+zJxzLP66EDfV7kK+i+HZs/MwaZCBpqGM2rGpKoaliDKRYWhEHNLcgIogh5hIJh9++CEuvvhi/PTTT7jnnnswe/ZsOJ0aza1aN6QmMnkiWrkbc33T8I7zJNw3eQgq3x5DQZB0x0qEV3iNjyLanATM2G1qqssWNBqTGGO4enArNt4yAH2KGM5+2YuZa7xo9Ukf+oDqsx/gkKrGtIrt5QENRHbCHJFdzNQkRxC2c9JJJ2HLli0455xz9BvutFRb/B1hzjBgPiqbzzpwu/tVyRl2btDfmNSDUgsmcPWsqPhkgSwbOcTZgs6H+eipf8anM3rglhPdeKLOh4rFLfhklwN/95+KnYEeCHCGnYEeuNE3E/3bloV3I8tKFBQpyG64HxFdzKLPHHOQtjBBxEBJSQleeeUVPPfcc/jiiy8wdOhQLF++PHwjCzeeZhPFh2A3Kt8cKU1MU9t8T6k0sr2qGXDmmj43EWccLsQ8tAPIClk2coizBb0Pc/kU5J7zKB44fyDevjgf+9sZxiz5FTd9UorR7Q/jiPalGNPxCFYFxqBXseIYYYMUCCKIHHXQ1DyF5DzTwA2CiAn1hLvzzjsP06ZNkybcbakRRwU1OMBzIzKCQkRDetwFUqCExjynDkV9gmUyOvKYZiO8WSDLRjXE2YSqweKL/tfjxq8HoqHJCydj8HOOsmIPmvftxb9rH4L320+R13c4uk+8CTmFpWAAFl0wDJXDy6TjiWpECUKuK9tSA6ycoW2Q07SWmGqIiVQjrOGuzyFYNrEdFQdrqPlo1BDLtcEAJFk1x144PCVA+68R5RWGFPWha0Iq4HABlY9L/19xlXi7NFKJiBVqqstyausbsHDtdjQ2edGr2IPZEwaFnFlZcH2c/8OQtqSyk5hzjgOb/4H9654Bc+Wi++k3IH/gCfihemLXCaiRjtBD7mJeMR3izwlLWTUJEeQQE6nKhx9+iEsqx2HXf3245+RczP6tG05HMF0ua4YDIc3hRt4d9/u61CM8LqdUIzy8TBFAMevgWpTqJJLP5KfTxu7GCjXVZTGyw9vQ5AUH0NDkxbwVW1Fb3wBAmiI3zv8hql3PoLdjDxwM6O3Yg4ddj2N+zrNgjKHbsNNx6GUPo/SgPOxecQ9Oe3cyWu8/pivVTY10hB5yPbHu58RgehJBEKY56aSTsHm6B5OPzsG8de049e+t2NEcHKDBA5LzUz4FuOkrsMmLUZLvwsPuJ7DePQuXF37e5QwDqPWPxuj2R9DJjd0ErvibSBM8pVnjDJuFHOI0p7a+AaOr30O/uWswuvq9MIfX6wtPU3t9fixcux0A0NjkxZycGuSzcBF0BwMucb6LSY71AIBzD/4P/nWlH7ee6MZTGztQ8cB2bF58DTkvhDl8XsC7X0rVGm1nttuZIAghJYf0wcvnevDc2Xn4osGPoU8ewGtf+9DAu6Pv3DXoP+9N1Dz7ILB6FvK9u+AAR2/HHlSxp0LqEcqAitNgIh0P/UWkFYOj0JGWm+jVikIZAjnEaYxWFHj2a5sxbP7baGjSHrncGHy8V7EHvdgezW0cTKolk/8tcfmwcHwe3r44H01tHMc/uQ/Vt1+HQCs1TmQ1nlLtprkIuFS36CmFLd3OBEGIOeVOMHc+Lh/mxqYZhRhQ6sD5r3oxvrYQgQ4v/Jzjtz88HinJprgpVQZUGngP3dPt54WG2xApyOZl1hzasCb6zMzskUOcxmhFgX1+jiavuBFCVomYPWEQdkFsxMrYHgQ4wpzmcf1zsOWaApw+IAfz1uzG+Jc6sevXOMyzJ9KDwecAQ6dKtYlGBHxSF/rkxeLtM0jPkiCShkINYECpEzV/PBwVvz0RX2/Zil3P34D2Xd8KgyHyTWmjIqCyLjBMV4GiGC1YFxhmWrqNSBGsZuW0dK0zLLNHDnEa0yiIAos4z/0x3mEzgapiVH4wAW19TxVmuvzBj0aj6s6/R74DKy/w4C9n9MCHP3Sg/MkWrN5usROZyAy2vCxFGfQkfZTIEQWt7TNMz5IgAHFJmy3opa+DdcKoasLvOx/F7v/vT/jNhX8B7+zAT/9zK/60Pgd+LS83eFMqB04mOdbjfOdHcOgkdhp5d5zi2ARGc5fjh6vAXODBgtQeAMkmmy2ByIJBS+QQpxmyge07d42lsq3LCz9HtesZ5Ht3QU539G98XZjAdgTrxhZ0TkErD6//9CIXu4ZOw8GXPorefQ7HpJe9uGZNG3Z25JvXsiTSn46WyIiBHsypvT1zZo30D5E9GDU2x4SF9LUz6KnmHTYEh17xGPIHnojq9/Zh7Itt2PnfrgxfK3dj+X8Ho/X+o7C+bTI25M7CXTkvRvSZKGnlbizonCKOOBP2MPQPwF37IC45Y9JAlGjC9GZLIGhSnTkYY88yxn5hjKWfqGgSsRo9UBpYK5QVezDH9Qpy/G3hT/i8wrtOuZxiVWAM5vqmhU2sm+ubhlWBMXD16IMJ97yGg0aegyfrOnDEYhc2/2QyWkhkFy6POJIsd7+bwe6mjgxvEhFBNjv+GDU2izB1XdBJX6v3H3VEl8qLM68QPc6+Dd1PvwGf7mIof7IVNds6sTPQA6/6f4fTA+8h37sLDBxlbA9KHQc018g5wq4F6kwiYTPbVgL394Owe5Ex/emgZtErgaBJdaZ5HsBpNh0rK4gmeqBlYJUwBrhUuS2Py4mTj+qJvNafNPfh3I9OZ174gy4PHmNTQz+uCozBmI5HQhPrAGC9exb+nTsV122dgstOHYSDp9yDQHsLRi1pwaJP2hGggjICQNhEo6I+2psU9TbnmNrd1JEFTSI6PA+y2XFFVNKmV+qmdV248ZVNGH732+HXBkGamjfvjNj/yx+bMfDgAkxyrMd69yx8n3cRNlWsxsXTrkB76QBc8Forjqrtj5P9H0dEg0XxSA4pewhI14JebA/VEMcT7z796X88INmugeNjP5fys6W0y+vulnpGMnhSnS0OMef8IwAkOWABq9GD2voG48gwBxaePxRlxR4wSJHh+yYPwfvf7EYj7665S0PwLr/VcyjkD/kXQ+bjpbZRmttPcqyP0C6udj2DC/q34NA/PoqyIwbg5rfbcfrSVmq4y3aK+kjT6m76SjKaogjDwPHmHFO7mzqyoElEBNns+BM25t7E44A46LG/1RceMBFEAhsC3TWvK4P3vh1htx/t8Rouv+hsHHTiFLRsfQenLv4FdY2R59ZydB0MWOR6HA+4FoeOyVh0GXvCJnxeYPPLsR9H/mxpBQw2L5PsuNKuZxAJqyFmjE1njNUxxup2796dqNOmLFaiB3LUwIhexR5UDi/Dhrlj8X31RGyYOxaVw8vQ2OTVrAWW679e6/gtxvHHUXv2Nhy9byHO/1icdtHSLs5nHZiTUwNnfhH85yzCyadNwP/+x08Nd+lGv5OAiivtOZZWKk3ufveUdj3mawPqlphzTO1u6siCJpFYIJsdG7MnDILHFV6S5nE5MXvCIOE+etHjsICJxs2lbM+1uNX5iqbdnpe7HCW/uxSvX9odbZ0cJy5pQfX6du2GOxVOBrhZ+HhoaqxLMr6W2I8h2+0sDBgkzCHmnC/mnFdwzit69uyZqNOmLFaiB/NXb9MtlQD0DW1xvku3FhjoSs15ffpRXVHzRC+2FwDAGMO/h16PdVf1RFk3hkkvezFzjRetPgodpDz7/g0cNgoxmwW5SQ7QLoPoVBpZnc+F2jG1u6kjC5pEYoFsdmxUDi/DuSPKQk1tTsZw7oiy0CQ4LfSix4DCYVZIqwEMP6FnmD1XU2Zgt8/q68PmGYU456iuCXdywx05udkE64r6ZmHAICfZC8hWZk8YhHkrtoY5unK97+jq99DY5EWvYg9OPqon9rfqR1nLgtstXLsdN76yCU7G4OccZcUezJ4wCG3Bc6wKjMGqDm2DaZZG3gO9NYyrsiRjkmM9juvZic+mFeBP77XjwU868OF//Fg22YOhh5iQjiGSQ/PO4N1/jKUuIy6X/l09qyvCIJdB5HjMK1OoHdNT7gw/JhBbU4fdxyPSltr6Bixcuz1kd2dPGKTruJo95vKNDfAH6wj8nGP5xgZUHF4qPLbWdUEJBzC6+r3g+qaEnJcTdVSHJjnWC4uBlXa7xMPwynkenL7Jh+vfakP5Ewfw9FkenHuMy9TrJVIY5pDqWYp6S/at/n+A7z+M3K7f77r+X9Q7WC6hIoMDBiS7liQqh5fhvslDwup9zx1RhuUbG8IaIpZ++qPwGHLkYVezF//z6Y+hGmPZADc0eXFzjXHU1wp6pRcyc3JqkMv8yM1heCA44W6/l+P4Z6jhLrXh2gbQKnVLgJUztNNteo0hSvRKLuxq6rD7eERaEi95NFGfyC01m4XHlq8LHpf40qxeX219Axw6YdzbXDWa/nCAdzXG7UchACnD98fhbtRfXYABpQ6c96oX01Z5caAjOpvt55ITTyY/ibg8wIg/Bh3cYNDjJ0EJ5r5/d/0/C1Ql1DBuwyeVMfYSgN8D6AHgZwB3cc6XiLavqKjgdXV1MZ830xhd/Z5lSbVEM8mxHnflvIhSJsnx7EchqnyXhqXq/p07NULIfU9rAFeuasOq7Z0Y39+J58/24NBudD9GaFDURzK6KeqYMsY2cs4rkr2OWCCb3YXI7pYVe7Bh7tioj9tPJ2rrcTlx3+QhwkixmWuBnAHUiyg7HQzfui7UHKwR4MAR7csASHb9AdfisJpgn5+j6oN23Le+AwNKHVh2rgcVvbQzfPLrVJ6mlbtDZRzzc57Fpc53qfwiURT1kZzfot5S0/LmZSYzc0xqmJPZUiM50PKxUtguG2HGbttSMsE5v9CO42Q6tfUNmL96W6gEotjjQtWkwSGjaHXyXKKRFSaUzRl5PFK0Xausoke+A7UXePDURh9uXtuG8idb8OykPJw1iNJxRBCXhyK0CYJsdhfRyKOZoVexR+jUyg1yIofYzLkbm7yGUpyBABeWue3nhSHJtEbeAy/5f49THJvQi+3Ff1khujkP4M+n5GF8/xxcvNKLE5e04O7f52LOaDecKg+bAWjnTrTAgxIcgB8O5EFqth4R+BfOd35EznCiKOojKUDILDo2+jK18ilZZY8pRJcgausbMPu1zWH1wE1eH25+ZVMo9WXUUJFsRAoTf3U9jvXuWVKtGqQ0XAePvNdijGFGhRsbpxeEGu6upYa77EYeDEPlCkSSiEYezQxaKhNK9JxeM+cuzncZRpFlvWB1mVs7d6IbawuTYTvf+REeDFyAVZXb8MHZn+G2wLXYGyjE7w7PwearCzHxqDzc/l57xIQ7mVwmOeZeuJHDAqHjXup8V3faHWEzrfukIR5VRcD8UvNlcBleDmEGW0omrJLJ6TcReikwBmDRBcMAQDf9lSwmOdZjTk4Nytge3bt8ZYpsq/uP6OZoF27b3slDDXdH93DgpXOp4S7rSNOIcCaUTFglk222XEOsbnDWK2mwcuxbajaH+jqU6JVkaK0pFmQb3ovtRSPvjnzWFip7U7Iz0APj+WNo1eg74ZyjZeu7OPDuY8h3+jUb7jgnVYr0g6V9OYQZzNhtcogThF49GSA5xRxASb7LUFXCCPlYdqBVJqHHzkAPjOl4BN/nTjVlGN/5rhOX1Xqx18tRfUoubhjl1m0QIdIIl0dSldBrpFOn99IAcogzDyOViVhUKLScW9lGy3XAACKOr3ysyONCS7sPdvVHa/V5AECAMxzRvlR337//eg1uqW3AF40BXDnchYdPy0Ohm2x2WsKc0pQ7kUOcqjXEUawrYTXEhDF69WRAlwMbqzP8cDDSfOMrm2I6joxWmYQesq6lWcb1z8GWawpw5ao23Px2O/7xXSdeqPTgkEKq5klL1PI+QKS0mZIM1rQk0ofK4WJ9YLVDK6s8yPuZOTYgObcNTd6wgEVDkxezX9sMcMAX6FIHuumVTbho1GGhCHJtfYNtNh0wJ5+pJpQp7NGMDVcU4K4P2lG9vgMf/cev23BHpDA8eJMmy2ICXY6lPKlOLZ2p3CYZxHFdFCE2iaghDoi8s1cbydr6BlSt2oYmb3yntrkcwMEH6TveZuhKr+0Bg3YKTJQakyPEX+ZO10zJifbnnIca7grdDM+enYczj6SGu7Skqjn85y01kgwb10j/UoQ4LUhHm20XdqpQRKMkVFbsQVNrB1o67Cul08r8ySVvAEL2v5H3CEmzaWUKP/ihE5es9OKnAxz3npyL2aMpw5fSyBFh5jC2x4uOFegQJ9lmR7kuihDbhNwQ5/N33TzIDXFOJws9rhU5sLsWrKzYg/0t7Zo1Xr4AbHGGzZRItCAPBbxN5dQC6wLDQs11evVk6sflhrvfHebE1BVenPWSFzMrOrFwfB7yXWRg04otNeF36vL/aQgGkYbYpUJRW98QlX2OhxTnqsAYwIewumItx7c324Nq1zNog1vzmvD7vjnYPKMQV7/hxdx17Vj7XSdePMeD3gdRhi8l4f6gJJug0U6ZsUvVSXVxXBd9ak2wcO32MGdYJgBEPB42bx7a4uxmULuAYfqONg7aUGOmRKKVu9HOczScWuAs56eodj2DUnYg9LyVJMQxBzvx2bQC3DzKjcfrfKhY3ILNP6VWkyFhgNasexqCQaQpZlUoausbMLr6PfSbuwajq98LG74hB0ZSiVWBMRjT8QiOaF+KMR2PYFVgjFBJqATibF+ph6HmPA+enZSHzxv8KH/iAJZ/Hd9sKBEDzTsgHF2olF1L1dH2cVwXOcQmsBoJUG4fjZalx+XERaMOQ1nQ4KprzuIZL+0lmHnPufRnHy/EXN80lAjKIUpwIMKgWs2g5eYwPDhBmnDX1EYT7lIKZqJOUHSnXj5FSmlVNUn/kjNMpAFa8mkelzPU+AYYT7uLNjCSaET23wjlhLv+wQl3V63yoiXKCXdENDiAyU8Hgw5GcEQ4xeqM3cDx2ruKHk8UcZygRw6xAUZjMbVQRg5E0QWn4JhOxnDf5CG4t3IINswdi7JiT4RiRDxNTEDwkWBM+pMHydlt5D2E22kRjS8rN9ydNiAHN7/djjOWtuKnA/GLjhMm0Ko7U+Mpif86CCJByOOUy4o9YJDK1tSSbKIxzXK2MNWHLsmI7Po+XhihZazFwO5OfHxFAeaNcWNJvQ/HLW5BXUPq3whkBM5gBaxpx5DrZ+y+fVt7N9HjiSKO2UaqIdZBvuvX0pAUwYCwyIHWaE2Py4lzR5Rh+caGCCkeP+chI1o5vEzXkOa7HLaXTzihf7x81oEq14uhiLHZe4V9vBBenoeyYATC7H7qCXdDnmjBc9Rwl9p0HIisIyaINEZPhQIwrjM2UhlKFRZ0TtFstpvfeSkAoMr1IkpwQNd+u5wMfwlOuLtkpRcnPtuCe06WJtxRw10c8XdI5Wo3fQW8dZu+3CVg3ByXqjXEQNwm6FGEWIdo0lwc4Q116mM4GcO5I8pwb+WQUNQBiCyLmP3qZgy/+23daLDXF4DHZe+vsEEQIVBSggPo7tA3iko4R0hx4gbfTMtrUk+4O+slmnCX0siGeUuN1BFcVSz9u6Um2SsjCMsY1QYPmy+203KGcPaEQXA57XEG3TYdR4tVgTGY65uGnYEeCHCGnYEeoWFLqwJjcFz7Ytzgmxl6fm+gEAd4bihAokRuuKsclIN569px6outmhPuCJN4So3LIZp3SPYWAJwGEX2jSHKq1hDHEZJd00E0TINBfMcvS/HoqUuopyBFI8UTL6wO4rBKK3ejDW5dSTY92js55q1rx6JPacJdyuPyRKpKZEAjHcmuZQ96U+wAYParm0P6wWrUdv6Y//dWXBuik4GZ6wXnHM9t8mHWW21wO6E54Y4wgcMFHHcp8OWLQMBE06LDJZW4cY3PnKcUuO17/f3Ver9AWttwM3abIsQ6iOp/i/NdaGnvjHhc2Wgxf/U2YXRZrUSRKs4w0BUh6OTaH41Y75/yWYdmNMEsuTkMD03Iw9qL87GfGu5SG/UwDp9XW4GCIJKMKAqsVxu8cO12oTMs94IoyywyzRkGjFWJpLI6his0Gu4OUMOdNQI+YONz5pxhefu8Yu0GtNPvN94/C5WBqIZYB636X5eT4UBbZ4QhLMl34a6zBqNyeBlq6xsMJ84pa84cDBDY1aQga1Sq7/wDHGjhuejG2mM6fglrwT5eiO5RRokBYHz/HGyZUYBpq6UJd2u/68TzNOEu9UmF+jOCUKA1ie6mVzbpToYzUvsJcG56tHO6MMmxPlRDDEh9IVYyfXLDnTzh7kOacGcdrWivHnIdMXN07ZujHejTJE61uqkKeQ86aHUXF7hzNKMC//V2RYyV0V8RyuhzKjnDMlq1ZDf6ZuJPnVcKu43NRn4ZOApYe8yvu2eB1HD3xMQ8fPQfP8qfaMEb/xLfiFAQOQXI4PozIj3RigIbmQonY8IMIiDZd3XU2e5+j0QyybEeD7gWh/TlGQO6Ow6YUjxSXi/khrv3LsuHt5PjxCUtuH89ZfjijtKR9u6TSiGopyMCqiG2iKiuWKbY4zIc0ayuLes7d42NK4w/oZn2bA/8cMCBQGjEZ5Xrxajrg2Phn7v9uHC5F5t/DuDakS4sHJcHD024Sz0mP532EQeqIc4sjGy6iIcvGKZZQ+xyMlwwsg9e+WJH2OAmp4PBn4rRDxOsd89Cb4e2RnGAS1lOLTq5Azf7ZoRGQTN0KQzt83Jc/YYXr33diZP7OmnCXaKJZgTzlhqp7K15pxTcOOXOtLHnNLo5DhjJ5zR5fWGKEWrKij2YPWFQWqfTVgXGYFXHGO0nNUotEsHRPaUJd7eva8dDn3bg/R/8eOlcD8p/Q+m4xKD3qSeI1CUaSbSyYk/Ihlet2hYKgsilc/NXb4uYYpquzjBgPLBDJMHpQCB0vZjkWI+/uJaggEsldyV50oS75zf5cP1bbSh/4gCemeTB5KOp4S4hWC1fe+NmoO5ZhOx88w4p0gykjVNsBN2OWURrapEajRkw8LicePiCYdgwdywAhKXS0jiTFoFRU148kSfcrb04H/u8HCOfpoa7xMG7ptgxnd89NdURKYYZm65E2TxdObwMm+4ajx+qJ+KH6omov3M8KoeXGfaQpBuigR3yc3pynZMc6zHJsR4LXU+hkLWHSi4kBzp8wt25NV5Mowl35tGztUZYKV/bUhPuDMtkWKN0BrliiUGuKxZNmpPhgOZkI60Rn5nQfDzJsR7r3bPw79ypmJNTg6X+sejgyUlAyA13E/rThLvEwbqm2Ok1flBTHZFiKHtFtHAwKfIr2/JzR5Rh4drtmrrEmcqCzima9rydO7GgcwrWBYZp9mg4mKREMSenBrksUnVJvowqJ9w9K0+4a+zaXn3sWJSK4oqZ0fZ24CmVys/OeUqgN+zQ1yG2Oup43d0QZgAzyKaTQxwFlcPL8OCUobpRBSdjaGzyopeqRCJdZtpbQdai7O3YAwcDejv24HznR2jnyStX6FngwOt/MN9wF29S0njbiskXSGOdiRSkcngZNswdi4cvGAaXqiDWyRjuOmswvq+eiNkTBmH5xoawgMa8FVtRW98Q1kSXrt0LysDGevcsTHKsByBl/m71Tcc+XhhyRvcGCjHbdzUA4HznR8JBTb3YXt2SC9k2KhvuWn2RDXe/BnJD23ZFmFOMvoJSQrtxF3SVKbgLw5/zlAKTnwLO/luXZJqnVPoTrXyantObQY3S1FQXA7X1DZi/epthekzZRBdtA0cqI2q4ENWVcQ60IA/tPAclrAUAFzZl2MHXu/2YSg138cFTajwiVInTLRnqNK45o6a6zEU0JEkeuCR6viTfhTZfIK2DHVpDNlq5OzSpTmt7ublazzndGyiEF3nCprw2noNcdIYdQ6vhrqybIzWdYEjhgP8W9EdRTodUW5sIJj+duMEZi44VvC4GTF6cFvacBnPEmcrhZai/czwuHnWYbgmFchCHnlRPumLUcKHF7b4rMKJjMY5oXxr3aMoxwYa7m0a58bcvfKh4ugVbfk7fC1fc8ZQab1PUB6hqlqYdicaJatW3yWOdCSIFaRQ018mPi57f3+pLa2cY0B6ykc86MCcnUp5LmRU0clIZk0outDKGnZzBgcjASalHarhbMikPnzf4Uf7EAaz8JnXrshmAgw58B26HM+wqkCLARmd867bEDT865c7IAR9gQMUVaeEMm4Uc4hiprW/A8o0N8BtE2huavCk1otkqolQaoN9woQUL1pUlEuWEO7nh7uFPqeEugqI+kpNb1Sz9Ed2uKFNoWsbS5RHXEmdQzRmRWYgCFvLjmRjQkBEFNnqxvRGPGU2oU1KMFqwKjMFs39XYG+gqueBcsi5uFjn1FYiccHdujTThLlUb7hgTWktr+FqAjlaDjbg4MxcP+6o1tW7yYuDMh+w/VxIhhzhGzNYEM6TWiGYraNUIV7ueCTnFCzqnRAzraOVu7OOFWocDEG5kD/Dc+CxcA2XD3U1rqeEuHBbZaCGqD1M+LhrxKYocZ1DNGZF+iMY0A9qKE0pVidkTBsHlDHd7XE6Wsql8K4gCG428e8RjVrKC8v6rAmMwv/NSeOEO1QA7mbFzq2y4W6LRcNeZqg12MRHDC4qXfS2fIukWVzVJ/2ZQZFiGHOIYEaXQ1KTz99UolaY11W6ubxrmd14qNFRN6EoJ+VhidSflhrvHz8jDh//xY0iSG+5SBx5p5ETRX7XjrGUsze5LEAlCS+VHbooDpDK4c0eUhUrgnIzh3BFlIYWgP63cGqEvjAxxyESBjQWdkY6P2aygen8rkWUlyoa7lg6p4a56fTua/W78F4VxvCFh0p9EqUdYwVNK9tVmyCGOkUxOocmYSaWtCozBmI5HcET7UozpeEQSYw+MQQu0o7/KC0gxEj/ZjjGGa0a6sXF6Acq6MZz1khfXvemF15cBV7Zo0YroiqK/ZqIDsexLEHFAK6On7PFQl8D5OcfyjQ24o3YrZr+2GS0dkdlAX4AbynCmA6LAhlZDnZbzrIRzYB8vjNg/mn4TJb/vm4Mt1xTi7EE5mLeuHZP+pxkt//1vTMfUw89ypDrZvKK4nSMqXB7g9Psj7evQqVINcVWx1AhH45ktQZPqNKitb8DCtds1ZdPUzJ4wCPNWbE37hgqgq2u4F9sTGsW8KjAGjbwHemsYMq1Umpp8tGs+LqlLyMfRPn4ikBvu5q1rx6Jsn3DX0SIZULXDWj4leic2ln0JwmaMmuZEDvNLn+3Q7RPxcw6Py5n21wHdKaSq7eBD8HqxF00oAOeSXW/k3UPXDjV22PpSD8Or53vw3CYfZr3VhqFPduKZSXlxmXDn5D7wuiW6tcE8OIkrrrdEnlKp0U5rZLL875aacNWJDJwkF29Idk2BSEZNKZsm2k92oNM1vqgnuQNEjmPWk+NRIpJk2xnogTEdjwjPrUb5MY1XMObt7zpxWa0X+70c95+ai+tPcMORAZGfEMwRrN0xqJn2lErRBzKimpDsWnqgFdhYuHa7Zi9HsceFgtycqPs8ygyOn80oAy37eSFK2QHbbPi3e/2YusKLusYApg134eHT8lDgTrDNLuoDDBwP1C2xvi9zdg00EmFWSk0kjVbURypjy3JIds0AZXPF8LvfxuxXN2tqCitTalrIou6LLhiWtqkzvTphK6k0NWbq0tTHD2jcVTAG7Ech/HH8yMoNd+P75+DGTGq4c3kkzcqDymDoDANS9/LqWZRuI9IWUa3wyUf1jGiaczkYWjo6Y3JkTz6qZ+g6IJp4l6noKRCpG7K7Ow7YGjQa2N2J9X8swA2/LQw13G1sNI7Sh5QuYl2AXLN75kNAxZXW9+cB6MaWrZSZidQlSNXHNFnrEKsN5v5WH3xanliQhiav7qhO+XhG8mupilGdsFaNsBmMnGnZmD7sehwA8Hf/KULzUIIDWOofG7cGFs67Gu7+Fmy4K3+iBWvSqeHOVSA5v1p1u1YMY6x6lltqpIgF1bIRSUBU+vD+N7tDY5rlUcyFeTmRjXIWWb6xIXRdmD1hUMS0u0xB7fzOz3lWV4FIK9DiYNAMegDS4wEePpVOpp07NcdH5+YwPDzOgXcv6Wq4W7BBX1JTVrkw+i3puASRzmo0TnFRbx0lnz7W1BzMKAIRumRtyUQsmsByCQWAUErOwVjaOsOAudIGu9EqlQhwCKfWcQ7c4JuJEY5/4SLne3AiEFe5o693+3Hhci+2/BzAzJEuPGAw4U40mU/5vA85Qt1N26hq1n5cOG1Ih6I+2nVreqhr2YDItN+WGsnhtnrsFIBKJlIf0URQBuD76ommtrUKY12lXfkuBzgAry8DMkxBrNhr+brx79ypms8HOELlEzL7UYgq36VhwZK7cl4MbbMfhVjtH4VTHJuE0/H2eTmmr/Zi+T87MbafEy9WelB2kPW4H+dd57sodz1y/G1dTxqVMGypkYZmGE3wlI8D2DNxzozdzWKoZEIHs3JpWnh9flSt2hYWYU5nZxiwJrljF6LogQjGEIokD2j/H/xvYHBc5Y6O6enE58EJd4+bmHBnxjl/yf/75Ek0aU4bMqB5BwDe1aBhJtK77m79CUqy4Y7m2ARhAqMBG2a2tYrye93qC6DTzzG6v4mpj2mCFXstZxZF8mz7eSE8rKMrUsuAPET2kCi3KWUHcL7zIyzonCK8gSnJkxrulkzKw2c7/Sh/sgUr/mk9w8cY0MrzsMh1NXLOftSaUk75lK7hRnoxaPk4dqnxkKpPzFCEmAjR1fywV7dT2C5E0QMjAhz4u/9UXOJ8N6r9o0HdcDfrBDdYFOFp+esWt8i2uwC4vVH8vDoyO3A8UPcsTFfTMSdwzpP6RraqWHA8JukUp3nzB0WIUx+5hE1ZNiFqjtbalojEir2WI8SiZu02uMOiw+r9AP2sJQDN55T8a68fU5d7sXFXdA13nAN1IxZg5KSrjTfWyngBwMoZ2k1zaWLrMgmKEOugNZHI5WQo9rhCtWUPXzAspgYJl4NFTDVKZaKtE44WqyOfZRwMuMj5XsKcYaCr4W6c3HC3LLqGOznaETc6WoD7+0VGW+Wa3hXTpZ8nL5YMstXRm9xvHM01qmWj5g8izlQOL4uoFRYpBam3JbQR2Wt1na0ysyjqIRFpzyu17fX6Wox0kAHgyO5OfHxlAeaOdltquJNhDBi59a4IW6eedPjFqqciM161M4HXr9V2hml4RsqStRFiwJzesCjSkOdyaCpSOBlDgHP0KvZgf0s7WjOohsxujOTW9Gpyo33OClrRXM45nqjz4Za329DNzfDs2Xk488jETtozhbJ2zKi2LNraYlGEI9rzpUnUhCLEmYMVabZsRxTtfdX/O5zi2GQps6gX/V3QOQVzcmqEdcLK6PNfXY+bsvXvf9+JS1Z68UsLx71jc3HLiZKkpqnrhMIuafkDG3JnocystrKZDBsRF8zYbVscYsbYaQD+CsAJ4BnOebXe9ulmXLWMJgBhSg4AqlZtQ5M3jdQJkoSeUZNlcbQiwZ3cgRwWebMR4EATCjXTcXby9W4pHbf55wCuHenCQoOGu6QgG3IjB1TLgTUkWP4gQq9pLs2bPzLFIbZit9PNZgPGAQ9RsOPcEWVYvrGBSig0sKusTs+5Pt/5kTBIota/FznWWigb7noffjjGnXUmHit9xXCUdAAMq87ehsrhZWGllvJ7IXLctTGwm0TcSIhDzBhzAvgXgHEAdgL4AsCFnPOvRfuko3HVwqyjTOijFy1YFxgWUSssMpzyR3kfL0Q31hZ3NYf2Th6acDe4pwPLEjrhjgGeEoNO5qDxNarpBRQO7A7pcaOa4lijuaQykVSs2u10s9l6zu773+zWVQaSh2xQUCO+aDnXc3JqNK8FnAMNiumpymM84Fps2tZzzvFMfSdm/qMTAWceTjvjVDw1eDN6sb0IgGkGWXYGemAc/xvumzwEN72yCRzmhklpkiZZsEwkUQ7xiQCqOOcTgj/PAwDO+X2ifdLNuFqBmvWsozclb1VgjDAqoZyABIRHktu5Ey3woBgtQkNnF3Y13Flm8tP6kV2zEWI1SmfVUwK0/woEFI5BGkVz40GGOMSW7Ha62WyRHTZxqxeSZrNqy8uKPdgwdyz6zl1jaa1EF2KZNoYj2pdq7rPRPR3dHdYygpv3ODF+eS5++eknXDi8G56aAPjc3VDIvHCj6yZKeR2S+4kamryWItMhstxuJptENdWVAVBebXcGH1MvZjpjrI4xVrd7924bTpuaxCLnlq0YDe8QNfvJjzfyHhFGNJf50crzcET7UtzsmxHRgCESebcK5xoT7qJsuLNEUZ8umR2PhrSTsnFDS25Nr7GjfIrkKFc1SfJBlY+TlE/mYWi309lmi+ywmfCPLMNm1ZY3NnvJGdZBb6KdjKhxr5F3Fx63xKA8TivmN7SHH5uuYLhltAcv1/+Kiqdb8P2uZoAz7A0UIsClsjwPpGmtkxzr0djkDTXjixr+hCjspropT2vQF5EcEqYywTlfzDmv4JxX9OzZM1GnTThGmpZuJ6NOZgXqSXU3+q7RVLgQGdNJjvXChgbllD21wz3bdzVu9U3HzkCPqHWBO3hO6AKrnHD3wQ/GE+44B5rQTZpsZFUbGOhyZmXNS9F0OpkcxTk8pdacWqWDbGVyEpHWpLPNLvJE1+jKgFDpm5EtL3CHl0eluRR9XFGPcFZPtJOJRg9f5ERzLpU7iH4th+S04IFTXVh3adeEu4c/bgFjQBvcyGHS4Cd5rZcVfh5SJPmFWfg+KCbOiUaKk1OcGtjhEDcA6KP4uXfwsaxES85NSYef2zrLPZ0xayRF28ljQ0XVCcqoglaUWX7Mb/FrwLlUp8zBwyLTjDHMHOnGxukF6NWN4cyXvLjuTS+8vvDfeCA4BakIB4Bv3waGTg06s4DxMFFIDq3aKRU5rXIDm7LWuJOyGERm2+1oK5Y4EGq8mz1hkHiMfL4Lxfn6sl9EF1pDPfKZFH1VYpQt1ELkRN/gmxnKIOpxcr8cbLmmEJMG5eC2d9tx/t9/xv5f28K2yWcdmON6BYD0+Thk8l8EgQzVtUSVidMaKT7O/yFGvX5Scsbcy3KcyTh3CmKHQ/wFgIGMsX6MMTeAPwBYZcNx0wJ1+gMA7ps8BM5E1JCmOWaNpGi7i5zv6XYjm52y54C18gYOhlaeh1ym3Th5TE8nPgtOuPvbFz6MVE24k6cuMVmzcvMyyWhWNUv6wEzna+nyAKffb36xRlPjiGwlo+12k4YkphmUuvOVw8uEwYv9rT7qFbGAWFM48nGrevhGTrSWw+xFLvajMPRzqUeacPfMWXn4NDjhbqVqwl2+96euH8qnSIEM9S2TMydYwqadqVOX4cjBnkOwGwmf2kkTQyOI2SHmnHcCuA7AWgD/BFDDOd8W63HTAVH6AwAenDJUN1JM6Auvm9nOKXBkOYdhVEGJ1QEhjby7YQ1Zbg7DQxPy8I+L8rGnleP4p1vw10/bwTmPjDrJDqrczMZFDjqzVuqwpUasL0xDMLKaTLfbonKHYo8r5PSqv4celzNULiFDgQ170LOxWrXEVjFyor3cLcl4cgCeUnx13D34C788zFFmjOGykQfhs2sPwRElDJNrvJi+2ouWjuBtkXrg0LdvI6Iq3d8hTQvVytQtOhbf5V0UVvKnFexJWMCCgiUR2FJDzDl/k3N+JOe8P+f8z3YcMx3QSn94fX4sXLsdlcPLcO6IyKlIRBdmGyhE24lKHRp4D0vamFoRhA6eg18DuRF1gR08Bws6p5hyotu5Eyf3z8PWa8In3P2s1XAn350bDciw4gyvniV+XjRNjsgaMtlua5WueVxOVE0ajA1zx+KH6olYFJxEqjfJTkuWjbDOgs4pERPtAEkZSJ0RtBM5AtvdcaBrSminFyP7lmDMOTOxwDVTiiyDodVzKOqH3o3nu12Bd/5YgttGu/HMlz6MWNyCz392hkof5KxwoMkg2BAqRyiSJoQ274ADPKw0UBhYSUTAgiaGRpC1o5vtQNSF3NDkRW19A9Zs2ZXgFaUXZhsoRNst9Y/VfHxdYJhhN7MSrZTbrb7p+FPnleiAqnEmGBHQWlM7d2IfL4xo3Gv3HIzaC/Lx59O744MfAhii1XDHnMaDMaw4sVp3/zI0OpTIcMyMbq4cXoYNc8fi++qJ2DB3rOZY5zKDxjrCHKsCY4T12OqMoJ3oRWArh5eh6o756H33d3BUNSH/tm9w49cD8VrHb3Fn4CpcN7Y33rmkAM0dDCc+3YwF//gBKzbuCGWFhUGRot6qcgRAHUmWSwOFzXmJCFiIzpHFwZLYdafSGDOjm/XoVewR1pHRcA5jVgXGAD5EaAwD0rCOXmwPGoNi7HN90zS1iDcGjgx7fF1gWNjAjt5MuhuHD7pR41WBMVjVEf78evesiDrhXObHnJwajOl4RHPt8jmUGsmNvAdu6pyJVeVjsPGq7Zg68zap4W5kJxaMy4MnP9/clLiOFsnQmokS693lk2wakQVUDi+zZM+1mD1hENlym2jgPdBbIyKqJ6kWK1YjsHKQK3Q96AXk/PFX5P3jUdx2220o6v8yCifcgJxu0nUpYjiHHGzQC0gE6e3YK/WMaE3tTETA4pQ7k3fuFCVrHWL1JCNl/a9ZI6pnLMmAmkPtiKqHdMgO7VzfNMkJNdh/vXuWsFFP7fDqYVbOTeuYotdQ6nLjmHPm47M+v8G8W2/Aog/34v0dDix79P+hfMcLxuUS3n1dZRBGDm1Rb/EwDnKGCcIU8rVg4drt1EQXI1oOpJXm52hoFDjhoiioVpDL6emGoZffjSsP/gFXXXMtfn32enQ//XqsOjI8oOMoVkzcXDHdeHFFvbtscTKmdibz3ClKzJPqoiEVph6JphDJ04bMUlvfgBtf2WTjyrIbvTHOWg6xGtGkI86Bfu3LTK3BaCynvBYnYxh1RAk2fBc+Pln0Glo9hyL/tm9CP69duxaXXXYZmpqacP8Nf8D1hW/B4W+L2C8CM+M/5ZSd+u6fosMxkwmT6qySCjY72YiuGU7BCGgiEtHU0XieTzOKO3Sq1BSncgS1Rn6f5/4YdxcsR773J2zYexAmv+bDLz/9hMKhE1Ay9io43HmRfoNoOqhyDWSLE0qiJtWlJaL6X6vTiSqHl1GdmY2YVZ4QIRRph/luZs26syDKiIafcyy96kRcPOqwsPo40WvI8/4UJsA+YcIEbN26FePGjcONC17AxLW9zE24M9P0IE+xowlzBGELoka9C0/oE/G41k05YV1SzY7zKftDGngPfNfrbEnqUkNuTF17fnnh55JD7d0FgGN092b860ofjhs1Cgc2v41dL9wItuf7CHUSzemg8lUi2baYtIeFpGXJRKy1v4C4/tdoOpEWJx/VE//z6Y+W9yMiEaW4zNaZLeicgoddj0dckORuZjNlEyKHVi3nJt8I3Vs5BPdWDglFF0SvoYkXRJTl9OzZE6tWrcITTzyBW265BeWbfXjuLDcmHqkzacts00P5FHKACcImlOUT6mtPxeGlEY/L21ottWAwN2Ka0KbY44LPH0BLhxTlVZe2ffqfGwAI5MbKp4TXni+6DWgOz9oV5fiwZsJuDOv7Z+x/8yHsfOFmfDvIi8DQW+BwBGOMqVqOoM4cyjcDQPLXlgKkXcmEVkrD43JqSuYk4jiAOJVGWEcrxdXK3ZZ0hUVlEwHOcET7UsP9zZRteFxOnDuiDO9/szviIrhpzWLc7nsMbtYZtn87d2K272psPGicZlnOtm3bMHXyGdjyrx9x3UiX1HDnUr0QSrUlHSqZIKzQb+4aSw5usceFJm90g0UIYHT/Unz5Y7Owj0d0fQCYpB+spKoY2rcn0rb79u3D9OnTsXz5cowdOxYvvvgiyspSWG5VVMphpgwvzcnIkgk97V8rmJHlMYvVMgtCTDSjO9WY1TcWYSQH52QMxx1WhOUbGyKGstT9Zx/eyTkJv/K8iOPKChWiz8vgwYPx2ebtuOniiXhMnnDXVKg7+YggiOSgnlIql0OpH893mx/Q5HE5yBmOkU//vV+3qd2S1JmBNFlpaSleffVVPPPMM/j0009RXl6OlStXWl1y4iDtYV3SrmTCrtpfwLosj6hUozjfhf1RjgolIhGpN5hFq5u5g+fAgzb8O3dqSMpN5GSL5ODk7f2c4+Pv9kXEDbw+f6h0piT3gOaxe7G9umU5eXl5eGj2pZjgrsNlr/yC4x9rxP2nl2LWvU+BDb1Aeyd5wl0iU3PJOCdBxIFoSvBEKkV1/9mH5Rsbwh63gtdnbYw8EYleg6PH5cRHh12Ds/5TDY/i+uDlbnzV/3qMVO9gQpqMMYYrr7wSY8aMwUUXXYTJkydj+vTpeOihh1BQUGDTq7IJofpQ9moPK0m7CLHImYim9tcKojHNtfUNEdPMiOSijjIf4HnIQSe6Ow7AwRA2KUjvGHrNH0a/cr0odUQDhpItNcDr12JCH2/XhLs39uGM8y7Bz+8v1t4+0fPok3FOgogDenZdD1GmculnP5LkZoriZAwvjvwPfvfjE8hDBzq5AwEulcLd5puGG78eGLmThebkQYMG4eOPP8acOXPw9NNPY8SIEfjyyy/j/8KsoNXsl+Xaw0rSziEWdfrqOhk2oFeqQSmu1EN2aG/0XYN8tEXUjMnaxPFCVHbxcd+Z+tGndXcDfily0bPAgVV/8OBvZ+Thgx98GDJpJtasWRO5faLn0SfjnAQRB6ItwRNlJCk4kpp4XE68fOIOjNx6Fw7BbjAG5LAA2uAOZf+EWebyKVJ9bVWT9K9OJsztduP+++/Hu+++iwMHDmDUqFFYsGABAoEUifyT+pAuaecQ21n7awW9Mc1E6jInp0YogRTPkaFatdB38umYcsUt+juqarkYY5g50o26qwpwaAHHmWeeieuvvx5er1dze9FxbIXq0IgMIdoSvHhnJAn7kH2Ekd89GnEjrwyM2Pk7HTt2LDZv3oyzzjoLt912G8aNG4eGBv2sQ8Kw4OBnG2lXQwzYM5LTKnpjmonURTi6E/EdGQpE1kKX5OtIqckIarwGH+zEZ9MOwryWy/Hwww/jgw8+wLJlyzAkGTVhVIdGZAC19Q1wCIZqGDlHNNI5dSgzuDaHFH1e175h78X2RmaZbeiR6N69O1577TUsWbIEN9xwA8rLy7FkyRJUVlZaOg6RONIuQmw3ok5hNVqlGkTqI6rlDXDEdWSoFk1mGi91arnycgI46dJbcfTl9+Hr73di6HEjcMvXx4DnqBQt4l0TRnVoRJoj1w5rOcNmSvAqh5fh3BFlcDIp/eRkDPmurL+cJoWTj+oZ+j2oCXtccMP+C+sRnmW2sUeCMYZp06bhyy+/RL9+/XDOOefg6quvRktLi+VjEfEnq7/BVhoqlKUaZhB9QYnEolXLG+DA3/2nxmVKkt410VRKrnxKUGYtklbPoZi3YitafzMEvf74GPIOH4aHnnkVo1f2xM+OQwEwad8cD7BievymEFEdGpHmaNUOA5LdNlOCV1vfgOUbG0IOtZ9z+AJUQJwM3v9mt1BZIuxxwY38IZP/Ev77jkOPRFo03BHZ7RBbbaioHF6GDXPHGjrFonGeMuQqJw6tWt4bfTNxV+cVcTlfYZ4LLo2iZZeTmW/8PP1+TcO9wHdB6PPqLChGz3PvROm4Gfhsy7cY8tg+vNnzGqDTC3j3Ie7qD1SHRqQxohrhAOemyvG0rh0+P4fbyci+J5jGJq/wmhz2uNkb+Tj1SGg13C1cuDB1Gu6I7HaIo22oMCqfyHM5UHF4aVhEWY4YlxV7sOiCYfiheqLpaDMRG0YSanbS1OrDwvOHotjTVS9cku/CwvOGmq97FxjuFw4cH7YZYwzdjjsTh1z6EA455BBMvK4a17++D16fIipC6g8EEUGs8p2ia0SHn+OiUYeFmr5L8rVvkAnriJKusna0KfUpMzfyBsM4YkXZcDdnzhyMHz8+dRruspy0bKqLBi3xdVGjnJFRVM+0L/K40NLRCZ9fckT2t/pw0yubcNGowzRH9MqcfFTP0CAHIr1wMKn0Qk2vYo89TZ/lUyKMda83tUeE9x14NNb97XPM+30RHv6sAx/8x49lkz0Y8pvgBYLUHwgiDK2mOCvynXpN1u9/szvM7iuvPVRUET1FeS60dwY0f2fqa7LZASuamBjGEStyw92zzz6LWbNmUcNdipAVEWJRrfDJR/WMWtNYLp/4vnoiCnJzQs6wDAew9NMfdQXe3/9mdzQvh7AZBmB0/9KwBhk9PC4npp5wWMRnx+VgaO3oNGzQjBa9KEheXh4WXTAQb12Uj90tHCOfbsEjn7WDc07qDwShIlb5Tr1rhDp6rLxWUFYwepq9PuHvLJppg0Ks9EhsqZF6NaqKLfdsyBPu6uvrqeEuRWA8CUriFRUVvK6uLmHnG12tHVkrC35xYv0i9Zu7RnjnX+xxoSA3R/P4evsRicXjcoTGppbku9DRGUBLR2TTjYMBD00ZFmGE1VkCQHK0Obo+Z3ZIBeoa/mB39C9NLbhyVRve+FcnTj/SjeeefBS/OXl6zOcmJBhjGznnFcleRyJJtM2ON7E4UPK+ogixns2vrW/A7Nc2RwRQ9JDtSLZTVuzRzLjeUbsVSz/9Mew98ric8Z9PIKtRqCPJUTQYd3R04M4778SCBQtw5JFHYtmyZTjuuONsXnB2Y8ZuZ4VDLHI8GYDvqyfGfPzBd/5D03nSQvlFFTnqRGricjJhLbDR7zIhBhoI6Wfyph14YlsBbnljD7odVIznnnsOEyfG/lknyCFOd+SMoTr1Lvp+Kp3n4nwXDrR1ChUlXA4GMIQ5vPKxAWD+6m3Yb0Z+MUixx4WqSYN1HfBswOVgKMzLQVOrL+wmo7a+ATe9sknz+i5yoG1j0bECPfY+Un1yFLz//vu45JJL8Msvv+DPf/4zbrnlFjgcWZHIjztm7HZWvNOxNlDocUftVtPOMBCuYhHvcdOEfRg1xhk1YpoZB2sLwaYRNr8ZM19rRN3GehxyyCGRE+6sEENakCBSDSvqQupyu/2tPqEzXFbsQWFeZPmc1+fH/NXbMG/FVkvOMCBuJBNub23zlIdBuikAk957tTzqwrXbhdFzI5scM3FQozj55JOp4S6JZIVDbLoDNQpe+kzjDtEA+YtaObxMOL0sGGhAWbEnTLGASA757hzd6K6Zm6u4G2gNBg8ejM8//xw33ngjHnvsMRx//PHYunWr+QPYKFJPEKmAFXUhkV6xGgZpIppo+M7+Vl9UU+32t/pCDrkZOJAx14tij0vYoyPfwOjZ1LiP146TGoXccPf000/jk08+QXl5OWpra2M6JmGOrHCIY22g0EMkCA6IR/Uqv6h3nTVY01l/aMowfF89ERvmjkXVpMEZd+efbsQqxQckwEALyMvLw6JFi/DWW29h9+7dGDlyJB555BGYKpeKg0g9QSSTYhN2WcbsTay8r93fcSdjlhzpsmIPNt013tY1xJuSfJemI/JreyeGzX9beDMg12hrwZCADGwcJ3bShLvkkBUOMRDe6bth7ljbajn1RkbeddbgCA1KlyN8QIMZZ71yeBk1VSQZB2PoO3cN+s97E301VCTUkwzVnwq7MhKxcNppp2HLli0YN24cbrjhBkycOBE///yz/k5xEqkniGRQW9+AA22dEY+LBueYcXCV320zN8ZmYdAPuIjWYbe6jRH5Lofl0g5AigD/UD0R9XeOR5HGTYo/wNHkFZeYiPSHGYCLRh0W/36NBEzslCfc3XbbbTThLgFkjQ5xvLjwhD6aWsIXntBH+o/KUAQAVK3ahpte2RTWHGD05S3T0b0k4o9yRCvQVccGdOlSK3+PtsoA2cjBBx+MVatW4fHHH8ett96K8vJyPPfcczjjjDO0dyjqLWgcISk3Iv1YuHa7Zg1wgaAkSkuv2OVkKHDnoNnri/huq/VwYwlkWN3X6/Pjxlc2xXDG6PAFOHIY4LO4YKWzKyo1EREX/eFo0NCLtxu3243q6mqMHz8el156KUaNGkUNd3EiK1QmrBCNI3NH7Va89NkO+DmHkzFceEIf3Fs5xJSKhFn1AS1pGZLjiR673ru4dzLHkW3btuHCCy/E1q1bcd1112HBggXweFQRMRulhTIFUplIX6JRHIrl5tboGlBW7EFrR6flZrtMwMkYvrtPuhG3orhkp4xlurF3715cddVVWLlyJU455RS88MILKCvLvvchGszYbYoQK1DL8WhFAbW4t3II7q0cEvG4mfozuTlA7/i19Q1YvrEhwhn+bf9SfPljc1TNGtmOXTcSyWiUswu54W7u3Ln461//ig8++ADLli3DkCGKz7Ls9K67WyqTKOot1chlqTNMpDfRTCeNZfKkaCKeMgiiJx2WySjLQbTeJy3SOQBhB927d8fy5cvxzDPP4MYbb6QJdzZD8XYFVuR4zGC2wcLIqdJaFwewrfFX3Dd5SMZ0FacjyWqUs4u8vDw8/PDD+g13QSk3VDVJ/5IzTKQpoprThiZvXKZLpmuPSCKauJVT+9TvU0m+K6L/JhX6MFIBxhiuuuoqfPnll+jbty813NkIOcQKrMjxmMFsg4WRUyU6v1yDVZBrLtDvIKkKW8kkAy033J166qnmG+4IIs3Qan6VnVGlvq3d5zRq6E7kSOeLRx1meD55wqYZRJcVBmB0/1LN57SaGJXvU/2d47Hw/KFxUYbKFAYNGoRPPvkEc+bMoYY7myCHWIHdAzzsuuvVO7+RFqOSIo9LqIpBSJh9dzLRQB988MFYvXo1HnvsMbz//vsoLy/Hm2++mexlEYStyI5XWbEnIjKbsAE6KqyoU8Rqwe+tHGJ4PidjofdIj5J8Fy4adZimdOiiC4Zh6VUn4uELhoVlMY2GHMlUDi/D7AmD0KvYg8YmLxau3Z5wBY1Ux+124/7778e7776LX3/9FaNGjcIDDzyAQCCQ7KWlJVRDrEBU7xVLFFBdf2amQUO9zclH9dRUsgAQGidqpilD3oaa8bQpK/agb3cPNny3T/P5hI1fTiKMMVx77bX4/e9/jwsvvBATJ07E9ddfjwULFiAvLy/ZyyMI27A7IxgLarWEIo8LjEnqC8X5LnCOkKpFLGpDDiY1sDU2eZHnEsfD/JyHGt20rhcelwP3TS4Prbvi8FLd61pBrrYqhx7R9vRkI2PHjsWWLVtw1VVXYfbs2Vi7di1eeOEF9OrVK9lLSytIZUJFPOWyzDrDWk55gHO0d0be9RW4nejoDAjHiRLGKB1dUbezkzE8OMU4qpFJtLW1hRrujj32WLz00ks49thjk72spEMqE5mB6Lue6o1bVhQZosUoaGI2OCC6npnZN11/P8mEcx5quPN4PFiyZAnOPvvsZC8rJTBjt6lkQoWdAzxq6xswuvo99Ju7BsPmv43Zr21GQ1CbUqterba+AbfUbNZs7OvQcIYBoLXDT85wDBR7XGHGWRQdCnCeVc4w0NVw9+abb2L37t2oqKjAo48+am7CHUGkOFplA+nQFyBqDBzdv9S2Zjijb7jZ0hKjRnXlNVLd1JhKEfx0Qdlwd/jhh6OyshIzZsyghjuTxOQQM8bOZ4xtY4wFGGNZFTExQr4zlh3gJq9POI9dub1oMpHIQOkZrgK3PROTMhm5IVE2yg5BjXW6q0nEwumnnx5quJs1axY13KU5ZLclzChApCJa65brdS8adVhCFCIAKaijdGS1nFs9p1Z9jWxo8uKmVzbhjlqpLMLunp5sQtlwt3jxYowYMQL19fXJXlbKE2uE+CsAkwF8ZMNaMgqtO2MtZMOgFRlWImqGEylHlOS7sO3u0xLavZyOyJF62Shr3ZDEU5YpXaCGu4yC7HYQOzOCqcC9lUOwSNXEZgUGwO0071LLjuzsVzdHZEBvemUTPII65V7FHqGc6P98+iNq6xvSNoKfKqgb7k444QQ8+OCD1HCnQ0wOMef8n5zzxLfkJhG9FI8SK8oPepFhQJKoufCEPprGITdH+1coHy6a9FI08mwiw5cKuJwMInENJ2O6NyKJkGVKF+SGu7q6OvzmN7/BxIkTMWvWLLS1tSV7aYQFstFuZxJakVWlXaocXmZailMNB9Dht14S5QvwiAwoB9DqC8Dl1FZW0rs2zV+9LW0j+KmG3HB35pln4tZbb8WECRPQ2NiY7GWlJAnzYhhj0xljdYyxut27dyfqtLZiZIiUmEnreFxOMAbjSDKXuni1jEObT/turzmoUWw1veRyMjw0ZZjuNrKEnPLD4xWsI9k4GcPC84Zi0ZRhmjcUejciWo0lyZJlSiXkCXc33HADHn/8cWzatCnZSyLiQCbY7EzEzACpVKqzLXDnaDq1etcmWREp0yL4yUKecLd48WJ8/PHHePnll5O9pJTE0CFmjL3LGPtK44+l1kXO+WLOeQXnvKJnz57RrziJWJlkp5XucTkYSvJdYYbBjFyaL8BxS81m3PTKJgDAoguGhYyDUZ2VFX1LWR/SiO+rJ4JzwKwLnEzlY1kZQhRt0CspEbnKqXSxSRZyw9327dsxatSoZC+HUGGH3c4Em50JqLOSIoUJpV2Kps42Xna62evTdGoTVfpgNqub6cgNd1999RVuuOGGZC8nJTHMq3DOT03EQtIBK12val1JLZm12voG05rAciRTrcVopJ0sn2/+6m26zrfH5cRdZw0OSY+JKMmXatPkKXlmuGjUYVi+sSFijeeOKMP73+xGQ5MXDgbYLZZRku+KGJGqFWFQv39GUFNHF/3790/2EggNyG5nBlpavKJrhtIuaV0X1LDgPvL1KV5SbiJ7WTm8DLev2IJWjexitDXQakjLOJJ+/folewkpCw3msIDIaOh94fW+dAvXbo9qQIYclVYeX8vxVuse6znEymMaRUD7zl1jab33Vg4xFG4HunSa7TDMDMBdZw023E5ewy01m4UNdcpHqamDIIhEIWo807NLsh31+vxwMiYsC7to1GG4t3JI6Od46Bsb2cu/TC7H7Fc3h0mHuhwMVZOMbbcZ9LK62eoQE2JicogZY+cAeBRATwBrGGObOOcTbFlZCmL3JLtYUu/KfbUcbyuRBZmGoOKFXrTATImHErkkwejmQLlNv7lrYpqkxyAZe7MGT95O63crR7HjMaiFIJJBttntdEZ0jeCQbKtWEERpx/ycw+Ny4rjDivDpv/fDzzmcTGrSVjrDgLmoshEuJ0OB2/xUOjOZ1FggLWPCCjE5xJzzlQBW2rSWlMfuL6/I8SwJjurUK0swStubjSyombdiK84dURZR4hDNuOdobxZiSd+VRfk7ibdhJohUIdvsdjojsoWiaW2iiOgPe7347r4zdM+lZQNPPqonXvpsh27zsRKfn6MgNweb7hpvanv5vPGys1azukR2QyUTFrHzyyuKON911mAsXLtd6BCbcTT1IgvFHpfw2F6fH+9/sxv3TR4SZhitOqjFHheqJg3WjFwr65m1tos2UhHrSM94GmaCIAirWMlK1tY3mGq400NpA42GRYlIpeir3VldIrMhhziJ6EUlZUUJLcxoMRpFFmrrG3Cj4ByNTd4I59CovkyOIOtFaGvrGzD7tc1hepVNXh9mv7oZQNf7IYpUyKULRR4XWjo6w45DRo4giEzDbOZKdl5FKCOi6t4Skb02O1xK71zJhjJ/hBXIIU4yoqiknkNr5stsRn1C1MCmZdC0jmfGCVaycO32CPF2QJKVUzc5GEVrzRp1giCIdMZM5krPeVU33JlVXYgm0puKgQnK/BFmIYc4RYk11WPmztjKOaK501Y7rXoRZqvGl4wcQRCEhJ79VGYUragumC2VczKGAOcUmCDSHnKIUxQ7HNDZEwbp1tRaPYe6vmzh2u246ZVNQo1lKyoXqZRmIwiCUJLqGSmzGUUrqgtmezkCnOP76okWV0wQqQc5xCmMlShotALkZs6hvhicfFTPMBUKrXOJVC60cDlYyqXZCIIggPQY7mA222dFdUEdMHEINI0pmEFkCuQQZwh2CJBrRUEARFwMln76Y4Rzqz6XXgqvJN+lqzJhF6ke1SEIIvVJh+EOZrN9Vkvx1FlBdVO0y5l+wQy6LhAiyCHOEGIVIBdFQXJzHKYjvQ1NXvSbuwa9ij0oEki7xSqNZpZ0iOoQBJH6pMtwB7PDj4AYVBfUxj+WCUpJgK4LhB4Z5RBn852fKBXmYCzkpOq9H6IoiFXZHQ7JyLicDC4HCxvJmcgO5HSI6hAEkfqk4nCHWK510TYkL1y7PcyeA9oKQakMXRcIPRzJXoBdyHd+DU3ekFM2b8VW1NY3JHtpCWH2hEHwuJwRj/s5N/V+2B3t8Pk5CvNyUFbsAYMUGTajn2wX6RLVIQgitdGyrcmUF0vWtS4TbGomvAYifmRMhDjb7/zMNEDovR96Y6TbfIGoxjg3tfpQf6f5EZ52kopRHYIg0o9UG+6QrGudkU1NhwwtXRcIPTImQkx3fpLh3jB3LL6vnoiAYNym6P0QRUHuOmsw7ps8JCzSe9GowzSj0WqSaWRSLapDEET6orStG+aOTaqjl6xrnZ5NTZcMLV0XCD0yJkJMd37hWH0/jKIg6gtAxeGloW1TcZRyqkV1CIIg7CBZ1zo9mzq6+r24RK3tjjrTdYHQg3FBJDGeVFRU8Lq6OluPqe4eBSSnLJF1q6lEot+PdEiXEYQdMMY2cs4rkr2ORBIPm01ERype6/rNXaNZRseAqId2pOLrJNIXM3Y7YyLEdOcXjvy6q1ZtC8mf5bniVyFDo5QJgiDiType6+IRtc72viAi8WSMQwyQU6ZFe2cg9P/9rT7SXCQIgkhzUu1aZ3XghxmoL4hINBnTVEdEoneHTRAEQRB2UDm8LKL5OtbSBlF0OVv7goj4k1ERYiIcusMmCIIgEoHdUet4RJ0JQg+KEGcwdIdNEARBpCPxiDoThB4UIc5g6A6bIAiCSFdSrVaayGzIIc5gUrEbmSAIgiAIItUghzjDoTtsgiAIgiAIfaiGmCAIgiAIgshqyCEmCIIgCIIgshpyiAmCIAiCIIishhxigiAIgiAIIqshh5ggCIIgCILIasghJgiCIAiCILIacogJgiAIgiCIrIYcYoIgCIIgCCKrIYeYIAiCIAiCyGrIISYIgiAIgiCyGnKICYIgCIIgiKyGHGKCIAiCIAgiqyGHmCAIgiAIgshqYnKIGWMLGWPfMMa2MMZWMsaKbVoXQRAEEQfIbhMEQUQSa4T4HQDHcs7LAfwLwLzYl0QQBEHEEbLbBEEQKmJyiDnnb3POO4M/fgqgd+xLIgiCIOIF2W2CIIhIcmw81hUAXhE9yRibDmB68Md2xthXNp47HegBYE+yF5Fg6DVnPtn2egFgULIXYCNCu002Oys/2/Sas4NsfM2GdptxzvU3YOxdAIdoPPUnzvnrwW3+BKACwGRudEBp+zrOeYXRdpkEvebsINtec7a9XiA9XrPddjsdXrPd0GvODug1ZwdmXrNhhJhzfqrBSS4HcCaAU8w4wwRBEER8IbtNEARhjZhKJhhjpwGYA+AkznmrPUsiCIIg4gXZbYIgiEhiVZl4DEA3AO8wxjYxxp40ud/iGM+bjtBrzg6y7TVn2+sF0v81R2O30/01RwO95uyAXnN2YPiaDWuICYIgCIIgCCKToUl1BEEQBEEQRFZDDjFBEARBEASR1STNIc7G8aGMsfMZY9sYYwHGWMZKnjDGTmOMbWeM/R9jbG6y15MIGGPPMsZ+yRatVsZYH8bY+4yxr4Of6RuSvaZ4wxjLY4x9zhjbHHzN85O9pkRCNptsdiZBNptstppkRoizcXzoVwAmA/go2QuJF4wxJ4C/ATgdwDEALmSMHZPcVSWE5wGcluxFJJBOALdwzo8BMArAtVnwe24HMJZzPhTAMACnMcZGJXdJCYVsdgZCNjtrIJttYLOT5hBn4/hQzvk/Oefbk72OOHM8gP/jnP+bc94B4GUAZyd5TXGHc/4RgH3JXkei4Jzv4px/Gfz/rwD+CaAsuauKL1ziQPBHV/BP1nQlk83OWMhmZwFks41tdqrUEF8B4K1kL4KwhTIAOxQ/70SGf+myHcZYXwDDAXyW5KXEHcaYkzG2CcAvAN7hnGf8axZANjtzIJudZZDN1iamwRwmFmJ2fGgngKXxXEuiMPOaCSJTYIwVAlgO4EbO+X+TvZ54wzn3AxgWrJ9dyRg7lnOeMTWIZLPDIJtNZBxks8U2O64OcTaODzV6zVlAA4A+ip97Bx8jMgzGmAuSYV3KOV+R7PUkEs55E2PsfUg1iBnjEJPNzkrIZmcJZLP1bXYyVSbk8aGTaHxoRvEFgIGMsX6MMTeAPwBYleQ1ETbDGGMAlgD4J+f8oWSvJxEwxnrKygqMMQ+AcQC+SeqiEgjZ7IyFbHYWQDbb2GYns4Y42rHPaQtj7BzG2E4AJwJYwxhbm+w12U2w6eY6AGshFe3XcM63JXdV8Ycx9hKATwAMYoztZIxdmew1xZnRAC4BMDb4/d3EGDsj2YuKM4cCeJ8xtgWSE/EO5/yNJK8pkZDNJpudMZDNJputhkY3EwRBEARBEFlNqqhMEARBEARBEERSIIeYIAiCIAiCyGrIISYIgiAIgiCyGnKICYIgCIIgiKyGHGKCIAiCIAgiqyGHmCAIgiAIgshqyCEmCIIgCIIgspr/H/xnWDv0ksO8AAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from valuation.utils.plotting import plot_datasets\n",
    "\n",
    "datasets = {\n",
    "    'train': (dataset.x_train, dataset.y_train),\n",
    "    'test': (dataset.x_test, dataset.y_test)\n",
    "}\n",
    "x_min = np.asarray([-2, -2])\n",
    "x_max = np.asarray([3, 3])\n",
    "plot_datasets(datasets, x_min, x_max, decision_boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b866c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that both the train and test set are plotted side by side and the training samples overlap with the optimal decision boundary. These samples would get wrongly by any discriminator as this region has some identifiability issues.\n",
    "\n",
    "#### Calculating influences using different reduction operators\n",
    "\n",
    "This section cares about how to calculate influences for this dataset under the assumption of using a logistic regression model for inferring the right labels. Using the pyDVL valuation library a model can be formalized and fitted by using just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa6af658",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6992916464805603\n",
      "Training loss: 0.6969473361968994\n",
      "Training loss: 0.6960689425468445\n",
      "Training loss: 0.694100558757782\n",
      "Training loss: 0.689233660697937\n",
      "Training loss: 0.6873267292976379\n",
      "Training loss: 0.6844278573989868\n",
      "Training loss: 0.6822760105133057\n",
      "Training loss: 0.6798536777496338\n",
      "Training loss: 0.6772058606147766\n",
      "Training loss: 0.6751559376716614\n",
      "Training loss: 0.6737927198410034\n",
      "Training loss: 0.6669039726257324\n",
      "Training loss: 0.6694988012313843\n",
      "Training loss: 0.6635289788246155\n",
      "Training loss: 0.6624935269355774\n",
      "Training loss: 0.6614458560943604\n",
      "Training loss: 0.6604691743850708\n",
      "Training loss: 0.6559029817581177\n",
      "Training loss: 0.6529050469398499\n",
      "Training loss: 0.6544086933135986\n",
      "Training loss: 0.6498947143554688\n",
      "Training loss: 0.6387320756912231\n",
      "Training loss: 0.6491490602493286\n",
      "Training loss: 0.6439175605773926\n",
      "Training loss: 0.6423365473747253\n",
      "Training loss: 0.6472393274307251\n",
      "Training loss: 0.6373187899589539\n",
      "Training loss: 0.6344636082649231\n",
      "Training loss: 0.6358363032341003\n",
      "Training loss: 0.627935528755188\n",
      "Training loss: 0.619563639163971\n",
      "Training loss: 0.6326642632484436\n",
      "Training loss: 0.6171635389328003\n",
      "Training loss: 0.623903751373291\n",
      "Training loss: 0.627859354019165\n",
      "Training loss: 0.6239509582519531\n",
      "Training loss: 0.6073152422904968\n",
      "Training loss: 0.6139663457870483\n",
      "Training loss: 0.6123691201210022\n",
      "Training loss: 0.6004865169525146\n",
      "Training loss: 0.6039763689041138\n",
      "Training loss: 0.6040962338447571\n",
      "Training loss: 0.5968834757804871\n",
      "Training loss: 0.6053683757781982\n",
      "Training loss: 0.6108034253120422\n",
      "Training loss: 0.6065726280212402\n",
      "Training loss: 0.6075528860092163\n",
      "Training loss: 0.5909566879272461\n",
      "Training loss: 0.578762948513031\n",
      "Training loss: 0.5931411981582642\n",
      "Training loss: 0.5899786949157715\n",
      "Training loss: 0.5916444659233093\n",
      "Training loss: 0.585474967956543\n",
      "Training loss: 0.5770435929298401\n",
      "Training loss: 0.5768575072288513\n",
      "Training loss: 0.5780481100082397\n",
      "Training loss: 0.5866444706916809\n",
      "Training loss: 0.5619579553604126\n",
      "Training loss: 0.5826389789581299\n",
      "Training loss: 0.5651266574859619\n",
      "Training loss: 0.578497052192688\n",
      "Training loss: 0.5659858584403992\n",
      "Training loss: 0.5744827389717102\n",
      "Training loss: 0.5626910328865051\n",
      "Training loss: 0.5606379508972168\n",
      "Training loss: 0.5671747922897339\n",
      "Training loss: 0.5399091243743896\n",
      "Training loss: 0.5671245455741882\n",
      "Training loss: 0.5430088043212891\n",
      "Training loss: 0.549655556678772\n",
      "Training loss: 0.5574337840080261\n",
      "Training loss: 0.5592690110206604\n",
      "Training loss: 0.5483152866363525\n",
      "Training loss: 0.543575644493103\n",
      "Training loss: 0.5567070245742798\n",
      "Training loss: 0.5420346260070801\n",
      "Training loss: 0.5226472020149231\n",
      "Training loss: 0.5555842518806458\n",
      "Training loss: 0.5394282341003418\n",
      "Training loss: 0.5436187386512756\n",
      "Training loss: 0.553819477558136\n",
      "Training loss: 0.5392590165138245\n",
      "Training loss: 0.5346193313598633\n",
      "Training loss: 0.5385768413543701\n",
      "Training loss: 0.5215160846710205\n",
      "Training loss: 0.5157248973846436\n",
      "Training loss: 0.542580246925354\n",
      "Training loss: 0.5120052695274353\n",
      "Training loss: 0.5237628221511841\n",
      "Training loss: 0.538565993309021\n",
      "Training loss: 0.528938353061676\n",
      "Training loss: 0.5033707618713379\n",
      "Training loss: 0.514668345451355\n",
      "Training loss: 0.5223854780197144\n",
      "Training loss: 0.5031518340110779\n",
      "Training loss: 0.5065650939941406\n",
      "Training loss: 0.5097880363464355\n",
      "Training loss: 0.4984435439109802\n",
      "Training loss: 0.514376163482666\n",
      "Training loss: 0.5245163440704346\n",
      "Training loss: 0.5213207006454468\n",
      "Training loss: 0.5221086144447327\n",
      "Training loss: 0.500821590423584\n",
      "Training loss: 0.486758291721344\n",
      "Training loss: 0.5008731484413147\n",
      "Training loss: 0.4995051920413971\n",
      "Training loss: 0.5060563087463379\n",
      "Training loss: 0.4998125731945038\n",
      "Training loss: 0.48446521162986755\n",
      "Training loss: 0.4846000075340271\n",
      "Training loss: 0.4875655174255371\n",
      "Training loss: 0.5099546909332275\n",
      "Training loss: 0.4729476273059845\n",
      "Training loss: 0.5001303553581238\n",
      "Training loss: 0.47612708806991577\n",
      "Training loss: 0.49404823780059814\n",
      "Training loss: 0.4825579524040222\n",
      "Training loss: 0.48883384466171265\n",
      "Training loss: 0.4774818420410156\n",
      "Training loss: 0.48080384731292725\n",
      "Training loss: 0.48613980412483215\n",
      "Training loss: 0.45268189907073975\n",
      "Training loss: 0.49231618642807007\n",
      "Training loss: 0.45666149258613586\n",
      "Training loss: 0.4660920798778534\n",
      "Training loss: 0.4828345775604248\n",
      "Training loss: 0.48461171984672546\n",
      "Training loss: 0.4673113226890564\n",
      "Training loss: 0.4625292122364044\n",
      "Training loss: 0.48449164628982544\n",
      "Training loss: 0.45878416299819946\n",
      "Training loss: 0.44128289818763733\n",
      "Training loss: 0.48522722721099854\n",
      "Training loss: 0.45953482389450073\n",
      "Training loss: 0.47059306502342224\n",
      "Training loss: 0.47881826758384705\n",
      "Training loss: 0.4676893651485443\n",
      "Training loss: 0.4598674774169922\n",
      "Training loss: 0.4647217392921448\n",
      "Training loss: 0.4409967362880707\n",
      "Training loss: 0.44195854663848877\n",
      "Training loss: 0.4744799733161926\n",
      "Training loss: 0.4358080327510834\n",
      "Training loss: 0.44803109765052795\n",
      "Training loss: 0.4704279601573944\n",
      "Training loss: 0.456035315990448\n",
      "Training loss: 0.4269427955150604\n",
      "Training loss: 0.4400671124458313\n",
      "Training loss: 0.45633748173713684\n",
      "Training loss: 0.4326111972332001\n",
      "Training loss: 0.43429598212242126\n",
      "Training loss: 0.4400705397129059\n",
      "Training loss: 0.4255526065826416\n",
      "Training loss: 0.44591379165649414\n",
      "Training loss: 0.4581919312477112\n",
      "Training loss: 0.45686259865760803\n",
      "Training loss: 0.45634007453918457\n",
      "Training loss: 0.43387776613235474\n",
      "Training loss: 0.42073097825050354\n",
      "Training loss: 0.4310588240623474\n",
      "Training loss: 0.43066561222076416\n",
      "Training loss: 0.4404859244823456\n",
      "Training loss: 0.4357261657714844\n",
      "Training loss: 0.4148370921611786\n",
      "Training loss: 0.41569235920906067\n",
      "Training loss: 0.41974809765815735\n",
      "Training loss: 0.45255908370018005\n",
      "Training loss: 0.40805843472480774\n",
      "Training loss: 0.43770653009414673\n",
      "Training loss: 0.4102906882762909\n",
      "Training loss: 0.42984914779663086\n",
      "Training loss: 0.42085376381874084\n",
      "Training loss: 0.4236578345298767\n",
      "Training loss: 0.41343605518341064\n",
      "Training loss: 0.42172494530677795\n",
      "Training loss: 0.42526108026504517\n",
      "Training loss: 0.3891902565956116\n",
      "Training loss: 0.43643325567245483\n",
      "Training loss: 0.39246249198913574\n",
      "Training loss: 0.4041973352432251\n",
      "Training loss: 0.42768698930740356\n",
      "Training loss: 0.42921262979507446\n",
      "Training loss: 0.4060742259025574\n",
      "Training loss: 0.40238890051841736\n",
      "Training loss: 0.4299854338169098\n",
      "Training loss: 0.39673736691474915\n",
      "Training loss: 0.381071537733078\n",
      "Training loss: 0.43107497692108154\n",
      "Training loss: 0.3994303047657013\n",
      "Training loss: 0.41625332832336426\n",
      "Training loss: 0.4209221303462982\n",
      "Training loss: 0.41496264934539795\n",
      "Training loss: 0.4032059609889984\n",
      "Training loss: 0.4091835618019104\n",
      "Training loss: 0.38033923506736755\n",
      "Training loss: 0.3874882757663727\n",
      "Training loss: 0.4228024184703827\n",
      "Training loss: 0.38003408908843994\n",
      "Training loss: 0.39164549112319946\n",
      "Training loss: 0.418809175491333\n",
      "Training loss: 0.40099865198135376\n",
      "Training loss: 0.36977094411849976\n",
      "Training loss: 0.3844073712825775\n",
      "Training loss: 0.4068180322647095\n",
      "Training loss: 0.3794165551662445\n",
      "Training loss: 0.3799803555011749\n",
      "Training loss: 0.38815218210220337\n",
      "Training loss: 0.370908260345459\n",
      "Training loss: 0.3942660987377167\n",
      "Training loss: 0.4074385166168213\n",
      "Training loss: 0.40789052844047546\n",
      "Training loss: 0.40590906143188477\n",
      "Training loss: 0.38327303528785706\n",
      "Training loss: 0.3716045320034027\n",
      "Training loss: 0.37840718030929565\n",
      "Training loss: 0.3780009150505066\n",
      "Training loss: 0.3897205591201782\n",
      "Training loss: 0.386905699968338\n",
      "Training loss: 0.36171236634254456\n",
      "Training loss: 0.36393022537231445\n",
      "Training loss: 0.3686768710613251\n",
      "Training loss: 0.4088374078273773\n",
      "Training loss: 0.3596702814102173\n",
      "Training loss: 0.39023488759994507\n",
      "Training loss: 0.3609059751033783\n",
      "Training loss: 0.380979984998703\n",
      "Training loss: 0.3744616210460663\n",
      "Training loss: 0.37405845522880554\n",
      "Training loss: 0.36474600434303284\n",
      "Training loss: 0.3771464228630066\n",
      "Training loss: 0.3790678381919861\n",
      "Training loss: 0.34195810556411743\n",
      "Training loss: 0.3941383957862854\n",
      "Training loss: 0.3438887596130371\n",
      "Training loss: 0.35776591300964355\n",
      "Training loss: 0.38627976179122925\n",
      "Training loss: 0.38758373260498047\n",
      "Training loss: 0.359225332736969\n",
      "Training loss: 0.35717886686325073\n",
      "Training loss: 0.38825762271881104\n",
      "Training loss: 0.3500882089138031\n",
      "Training loss: 0.33547019958496094\n",
      "Training loss: 0.38881179690361023\n",
      "Training loss: 0.3537505269050598\n",
      "Training loss: 0.3751981258392334\n",
      "Training loss: 0.3759269714355469\n",
      "Training loss: 0.3755168914794922\n",
      "Training loss: 0.3596506118774414\n",
      "Training loss: 0.366909921169281\n",
      "Training loss: 0.33406636118888855\n",
      "Training loss: 0.34638720750808716\n",
      "Training loss: 0.38295626640319824\n",
      "Training loss: 0.3385215997695923\n",
      "Training loss: 0.3491697609424591\n",
      "Training loss: 0.3792755603790283\n",
      "Training loss: 0.35892945528030396\n",
      "Training loss: 0.32626527547836304\n",
      "Training loss: 0.34233057498931885\n",
      "Training loss: 0.3690149188041687\n",
      "Training loss: 0.3384588360786438\n",
      "Training loss: 0.3383716940879822\n",
      "Training loss: 0.3488065302371979\n",
      "Training loss: 0.3291724920272827\n",
      "Training loss: 0.3547493517398834\n",
      "Training loss: 0.3680799603462219\n",
      "Training loss: 0.37002548575401306\n",
      "Training loss: 0.3666788935661316\n",
      "Training loss: 0.34429121017456055\n",
      "Training loss: 0.33418262004852295\n",
      "Training loss: 0.33813318610191345\n",
      "Training loss: 0.33707481622695923\n",
      "Training loss: 0.3497851490974426\n",
      "Training loss: 0.34898120164871216\n",
      "Training loss: 0.3203926980495453\n",
      "Training loss: 0.3243912160396576\n",
      "Training loss: 0.32958048582077026\n",
      "Training loss: 0.3749309778213501\n",
      "Training loss: 0.3228079080581665\n",
      "Training loss: 0.3535225987434387\n",
      "Training loss: 0.3231041431427002\n",
      "Training loss: 0.34321358799934387\n",
      "Training loss: 0.3388442397117615\n",
      "Training loss: 0.33573901653289795\n",
      "Training loss: 0.32706618309020996\n",
      "Training loss: 0.3428319990634918\n",
      "Training loss: 0.34336596727371216\n",
      "Training loss: 0.3060322105884552\n",
      "Training loss: 0.36155590415000916\n",
      "Training loss: 0.3063870072364807\n",
      "Training loss: 0.3222540616989136\n",
      "Training loss: 0.35459399223327637\n",
      "Training loss: 0.35570645332336426\n",
      "Training loss: 0.32274937629699707\n",
      "Training loss: 0.3225363492965698\n",
      "Training loss: 0.3557470440864563\n",
      "Training loss: 0.3143579363822937\n",
      "Training loss: 0.3001439571380615\n",
      "Training loss: 0.35528063774108887\n",
      "Training loss: 0.31842517852783203\n",
      "Training loss: 0.3435613512992859\n",
      "Training loss: 0.340398371219635\n",
      "Training loss: 0.3454280495643616\n",
      "Training loss: 0.32557743787765503\n",
      "Training loss: 0.3341428339481354\n",
      "Training loss: 0.29813313484191895\n",
      "Training loss: 0.31468597054481506\n",
      "Training loss: 0.35161787271499634\n",
      "Training loss: 0.3069881796836853\n",
      "Training loss: 0.31658852100372314\n",
      "Training loss: 0.34850144386291504\n",
      "Training loss: 0.32617101073265076\n",
      "Training loss: 0.29250773787498474\n",
      "Training loss: 0.3099297285079956\n",
      "Training loss: 0.33959686756134033\n",
      "Training loss: 0.3062903881072998\n",
      "Training loss: 0.30581164360046387\n",
      "Training loss: 0.31836655735969543\n",
      "Training loss: 0.29662060737609863\n",
      "Training loss: 0.3239871561527252\n",
      "Training loss: 0.33701711893081665\n",
      "Training loss: 0.3401619493961334\n",
      "Training loss: 0.33559471368789673\n",
      "Training loss: 0.3136531710624695\n",
      "Training loss: 0.30503150820732117\n",
      "Training loss: 0.30675771832466125\n",
      "Training loss: 0.30467820167541504\n",
      "Training loss: 0.31780633330345154\n",
      "Training loss: 0.3189300000667572\n",
      "Training loss: 0.28760817646980286\n",
      "Training loss: 0.2935931980609894\n",
      "Training loss: 0.2990698516368866\n",
      "Training loss: 0.3481432795524597\n",
      "Training loss: 0.29412034153938293\n",
      "Training loss: 0.3245849609375\n",
      "Training loss: 0.2935459613800049\n",
      "Training loss: 0.31348395347595215\n",
      "Training loss: 0.31090182065963745\n",
      "Training loss: 0.30558472871780396\n",
      "Training loss: 0.29733842611312866\n",
      "Training loss: 0.31588271260261536\n",
      "Training loss: 0.3152148127555847\n",
      "Training loss: 0.27809980511665344\n",
      "Training loss: 0.3359810709953308\n",
      "Training loss: 0.27682679891586304\n",
      "Training loss: 0.29453036189079285\n",
      "Training loss: 0.32986754179000854\n",
      "Training loss: 0.3307989239692688\n",
      "Training loss: 0.2938106656074524\n",
      "Training loss: 0.29544928669929504\n",
      "Training loss: 0.32995688915252686\n",
      "Training loss: 0.28642359375953674\n",
      "Training loss: 0.2721789479255676\n",
      "Training loss: 0.32822540402412415\n",
      "Training loss: 0.2905891537666321\n",
      "Training loss: 0.3186902701854706\n",
      "Training loss: 0.31184470653533936\n",
      "Training loss: 0.3220226764678955\n",
      "Training loss: 0.2984382212162018\n",
      "Training loss: 0.30825525522232056\n",
      "Training loss: 0.2697075307369232\n",
      "Training loss: 0.2897144854068756\n",
      "Training loss: 0.32648003101348877\n",
      "Training loss: 0.2825426161289215\n",
      "Training loss: 0.29112035036087036\n",
      "Training loss: 0.3241320848464966\n",
      "Training loss: 0.3001704812049866\n",
      "Training loss: 0.2658044695854187\n",
      "Training loss: 0.2845005989074707\n",
      "Training loss: 0.31627988815307617\n",
      "Training loss: 0.28054943680763245\n",
      "Training loss: 0.2798067331314087\n",
      "Training loss: 0.29433152079582214\n",
      "Training loss: 0.27071088552474976\n",
      "Training loss: 0.2996255159378052\n",
      "Training loss: 0.3120608925819397\n",
      "Training loss: 0.3161601722240448\n",
      "Training loss: 0.31050607562065125\n",
      "Training loss: 0.2891041934490204\n",
      "Training loss: 0.28184983134269714\n",
      "Training loss: 0.281857430934906\n",
      "Training loss: 0.27856016159057617\n",
      "Training loss: 0.29175159335136414\n",
      "Training loss: 0.2946752905845642\n",
      "Training loss: 0.2611030042171478\n",
      "Training loss: 0.2691400647163391\n",
      "Training loss: 0.27480554580688477\n",
      "Training loss: 0.32660409808158875\n",
      "Training loss: 0.2713460326194763\n",
      "Training loss: 0.301352322101593\n",
      "Training loss: 0.26996710896492004\n",
      "Training loss: 0.28964605927467346\n",
      "Training loss: 0.2885326147079468\n",
      "Training loss: 0.2814192771911621\n",
      "Training loss: 0.2734462320804596\n",
      "Training loss: 0.29431694746017456\n",
      "Training loss: 0.29259130358695984\n",
      "Training loss: 0.25593942403793335\n",
      "Training loss: 0.31554633378982544\n",
      "Training loss: 0.25306734442710876\n",
      "Training loss: 0.27246248722076416\n",
      "Training loss: 0.3102131485939026\n",
      "Training loss: 0.31095945835113525\n",
      "Training loss: 0.2704346179962158\n",
      "Training loss: 0.2738608121871948\n",
      "Training loss: 0.30914899706840515\n",
      "Training loss: 0.2641483545303345\n",
      "Training loss: 0.2496015429496765\n",
      "Training loss: 0.3060453236103058\n",
      "Training loss: 0.2682558000087738\n",
      "Training loss: 0.2987726926803589\n",
      "Training loss: 0.2885020673274994\n",
      "Training loss: 0.3034837245941162\n",
      "Training loss: 0.2764527201652527\n",
      "Training loss: 0.2874308228492737\n",
      "Training loss: 0.2468244433403015\n",
      "Training loss: 0.2696624994277954\n",
      "Training loss: 0.30594754219055176\n",
      "Training loss: 0.2632325291633606\n",
      "Training loss: 0.27085694670677185\n",
      "Training loss: 0.30451837182044983\n",
      "Training loss: 0.279162734746933\n",
      "Training loss: 0.24430091679096222\n",
      "Training loss: 0.26418444514274597\n",
      "Training loss: 0.297488272190094\n",
      "Training loss: 0.25960013270378113\n",
      "Training loss: 0.25865083932876587\n",
      "Training loss: 0.27499401569366455\n",
      "Training loss: 0.24970345199108124\n",
      "Training loss: 0.28002509474754333\n",
      "Training loss: 0.29167550802230835\n",
      "Training loss: 0.2965397536754608\n",
      "Training loss: 0.2899077534675598\n",
      "Training loss: 0.26908546686172485\n",
      "Training loss: 0.2630719244480133\n",
      "Training loss: 0.26175135374069214\n",
      "Training loss: 0.2571439743041992\n",
      "Training loss: 0.2701820135116577\n",
      "Training loss: 0.2747751474380493\n",
      "Training loss: 0.2393069714307785\n",
      "Training loss: 0.2493804693222046\n",
      "Training loss: 0.2551707923412323\n",
      "Training loss: 0.3090066909790039\n",
      "Training loss: 0.25293928384780884\n",
      "Training loss: 0.2823854982852936\n",
      "Training loss: 0.2508173882961273\n",
      "Training loss: 0.27020254731178284\n",
      "Training loss: 0.2702970504760742\n",
      "Training loss: 0.2617247700691223\n",
      "Training loss: 0.25391724705696106\n",
      "Training loss: 0.2767653167247772\n",
      "Training loss: 0.2740950584411621\n",
      "Training loss: 0.23803944885730743\n",
      "Training loss: 0.2989528477191925\n",
      "Training loss: 0.2336302101612091\n",
      "Training loss: 0.2545860707759857\n",
      "Training loss: 0.29432860016822815\n",
      "Training loss: 0.2948787212371826\n",
      "Training loss: 0.2512388527393341\n",
      "Training loss: 0.2563543915748596\n",
      "Training loss: 0.2921026647090912\n",
      "Training loss: 0.2460624873638153\n",
      "Training loss: 0.23105324804782867\n",
      "Training loss: 0.28759676218032837\n",
      "Training loss: 0.2500397861003876\n",
      "Training loss: 0.2825561463832855\n",
      "Training loss: 0.2691197693347931\n",
      "Training loss: 0.28856080770492554\n",
      "Training loss: 0.25836655497550964\n",
      "Training loss: 0.27040329575538635\n",
      "Training loss: 0.22810867428779602\n",
      "Training loss: 0.2532828450202942\n",
      "Training loss: 0.2889043688774109\n",
      "Training loss: 0.24772170186042786\n",
      "Training loss: 0.2544768750667572\n",
      "Training loss: 0.288498193025589\n",
      "Training loss: 0.26191583275794983\n",
      "Training loss: 0.2267046421766281\n",
      "Training loss: 0.2476913183927536\n",
      "Training loss: 0.2821189761161804\n",
      "Training loss: 0.24229155480861664\n",
      "Training loss: 0.2411581426858902\n",
      "Training loss: 0.25917279720306396\n",
      "Training loss: 0.2323896288871765\n",
      "Training loss: 0.26403146982192993\n",
      "Training loss: 0.2747736871242523\n",
      "Training loss: 0.28026047348976135\n",
      "Training loss: 0.27273568511009216\n",
      "Training loss: 0.25250354409217834\n",
      "Training loss: 0.24761201441287994\n",
      "Training loss: 0.24526138603687286\n",
      "Training loss: 0.23931318521499634\n",
      "Training loss: 0.2520677149295807\n",
      "Training loss: 0.2582113742828369\n",
      "Training loss: 0.2211104780435562\n",
      "Training loss: 0.23316138982772827\n",
      "Training loss: 0.2390337586402893\n",
      "Training loss: 0.29442495107650757\n",
      "Training loss: 0.2378246933221817\n",
      "Training loss: 0.2666703164577484\n",
      "Training loss: 0.23501697182655334\n",
      "Training loss: 0.2540971636772156\n",
      "Training loss: 0.25519177317619324\n",
      "Training loss: 0.24543003737926483\n",
      "Training loss: 0.2377130389213562\n",
      "Training loss: 0.26226556301116943\n",
      "Training loss: 0.2587423324584961\n",
      "Training loss: 0.22335103154182434\n",
      "Training loss: 0.2852843403816223\n",
      "Training loss: 0.2174779623746872\n",
      "Training loss: 0.23987892270088196\n",
      "Training loss: 0.2813010811805725\n",
      "Training loss: 0.28164142370224\n",
      "Training loss: 0.2352423518896103\n",
      "Training loss: 0.24193887412548065\n",
      "Training loss: 0.2779483199119568\n",
      "Training loss: 0.23114004731178284\n",
      "Training loss: 0.21558064222335815\n",
      "Training loss: 0.27205216884613037\n",
      "Training loss: 0.23496155440807343\n",
      "Training loss: 0.2691597640514374\n",
      "Training loss: 0.252800315618515\n",
      "Training loss: 0.2763783037662506\n",
      "Training loss: 0.2432832419872284\n",
      "Training loss: 0.2562769055366516\n",
      "Training loss: 0.21258360147476196\n",
      "Training loss: 0.23969976603984833\n",
      "Training loss: 0.274556428194046\n",
      "Training loss: 0.23507845401763916\n",
      "Training loss: 0.24105015397071838\n",
      "Training loss: 0.2752409279346466\n",
      "Training loss: 0.2475556582212448\n",
      "Training loss: 0.21209967136383057\n",
      "Training loss: 0.2341109663248062\n",
      "Training loss: 0.26938560605049133\n",
      "Training loss: 0.22779959440231323\n",
      "Training loss: 0.22648832201957703\n",
      "Training loss: 0.24603548645973206\n",
      "Training loss: 0.21791376173496246\n",
      "Training loss: 0.2508179545402527\n",
      "Training loss: 0.2605736553668976\n",
      "Training loss: 0.2665770649909973\n",
      "Training loss: 0.25822576880455017\n",
      "Training loss: 0.23857775330543518\n",
      "Training loss: 0.23470136523246765\n",
      "Training loss: 0.23154860734939575\n",
      "Training loss: 0.22426460683345795\n",
      "Training loss: 0.23666022717952728\n",
      "Training loss: 0.2442503571510315\n",
      "Training loss: 0.2057146579027176\n",
      "Training loss: 0.21966299414634705\n",
      "Training loss: 0.22558820247650146\n",
      "Training loss: 0.2821907103061676\n",
      "Training loss: 0.22523871064186096\n",
      "Training loss: 0.2534789443016052\n",
      "Training loss: 0.22179827094078064\n",
      "Training loss: 0.24057263135910034\n",
      "Training loss: 0.24250301718711853\n",
      "Training loss: 0.231766939163208\n",
      "Training loss: 0.2240881621837616\n",
      "Training loss: 0.25012874603271484\n",
      "Training loss: 0.2458287626504898\n",
      "Training loss: 0.21113072335720062\n",
      "Training loss: 0.273882657289505\n",
      "Training loss: 0.20386850833892822\n",
      "Training loss: 0.2276133894920349\n",
      "Training loss: 0.27047833800315857\n",
      "Training loss: 0.2705954909324646\n",
      "Training loss: 0.22173678874969482\n",
      "Training loss: 0.22990742325782776\n",
      "Training loss: 0.266055166721344\n",
      "Training loss: 0.21865145862102509\n",
      "Training loss: 0.20250046253204346\n",
      "Training loss: 0.25880277156829834\n",
      "Training loss: 0.22231639921665192\n",
      "Training loss: 0.2579514980316162\n",
      "Training loss: 0.23888951539993286\n",
      "Training loss: 0.2663104236125946\n",
      "Training loss: 0.23055091500282288\n",
      "Training loss: 0.24440622329711914\n",
      "Training loss: 0.19954320788383484\n",
      "Training loss: 0.22828559577465057\n",
      "Training loss: 0.2623275816440582\n",
      "Training loss: 0.22463995218276978\n",
      "Training loss: 0.22990967333316803\n",
      "Training loss: 0.26414284110069275\n",
      "Training loss: 0.23545025289058685\n",
      "Training loss: 0.19982440769672394\n",
      "Training loss: 0.22278913855552673\n",
      "Training loss: 0.2587165832519531\n",
      "Training loss: 0.21552273631095886\n",
      "Training loss: 0.2140333503484726\n",
      "Training loss: 0.23498417437076569\n",
      "Training loss: 0.2056577354669571\n",
      "Training loss: 0.2397821843624115\n",
      "Training loss: 0.24850349128246307\n",
      "Training loss: 0.25494495034217834\n",
      "Training loss: 0.2458193004131317\n",
      "Training loss: 0.22674010694026947\n",
      "Training loss: 0.22378437221050262\n",
      "Training loss: 0.22000475227832794\n",
      "Training loss: 0.2114102840423584\n",
      "Training loss: 0.2234063446521759\n",
      "Training loss: 0.23235353827476501\n",
      "Training loss: 0.19253407418727875\n",
      "Training loss: 0.2082914263010025\n",
      "Training loss: 0.21424901485443115\n",
      "Training loss: 0.27181315422058105\n",
      "Training loss: 0.2146286517381668\n",
      "Training loss: 0.24227945506572723\n",
      "Training loss: 0.21060451865196228\n",
      "Training loss: 0.22907669842243195\n",
      "Training loss: 0.2317127138376236\n",
      "Training loss: 0.22017449140548706\n",
      "Training loss: 0.21249765157699585\n",
      "Training loss: 0.23985199630260468\n",
      "Training loss: 0.2348402589559555\n",
      "Training loss: 0.2008405327796936\n",
      "Training loss: 0.2642664611339569\n",
      "Training loss: 0.19226086139678955\n",
      "Training loss: 0.21726137399673462\n",
      "Training loss: 0.2613854706287384\n",
      "Training loss: 0.26126766204833984\n",
      "Training loss: 0.2102019488811493\n",
      "Training loss: 0.21974623203277588\n",
      "Training loss: 0.2559576630592346\n",
      "Training loss: 0.20806793868541718\n",
      "Training loss: 0.191313236951828\n",
      "Training loss: 0.2473941147327423\n",
      "Training loss: 0.21158835291862488\n",
      "Training loss: 0.2484693080186844\n",
      "Training loss: 0.22690220177173615\n",
      "Training loss: 0.2579014301300049\n",
      "Training loss: 0.21968743205070496\n",
      "Training loss: 0.23431791365146637\n",
      "Training loss: 0.1884680539369583\n",
      "Training loss: 0.21858178079128265\n",
      "Training loss: 0.2517918646335602\n",
      "Training loss: 0.2159252017736435\n",
      "Training loss: 0.2205679714679718\n",
      "Training loss: 0.2547578513622284\n",
      "Training loss: 0.22513434290885925\n",
      "Training loss: 0.18939262628555298\n",
      "Training loss: 0.21324679255485535\n",
      "Training loss: 0.249688982963562\n",
      "Training loss: 0.20501437783241272\n",
      "Training loss: 0.20334455370903015\n",
      "Training loss: 0.22558099031448364\n",
      "Training loss: 0.1951664835214615\n",
      "Training loss: 0.23047739267349243\n",
      "Training loss: 0.23813773691654205\n",
      "Training loss: 0.2449587732553482\n",
      "Training loss: 0.23510053753852844\n",
      "Training loss: 0.2165697067975998\n",
      "Training loss: 0.21445181965827942\n",
      "Training loss: 0.21018122136592865\n",
      "Training loss: 0.20031259953975677\n",
      "Training loss: 0.2118905484676361\n",
      "Training loss: 0.222118079662323\n",
      "Training loss: 0.18113194406032562\n",
      "Training loss: 0.19860856235027313\n",
      "Training loss: 0.20458386838436127\n",
      "Training loss: 0.2629257142543793\n",
      "Training loss: 0.2055867612361908\n",
      "Training loss: 0.23267586529254913\n",
      "Training loss: 0.2010241597890854\n",
      "Training loss: 0.2191995084285736\n",
      "Training loss: 0.22243770956993103\n",
      "Training loss: 0.21023595333099365\n",
      "Training loss: 0.20253586769104004\n",
      "Training loss: 0.23106180131435394\n",
      "Training loss: 0.2253945916891098\n",
      "Training loss: 0.19208365678787231\n",
      "Training loss: 0.2560776472091675\n",
      "Training loss: 0.18225352466106415\n",
      "Training loss: 0.20843230187892914\n",
      "Training loss: 0.2536705732345581\n",
      "Training loss: 0.2533079981803894\n",
      "Training loss: 0.20024915039539337\n",
      "Training loss: 0.21107493340969086\n",
      "Training loss: 0.24730634689331055\n",
      "Training loss: 0.19899919629096985\n",
      "Training loss: 0.18164677917957306\n",
      "Training loss: 0.23748120665550232\n",
      "Training loss: 0.20239311456680298\n",
      "Training loss: 0.24036893248558044\n",
      "Training loss: 0.21647275984287262\n",
      "Training loss: 0.25081318616867065\n",
      "Training loss: 0.2103303074836731\n",
      "Training loss: 0.22565843164920807\n",
      "Training loss: 0.17896954715251923\n",
      "Training loss: 0.2102470099925995\n",
      "Training loss: 0.24262867867946625\n",
      "Training loss: 0.2085787057876587\n",
      "Training loss: 0.21266236901283264\n",
      "Training loss: 0.24675074219703674\n",
      "Training loss: 0.2162591516971588\n",
      "Training loss: 0.18044042587280273\n",
      "Training loss: 0.20512697100639343\n",
      "Training loss: 0.24198448657989502\n",
      "Training loss: 0.1959371566772461\n",
      "Training loss: 0.19408470392227173\n",
      "Training loss: 0.21749940514564514\n",
      "Training loss: 0.18609872460365295\n",
      "Training loss: 0.22256679832935333\n",
      "Training loss: 0.22915437817573547\n",
      "Training loss: 0.23631127178668976\n",
      "Training loss: 0.22575482726097107\n",
      "Training loss: 0.20774930715560913\n",
      "Training loss: 0.2063966542482376\n",
      "Training loss: 0.20174136757850647\n",
      "Training loss: 0.19064031541347504\n",
      "Training loss: 0.201795756816864\n",
      "Training loss: 0.21323749423027039\n",
      "Training loss: 0.17117685079574585\n",
      "Training loss: 0.1902855485677719\n",
      "Training loss: 0.19626782834529877\n",
      "Training loss: 0.2552497684955597\n",
      "Training loss: 0.19780707359313965\n",
      "Training loss: 0.22436857223510742\n",
      "Training loss: 0.19274753332138062\n",
      "Training loss: 0.21063192188739777\n",
      "Training loss: 0.21438944339752197\n",
      "Training loss: 0.20163647830486298\n",
      "Training loss: 0.19389593601226807\n",
      "Training loss: 0.22347579896450043\n",
      "Training loss: 0.2172025740146637\n",
      "Training loss: 0.18456198275089264\n",
      "Training loss: 0.24904510378837585\n",
      "Training loss: 0.17354321479797363\n",
      "Training loss: 0.20083242654800415\n",
      "Training loss: 0.24706818163394928\n",
      "Training loss: 0.246453195810318\n",
      "Training loss: 0.19158309698104858\n",
      "Training loss: 0.20360702276229858\n",
      "Training loss: 0.23983456194400787\n",
      "Training loss: 0.1911522001028061\n",
      "Training loss: 0.17321892082691193\n",
      "Training loss: 0.2287983000278473\n",
      "Training loss: 0.19443966448307037\n",
      "Training loss: 0.2333897352218628\n",
      "Training loss: 0.2073211967945099\n",
      "Training loss: 0.24479055404663086\n",
      "Training loss: 0.20220260322093964\n",
      "Training loss: 0.21815960109233856\n",
      "Training loss: 0.17075207829475403\n",
      "Training loss: 0.20302289724349976\n",
      "Training loss: 0.23459261655807495\n",
      "Training loss: 0.2023329734802246\n",
      "Training loss: 0.20591849088668823\n",
      "Training loss: 0.2398657202720642\n",
      "Training loss: 0.20855894684791565\n",
      "Training loss: 0.17269128561019897\n",
      "Training loss: 0.19815921783447266\n",
      "Training loss: 0.23535948991775513\n",
      "Training loss: 0.18803241848945618\n",
      "Training loss: 0.18599602580070496\n",
      "Training loss: 0.21049177646636963\n",
      "Training loss: 0.17819422483444214\n",
      "Training loss: 0.2157926708459854\n",
      "Training loss: 0.22130601108074188\n",
      "Training loss: 0.22876602411270142\n",
      "Training loss: 0.21754008531570435\n",
      "Training loss: 0.20003588497638702\n",
      "Training loss: 0.19938477873802185\n",
      "Training loss: 0.19442853331565857\n",
      "Training loss: 0.18213893473148346\n",
      "Training loss: 0.19287636876106262\n",
      "Training loss: 0.20547500252723694\n",
      "Training loss: 0.1624135971069336\n",
      "Training loss: 0.18307140469551086\n",
      "Training loss: 0.1890527307987213\n",
      "Training loss: 0.2485705018043518\n",
      "Training loss: 0.1910560429096222\n",
      "Training loss: 0.2171270102262497\n",
      "Training loss: 0.1855376809835434\n",
      "Training loss: 0.20313698053359985\n",
      "Training loss: 0.20734699070453644\n",
      "Training loss: 0.1941344141960144\n",
      "Training loss: 0.18634194135665894\n",
      "Training loss: 0.2168773114681244\n",
      "Training loss: 0.21004198491573334\n",
      "Training loss: 0.1780480146408081\n",
      "Training loss: 0.24296069145202637\n",
      "Training loss: 0.16589735448360443\n",
      "Training loss: 0.19423708319664001\n",
      "Training loss: 0.2413751184940338\n",
      "Training loss: 0.24050191044807434\n",
      "Training loss: 0.18397635221481323\n",
      "Training loss: 0.19712324440479279\n",
      "Training loss: 0.2333359569311142\n",
      "Training loss: 0.18430322408676147\n",
      "Training loss: 0.1658123880624771\n",
      "Training loss: 0.22113804519176483\n",
      "Training loss: 0.18750441074371338\n",
      "Training loss: 0.22733110189437866\n",
      "Training loss: 0.19923004508018494\n",
      "Training loss: 0.23963810503482819\n",
      "Training loss: 0.19508984684944153\n",
      "Training loss: 0.2116149365901947\n",
      "Training loss: 0.1635877937078476\n",
      "Training loss: 0.19671092927455902\n",
      "Training loss: 0.2274930477142334\n",
      "Training loss: 0.19698387384414673\n",
      "Training loss: 0.20012563467025757\n",
      "Training loss: 0.2339048832654953\n",
      "Training loss: 0.2018282264471054\n",
      "Training loss: 0.16593186557292938\n",
      "Training loss: 0.19213561713695526\n",
      "Training loss: 0.2296251654624939\n",
      "Training loss: 0.1810990422964096\n",
      "Training loss: 0.17887838184833527\n",
      "Training loss: 0.20436745882034302\n",
      "Training loss: 0.17125147581100464\n",
      "Training loss: 0.20995484292507172\n",
      "Training loss: 0.2143997997045517\n",
      "Training loss: 0.2221381515264511\n",
      "Training loss: 0.21026751399040222\n",
      "Training loss: 0.19324055314064026\n",
      "Training loss: 0.19323498010635376\n",
      "Training loss: 0.1880442351102829\n",
      "Training loss: 0.1746101677417755\n",
      "Training loss: 0.1849396526813507\n",
      "Training loss: 0.19864502549171448\n",
      "Training loss: 0.15464292466640472\n",
      "Training loss: 0.17677181959152222\n",
      "Training loss: 0.18274620175361633\n",
      "Training loss: 0.2427201271057129\n",
      "Training loss: 0.1851530820131302\n",
      "Training loss: 0.2107713669538498\n",
      "Training loss: 0.17921075224876404\n",
      "Training loss: 0.19653062522411346\n",
      "Training loss: 0.20113873481750488\n",
      "Training loss: 0.18754179775714874\n",
      "Training loss: 0.17968998849391937\n",
      "Training loss: 0.21109749376773834\n",
      "Training loss: 0.20373933017253876\n",
      "Training loss: 0.17236541211605072\n",
      "Training loss: 0.23766198754310608\n",
      "Training loss: 0.15913498401641846\n",
      "Training loss: 0.18847200274467468\n",
      "Training loss: 0.23643338680267334\n",
      "Training loss: 0.23529797792434692\n",
      "Training loss: 0.1772509515285492\n",
      "Training loss: 0.19145341217517853\n",
      "Training loss: 0.2276485413312912\n",
      "Training loss: 0.17827899754047394\n",
      "Training loss: 0.15925724804401398\n",
      "Training loss: 0.2143363654613495\n",
      "Training loss: 0.18141308426856995\n",
      "Training loss: 0.22203654050827026\n",
      "Training loss: 0.19202803075313568\n",
      "Training loss: 0.235203817486763\n",
      "Training loss: 0.18882353603839874\n",
      "Training loss: 0.20586296916007996\n",
      "Training loss: 0.15729832649230957\n",
      "Training loss: 0.19115617871284485\n",
      "Training loss: 0.2211797833442688\n",
      "Training loss: 0.19237299263477325\n",
      "Training loss: 0.19511951506137848\n",
      "Training loss: 0.22871282696723938\n",
      "Training loss: 0.19590584933757782\n",
      "Training loss: 0.15999524295330048\n",
      "Training loss: 0.18689388036727905\n",
      "Training loss: 0.22463282942771912\n",
      "Training loss: 0.17497819662094116\n",
      "Training loss: 0.17257428169250488\n",
      "Training loss: 0.19897757470607758\n",
      "Training loss: 0.16511236131191254\n",
      "Training loss: 0.20489594340324402\n",
      "Training loss: 0.20828364789485931\n",
      "Training loss: 0.216281458735466\n",
      "Training loss: 0.2037879228591919\n",
      "Training loss: 0.18721453845500946\n",
      "Training loss: 0.18780502676963806\n",
      "Training loss: 0.18243296444416046\n",
      "Training loss: 0.16789747774600983\n",
      "Training loss: 0.1778324693441391\n",
      "Training loss: 0.1926002949476242\n",
      "Training loss: 0.147707000374794\n",
      "Training loss: 0.17123447358608246\n",
      "Training loss: 0.17719732224941254\n",
      "Training loss: 0.2375655472278595\n",
      "Training loss: 0.17995640635490417\n",
      "Training loss: 0.20515954494476318\n",
      "Training loss: 0.1736220270395279\n",
      "Training loss: 0.19066797196865082\n",
      "Training loss: 0.1956295371055603\n",
      "Training loss: 0.1817104071378708\n",
      "Training loss: 0.173794686794281\n",
      "Training loss: 0.2060033231973648\n",
      "Training loss: 0.1981574296951294\n",
      "Training loss: 0.16737575829029083\n",
      "Training loss: 0.233021080493927\n",
      "Training loss: 0.1531132161617279\n",
      "Training loss: 0.18340004980564117\n",
      "Training loss: 0.23211881518363953\n",
      "Training loss: 0.23071861267089844\n",
      "Training loss: 0.17126622796058655\n",
      "Training loss: 0.18646350502967834\n",
      "Training loss: 0.22264358401298523\n",
      "Training loss: 0.17294353246688843\n",
      "Training loss: 0.15341901779174805\n",
      "Training loss: 0.20826192200183868\n",
      "Training loss: 0.17602814733982086\n",
      "Training loss: 0.2173822671175003\n",
      "Training loss: 0.1855785995721817\n",
      "Training loss: 0.23136797547340393\n",
      "Training loss: 0.18326975405216217\n",
      "Training loss: 0.20077607035636902\n",
      "Training loss: 0.15174254775047302\n",
      "Training loss: 0.18623630702495575\n",
      "Training loss: 0.21553291380405426\n",
      "Training loss: 0.18837593495845795\n",
      "Training loss: 0.19077059626579285\n",
      "Training loss: 0.22416627407073975\n",
      "Training loss: 0.19066378474235535\n",
      "Training loss: 0.15474915504455566\n",
      "Training loss: 0.18230588734149933\n",
      "Training loss: 0.2202642858028412\n",
      "Training loss: 0.16954343020915985\n",
      "Training loss: 0.16695809364318848\n",
      "Training loss: 0.19420428574085236\n",
      "Training loss: 0.1596510112285614\n",
      "Training loss: 0.20049066841602325\n",
      "Training loss: 0.2028357833623886\n",
      "Training loss: 0.21107865869998932\n",
      "Training loss: 0.19798161089420319\n",
      "Training loss: 0.18183907866477966\n",
      "Training loss: 0.1829816997051239\n",
      "Training loss: 0.17747119069099426\n",
      "Training loss: 0.16187575459480286\n",
      "Training loss: 0.1714315116405487\n",
      "Training loss: 0.18722233176231384\n",
      "Training loss: 0.1414795070886612\n",
      "Training loss: 0.16633816063404083\n",
      "Training loss: 0.17228591442108154\n",
      "Training loss: 0.23300009965896606\n",
      "Training loss: 0.17535343766212463\n",
      "Training loss: 0.20017774403095245\n",
      "Training loss: 0.16865622997283936\n",
      "Training loss: 0.18543340265750885\n",
      "Training loss: 0.19071148335933685\n",
      "Training loss: 0.17652194201946259\n",
      "Training loss: 0.16854003071784973\n",
      "Training loss: 0.20148839056491852\n",
      "Training loss: 0.19318673014640808\n",
      "Training loss: 0.16296888887882233\n",
      "Training loss: 0.22893548011779785\n",
      "Training loss: 0.1477181613445282\n",
      "Training loss: 0.1789122223854065\n",
      "Training loss: 0.22833232581615448\n",
      "Training loss: 0.2266659140586853\n",
      "Training loss: 0.1659093201160431\n",
      "Training loss: 0.1820468157529831\n",
      "Training loss: 0.2182176411151886\n",
      "Training loss: 0.16818872094154358\n",
      "Training loss: 0.1481897234916687\n",
      "Training loss: 0.20280863344669342\n",
      "Training loss: 0.17123974859714508\n",
      "Training loss: 0.21326927840709686\n",
      "Training loss: 0.1797715425491333\n",
      "Training loss: 0.22803518176078796\n",
      "Training loss: 0.1783207803964615\n",
      "Training loss: 0.19625206291675568\n",
      "Training loss: 0.14680734276771545\n",
      "Training loss: 0.1818535327911377\n",
      "Training loss: 0.21045564115047455\n",
      "Training loss: 0.1848939210176468\n",
      "Training loss: 0.18697534501552582\n",
      "Training loss: 0.22016625106334686\n",
      "Training loss: 0.18599924445152283\n",
      "Training loss: 0.15008783340454102\n",
      "Training loss: 0.17826929688453674\n",
      "Training loss: 0.21642443537712097\n",
      "Training loss: 0.16469275951385498\n",
      "Training loss: 0.16192856431007385\n",
      "Training loss: 0.18995322287082672\n",
      "Training loss: 0.15476632118225098\n",
      "Training loss: 0.19663836061954498\n",
      "Training loss: 0.19795803725719452\n",
      "Training loss: 0.20643478631973267\n",
      "Training loss: 0.1927519589662552\n",
      "Training loss: 0.17701849341392517\n",
      "Training loss: 0.17867425084114075\n",
      "Training loss: 0.17305989563465118\n",
      "Training loss: 0.1564439982175827\n",
      "Training loss: 0.1656367927789688\n",
      "Training loss: 0.182415172457695\n",
      "Training loss: 0.1358581930398941\n",
      "Training loss: 0.1619855910539627\n",
      "Training loss: 0.1679154932498932\n",
      "Training loss: 0.22893767058849335\n",
      "Training loss: 0.1712537407875061\n",
      "Training loss: 0.1957339346408844\n",
      "Training loss: 0.16422048211097717\n",
      "Training loss: 0.18073394894599915\n",
      "Training loss: 0.18629775941371918\n",
      "Training loss: 0.17188102006912231\n",
      "Training loss: 0.16383203864097595\n",
      "Training loss: 0.19746695458889008\n",
      "Training loss: 0.18873830139636993\n",
      "Training loss: 0.15905636548995972\n",
      "Training loss: 0.2253226637840271\n",
      "Training loss: 0.1428578794002533\n",
      "Training loss: 0.1749206781387329\n",
      "Training loss: 0.22499412298202515\n",
      "Training loss: 0.22306124866008759\n",
      "Training loss: 0.16108904778957367\n",
      "Training loss: 0.17811746895313263\n",
      "Training loss: 0.21428672969341278\n",
      "Training loss: 0.16392770409584045\n",
      "Training loss: 0.1434820592403412\n",
      "Training loss: 0.1978897750377655\n",
      "Training loss: 0.16695933043956757\n",
      "Training loss: 0.20961761474609375\n",
      "Training loss: 0.17451730370521545\n",
      "Training loss: 0.22512860596179962\n",
      "Training loss: 0.17388930916786194\n",
      "Training loss: 0.19220833480358124\n",
      "Training loss: 0.1424012929201126\n",
      "Training loss: 0.1779288500547409\n",
      "Training loss: 0.20586912333965302\n",
      "Training loss: 0.1818474680185318\n",
      "Training loss: 0.18365031480789185\n",
      "Training loss: 0.2166324406862259\n",
      "Training loss: 0.1818287968635559\n",
      "Training loss: 0.14592544734477997\n",
      "Training loss: 0.17470142245292664\n",
      "Training loss: 0.2130359560251236\n",
      "Training loss: 0.16034294664859772\n",
      "Training loss: 0.15740317106246948\n",
      "Training loss: 0.18614816665649414\n",
      "Training loss: 0.15037602186203003\n",
      "Training loss: 0.193257138133049\n",
      "Training loss: 0.19357016682624817\n",
      "Training loss: 0.2022722214460373\n",
      "Training loss: 0.18801984190940857\n",
      "Training loss: 0.17267479002475739\n",
      "Training loss: 0.1748087853193283\n",
      "Training loss: 0.1691187471151352\n",
      "Training loss: 0.15151986479759216\n",
      "Training loss: 0.1603662073612213\n",
      "Training loss: 0.1781000792980194\n",
      "Training loss: 0.1307595670223236\n",
      "Training loss: 0.1580977886915207\n",
      "Training loss: 0.16400764882564545\n",
      "Training loss: 0.2253076732158661\n",
      "Training loss: 0.16758404672145844\n",
      "Training loss: 0.19175295531749725\n",
      "Training loss: 0.16023926436901093\n",
      "Training loss: 0.1764937937259674\n",
      "Training loss: 0.18231745064258575\n",
      "Training loss: 0.1677098572254181\n",
      "Training loss: 0.15959416329860687\n",
      "Training loss: 0.1938689798116684\n",
      "Training loss: 0.18473970890045166\n",
      "Training loss: 0.15556633472442627\n",
      "Training loss: 0.2221151888370514\n",
      "Training loss: 0.13845744729042053\n",
      "Training loss: 0.1713539958000183\n",
      "Training loss: 0.2220393717288971\n",
      "Training loss: 0.21984045207500458\n",
      "Training loss: 0.15673087537288666\n",
      "Training loss: 0.17460548877716064\n",
      "Training loss: 0.2107820212841034\n",
      "Training loss: 0.16008982062339783\n",
      "Training loss: 0.13922441005706787\n",
      "Training loss: 0.19343385100364685\n",
      "Training loss: 0.16311457753181458\n",
      "Training loss: 0.20636187493801117\n",
      "Training loss: 0.1697419136762619\n",
      "Training loss: 0.2225857377052307\n",
      "Training loss: 0.16990363597869873\n",
      "Training loss: 0.1885772943496704\n",
      "Training loss: 0.13844959437847137\n",
      "Training loss: 0.17439770698547363\n",
      "Training loss: 0.20170818269252777\n",
      "Training loss: 0.1791720688343048\n",
      "Training loss: 0.18072736263275146\n",
      "Training loss: 0.2134990692138672\n",
      "Training loss: 0.17808404564857483\n",
      "Training loss: 0.14219191670417786\n",
      "Training loss: 0.17153482139110565\n",
      "Training loss: 0.21003536880016327\n",
      "Training loss: 0.15642572939395905\n",
      "Training loss: 0.153314009308815\n",
      "Training loss: 0.1827266663312912\n",
      "Training loss: 0.14641256630420685\n",
      "Training loss: 0.19027994573116302\n",
      "Training loss: 0.1896061897277832\n",
      "Training loss: 0.19852672517299652\n",
      "Training loss: 0.18371988832950592\n",
      "Training loss: 0.1687437742948532\n",
      "Training loss: 0.17132461071014404\n",
      "Training loss: 0.16558200120925903\n",
      "Training loss: 0.14703553915023804\n",
      "Training loss: 0.1555517613887787\n",
      "Training loss: 0.1742118000984192\n",
      "Training loss: 0.12611450254917145\n",
      "Training loss: 0.1546100229024887\n",
      "Training loss: 0.16049817204475403\n",
      "Training loss: 0.2220517247915268\n",
      "Training loss: 0.16428427398204803\n",
      "Training loss: 0.18817275762557983\n",
      "Training loss: 0.1566503793001175\n",
      "Training loss: 0.17265069484710693\n",
      "Training loss: 0.17871227860450745\n",
      "Training loss: 0.1639445722103119\n",
      "Training loss: 0.1557631939649582\n",
      "Training loss: 0.19063693284988403\n",
      "Training loss: 0.18113085627555847\n",
      "Training loss: 0.15243972837924957\n",
      "Training loss: 0.21925756335258484\n",
      "Training loss: 0.13445508480072021\n",
      "Training loss: 0.1681535840034485\n",
      "Training loss: 0.21941478550434113\n",
      "Training loss: 0.21695099771022797\n",
      "Training loss: 0.15277324616909027\n",
      "Training loss: 0.1714535355567932\n",
      "Training loss: 0.20764648914337158\n",
      "Training loss: 0.15661728382110596\n",
      "Training loss: 0.13535770773887634\n",
      "Training loss: 0.1893814653158188\n",
      "Training loss: 0.1596461534500122\n",
      "Training loss: 0.20344819128513336\n",
      "Training loss: 0.16538424789905548\n",
      "Training loss: 0.22035524249076843\n",
      "Training loss: 0.16630451381206512\n",
      "Training loss: 0.18530336022377014\n",
      "Training loss: 0.134890615940094\n",
      "Training loss: 0.1712070107460022\n",
      "Training loss: 0.1979188770055771\n",
      "Training loss: 0.1768149882555008\n",
      "Training loss: 0.17815053462982178\n",
      "Training loss: 0.21071182191371918\n",
      "Training loss: 0.17470844089984894\n",
      "Training loss: 0.13882946968078613\n",
      "Training loss: 0.16871406137943268\n",
      "Training loss: 0.20737020671367645\n",
      "Training loss: 0.15288440883159637\n",
      "Training loss: 0.14960475265979767\n",
      "Training loss: 0.1796373724937439\n",
      "Training loss: 0.14282016456127167\n",
      "Training loss: 0.1876513510942459\n",
      "Training loss: 0.18601131439208984\n",
      "Training loss: 0.19514493644237518\n",
      "Training loss: 0.17979782819747925\n",
      "Training loss: 0.1651722490787506\n",
      "Training loss: 0.16817185282707214\n",
      "Training loss: 0.16239537298679352\n",
      "Training loss: 0.14293470978736877\n",
      "Training loss: 0.1511368304491043\n",
      "Training loss: 0.17069610953330994\n",
      "Training loss: 0.12186580151319504\n",
      "Training loss: 0.15146875381469727\n",
      "Training loss: 0.15733382105827332\n",
      "Training loss: 0.21912172436714172\n",
      "Training loss: 0.16130484640598297\n",
      "Training loss: 0.18494179844856262\n",
      "Training loss: 0.15340252220630646\n",
      "Training loss: 0.16915316879749298\n",
      "Training loss: 0.1754339635372162\n",
      "Training loss: 0.16053219139575958\n",
      "Training loss: 0.1522868275642395\n",
      "Training loss: 0.18772301077842712\n",
      "Training loss: 0.1778622269630432\n",
      "Training loss: 0.14962796866893768\n",
      "Training loss: 0.21670377254486084\n",
      "Training loss: 0.13079975545406342\n",
      "Training loss: 0.16527102887630463\n",
      "Training loss: 0.21707621216773987\n",
      "Training loss: 0.21434926986694336\n",
      "Training loss: 0.1491650640964508\n",
      "Training loss: 0.1686139702796936\n",
      "Training loss: 0.20483270287513733\n",
      "Training loss: 0.15346218645572662\n",
      "Training loss: 0.13183267414569855\n",
      "Training loss: 0.18568292260169983\n",
      "Training loss: 0.1565047949552536\n",
      "Training loss: 0.2008318454027176\n",
      "Training loss: 0.16139306128025055\n",
      "Training loss: 0.21839463710784912\n",
      "Training loss: 0.1630425900220871\n",
      "Training loss: 0.18234023451805115\n",
      "Training loss: 0.13167326152324677\n",
      "Training loss: 0.16831272840499878\n",
      "Training loss: 0.19445602595806122\n",
      "Training loss: 0.174732506275177\n",
      "Training loss: 0.17587342858314514\n",
      "Training loss: 0.20822548866271973\n",
      "Training loss: 0.17165488004684448\n",
      "Training loss: 0.13579007983207703\n",
      "Training loss: 0.1661931276321411\n",
      "Training loss: 0.2049967348575592\n",
      "Training loss: 0.14967168867588043\n",
      "Training loss: 0.14622820913791656\n",
      "Training loss: 0.1768375188112259\n",
      "Training loss: 0.13955214619636536\n",
      "Training loss: 0.18532514572143555\n",
      "Training loss: 0.18273966014385223\n",
      "Training loss: 0.1920819878578186\n",
      "Training loss: 0.176208034157753\n",
      "Training loss: 0.16191570460796356\n",
      "Training loss: 0.16530844569206238\n",
      "Training loss: 0.15951372683048248\n",
      "Training loss: 0.1391703188419342\n",
      "Training loss: 0.14707377552986145\n",
      "Training loss: 0.1675073653459549\n",
      "Training loss: 0.11796527355909348\n",
      "Training loss: 0.14862948656082153\n",
      "Training loss: 0.15447041392326355\n",
      "Training loss: 0.21647684276103973\n",
      "Training loss: 0.1586044430732727\n",
      "Training loss: 0.18201687932014465\n",
      "Training loss: 0.1504526436328888\n",
      "Training loss: 0.16595804691314697\n",
      "Training loss: 0.1724420189857483\n",
      "Training loss: 0.15742836892604828\n",
      "Training loss: 0.1491210162639618\n",
      "Training loss: 0.18508726358413696\n",
      "Training loss: 0.1748918890953064\n",
      "Training loss: 0.1470903605222702\n",
      "Training loss: 0.2144152969121933\n",
      "Training loss: 0.12744857370853424\n",
      "Training loss: 0.1626657098531723\n",
      "Training loss: 0.2149868607521057\n",
      "Training loss: 0.2119988650083542\n",
      "Training loss: 0.14586342871189117\n",
      "Training loss: 0.16604720056056976\n",
      "Training loss: 0.2023007869720459\n",
      "Training loss: 0.1505846232175827\n",
      "Training loss: 0.12860798835754395\n",
      "Training loss: 0.18229615688323975\n",
      "Training loss: 0.15364928543567657\n",
      "Training loss: 0.19847537577152252\n",
      "Training loss: 0.15772520005702972\n",
      "Training loss: 0.2166684865951538\n",
      "Training loss: 0.1600763350725174\n",
      "Training loss: 0.179649218916893\n",
      "Training loss: 0.12875458598136902\n",
      "Training loss: 0.16567790508270264\n",
      "Training loss: 0.19128131866455078\n",
      "Training loss: 0.17288833856582642\n",
      "Training loss: 0.1738572120666504\n",
      "Training loss: 0.20600219070911407\n",
      "Training loss: 0.1688838005065918\n",
      "Training loss: 0.13303348422050476\n",
      "Training loss: 0.16393359005451202\n",
      "Training loss: 0.20287826657295227\n",
      "Training loss: 0.14674778282642365\n",
      "Training loss: 0.14314477145671844\n",
      "Training loss: 0.1742912083864212\n",
      "Training loss: 0.13656926155090332\n",
      "Training loss: 0.18326260149478912\n",
      "Training loss: 0.17975251376628876\n",
      "Training loss: 0.189300075173378\n",
      "Training loss: 0.1729118973016739\n",
      "Training loss: 0.1589365303516388\n",
      "Training loss: 0.16269943118095398\n",
      "Training loss: 0.15689916908740997\n",
      "Training loss: 0.13570262491703033\n",
      "Training loss: 0.14332222938537598\n",
      "Training loss: 0.16460686922073364\n",
      "Training loss: 0.11437234282493591\n",
      "Training loss: 0.14605465531349182\n",
      "Training loss: 0.15187065303325653\n",
      "Training loss: 0.21408294141292572\n",
      "Training loss: 0.15614843368530273\n",
      "Training loss: 0.17936140298843384\n",
      "Training loss: 0.14776459336280823\n",
      "Training loss: 0.16302914917469025\n",
      "Training loss: 0.16970239579677582\n",
      "Training loss: 0.15459586679935455\n",
      "Training loss: 0.1462288200855255\n",
      "Training loss: 0.18269604444503784\n",
      "Training loss: 0.17218460142612457\n",
      "Training loss: 0.144792839884758\n",
      "Training loss: 0.2123596966266632\n",
      "Training loss: 0.12436555325984955\n",
      "Training loss: 0.1603037565946579\n",
      "Training loss: 0.21311572194099426\n",
      "Training loss: 0.2098691463470459\n",
      "Training loss: 0.14283223450183868\n",
      "Training loss: 0.16371971368789673\n",
      "Training loss: 0.20001694560050964\n",
      "Training loss: 0.14795100688934326\n",
      "Training loss: 0.1256486177444458\n",
      "Training loss: 0.17918559908866882\n",
      "Training loss: 0.15104499459266663\n",
      "Training loss: 0.1963471621274948\n",
      "Training loss: 0.15434393286705017\n",
      "Training loss: 0.21514692902565002\n",
      "Training loss: 0.1573707014322281\n",
      "Training loss: 0.1771976351737976\n",
      "Training loss: 0.12609843909740448\n",
      "Training loss: 0.16327138245105743\n",
      "Training loss: 0.18836233019828796\n",
      "Training loss: 0.17125196754932404\n",
      "Training loss: 0.17206917703151703\n",
      "Training loss: 0.2040097713470459\n",
      "Training loss: 0.16636167466640472\n",
      "Training loss: 0.13052569329738617\n",
      "Training loss: 0.1619029939174652\n",
      "Training loss: 0.2009836584329605\n",
      "Training loss: 0.1440788209438324\n",
      "Training loss: 0.14032061398029327\n",
      "Training loss: 0.1719682514667511\n",
      "Training loss: 0.13383829593658447\n",
      "Training loss: 0.18143102526664734\n",
      "Training loss: 0.17701706290245056\n",
      "Training loss: 0.1867668628692627\n",
      "Training loss: 0.16987666487693787\n",
      "Training loss: 0.15620304644107819\n",
      "Training loss: 0.16031506657600403\n",
      "Training loss: 0.1545197069644928\n",
      "Training loss: 0.13249799609184265\n",
      "Training loss: 0.13984782993793488\n",
      "Training loss: 0.16196176409721375\n",
      "Training loss: 0.11105254292488098\n",
      "Training loss: 0.14371278882026672\n",
      "Training loss: 0.14950314164161682\n",
      "Training loss: 0.2119108885526657\n",
      "Training loss: 0.15390759706497192\n",
      "Training loss: 0.17694444954395294\n",
      "Training loss: 0.14530779421329498\n",
      "Training loss: 0.16033580899238586\n",
      "Training loss: 0.16718623042106628\n",
      "Training loss: 0.15200309455394745\n",
      "Training loss: 0.14357882738113403\n",
      "Training loss: 0.18052077293395996\n",
      "Training loss: 0.1697104126214981\n",
      "Training loss: 0.14270660281181335\n",
      "Training loss: 0.21050943434238434\n",
      "Training loss: 0.12152013182640076\n",
      "Training loss: 0.15815632045269012\n",
      "Training loss: 0.21143661439418793\n",
      "Training loss: 0.20793423056602478\n",
      "Training loss: 0.140040785074234\n",
      "Training loss: 0.16160337626934052\n",
      "Training loss: 0.19795258343219757\n",
      "Training loss: 0.1455329954624176\n",
      "Training loss: 0.12292493879795074\n",
      "Training loss: 0.17632082104682922\n",
      "Training loss: 0.14866261184215546\n",
      "Training loss: 0.1944204568862915\n",
      "Training loss: 0.15121810138225555\n",
      "Training loss: 0.21380479633808136\n",
      "Training loss: 0.15489590167999268\n",
      "Training loss: 0.17495779693126678\n",
      "Training loss: 0.12367428839206696\n",
      "Training loss: 0.1610669046640396\n",
      "Training loss: 0.18567132949829102\n",
      "Training loss: 0.16979771852493286\n",
      "Training loss: 0.17048172652721405\n",
      "Training loss: 0.20222114026546478\n",
      "Training loss: 0.1640601009130478\n",
      "Training loss: 0.12823790311813354\n",
      "Training loss: 0.16007401049137115\n",
      "Training loss: 0.19928652048110962\n",
      "Training loss: 0.14163611829280853\n",
      "Training loss: 0.13772699236869812\n",
      "Training loss: 0.16984303295612335\n",
      "Training loss: 0.1313309669494629\n",
      "Training loss: 0.17980259656906128\n",
      "Training loss: 0.1745053082704544\n",
      "Training loss: 0.18445487320423126\n",
      "Training loss: 0.1670742630958557\n",
      "Training loss: 0.1536879539489746\n",
      "Training loss: 0.15812994539737701\n",
      "Training loss: 0.15234814584255219\n",
      "Training loss: 0.12952768802642822\n",
      "Training loss: 0.13662110269069672\n",
      "Training loss: 0.15954387187957764\n",
      "Training loss: 0.1079762727022171\n",
      "Training loss: 0.141576886177063\n",
      "Training loss: 0.14734120666980743\n",
      "Training loss: 0.2099357545375824\n",
      "Training loss: 0.15185703337192535\n",
      "Training loss: 0.17473942041397095\n",
      "Training loss: 0.14305609464645386\n",
      "Training loss: 0.15785178542137146\n",
      "Training loss: 0.1648688018321991\n",
      "Training loss: 0.1496230661869049\n",
      "Training loss: 0.14114412665367126\n",
      "Training loss: 0.17853710055351257\n",
      "Training loss: 0.1674436628818512\n",
      "Training loss: 0.14080710709095\n",
      "Training loss: 0.2088409662246704\n",
      "Training loss: 0.11888618767261505\n",
      "Training loss: 0.15619885921478271\n",
      "Training loss: 0.20992721617221832\n",
      "Training loss: 0.20617198944091797\n",
      "Training loss: 0.13746275007724762\n",
      "Training loss: 0.15967398881912231\n",
      "Training loss: 0.19608299434185028\n",
      "Training loss: 0.14330646395683289\n",
      "Training loss: 0.12041135877370834\n",
      "Training loss: 0.17367559671401978\n",
      "Training loss: 0.14647705852985382\n",
      "Training loss: 0.19267234206199646\n",
      "Training loss: 0.1483207494020462\n",
      "Training loss: 0.21262066066265106\n",
      "Training loss: 0.15262636542320251\n",
      "Training loss: 0.17290596663951874\n",
      "Training loss: 0.12145582586526871\n",
      "Training loss: 0.15904182195663452\n",
      "Training loss: 0.18318434059619904\n",
      "Training loss: 0.16850368678569794\n",
      "Training loss: 0.169071227312088\n",
      "Training loss: 0.20061299204826355\n",
      "Training loss: 0.16195465624332428\n",
      "Training loss: 0.12614545226097107\n",
      "Training loss: 0.15842320024967194\n",
      "Training loss: 0.19776421785354614\n",
      "Training loss: 0.13939493894577026\n",
      "Training loss: 0.1353391855955124\n",
      "Training loss: 0.16789348423480988\n",
      "Training loss: 0.1290230005979538\n",
      "Training loss: 0.17835356295108795\n",
      "Training loss: 0.17219319939613342\n",
      "Training loss: 0.18234029412269592\n",
      "Training loss: 0.16448047757148743\n",
      "Training loss: 0.15136802196502686\n",
      "Training loss: 0.15612240135669708\n",
      "Training loss: 0.15036126971244812\n",
      "Training loss: 0.12676696479320526\n",
      "Training loss: 0.13361665606498718\n",
      "Training loss: 0.15732893347740173\n",
      "Training loss: 0.10511820763349533\n",
      "Training loss: 0.13962388038635254\n",
      "Training loss: 0.14536181092262268\n",
      "Training loss: 0.20813621580600739\n",
      "Training loss: 0.1499755084514618\n",
      "Training loss: 0.17272353172302246\n",
      "Training loss: 0.14098715782165527\n",
      "Training loss: 0.15555472671985626\n",
      "Training loss: 0.16272899508476257\n",
      "Training loss: 0.14743271470069885\n",
      "Training loss: 0.13890168070793152\n",
      "Training loss: 0.17672406136989594\n",
      "Training loss: 0.1653621792793274\n",
      "Training loss: 0.1390734165906906\n",
      "Training loss: 0.2073342204093933\n",
      "Training loss: 0.11644136905670166\n",
      "Training loss: 0.15441040694713593\n",
      "Training loss: 0.20856834948062897\n",
      "Training loss: 0.20456340909004211\n",
      "Training loss: 0.13507559895515442\n",
      "Training loss: 0.15791095793247223\n",
      "Training loss: 0.1943870484828949\n",
      "Training loss: 0.14125066995620728\n",
      "Training loss: 0.11808596551418304\n",
      "Training loss: 0.17122730612754822\n",
      "Training loss: 0.14446689188480377\n",
      "Training loss: 0.19108310341835022\n",
      "Training loss: 0.1456286609172821\n",
      "Training loss: 0.21157613396644592\n",
      "Training loss: 0.15054012835025787\n",
      "Training loss: 0.17102184891700745\n",
      "Training loss: 0.11942064762115479\n",
      "Training loss: 0.15717682242393494\n",
      "Training loss: 0.1808808147907257\n",
      "Training loss: 0.16735117137432098\n",
      "Training loss: 0.16781744360923767\n",
      "Training loss: 0.1991652250289917\n",
      "Training loss: 0.16002430021762848\n",
      "Training loss: 0.12422715127468109\n",
      "Training loss: 0.15693050622940063\n",
      "Training loss: 0.19639727473258972\n",
      "Training loss: 0.1373339593410492\n",
      "Training loss: 0.1331358104944229\n",
      "Training loss: 0.16610080003738403\n",
      "Training loss: 0.12689344584941864\n",
      "Training loss: 0.1770634651184082\n",
      "Training loss: 0.17005988955497742\n",
      "Training loss: 0.18040256202220917\n",
      "Training loss: 0.16207431256771088\n",
      "Training loss: 0.14922301471233368\n",
      "Training loss: 0.15427367389202118\n",
      "Training loss: 0.14853905141353607\n",
      "Training loss: 0.12419452518224716\n",
      "Training loss: 0.13081245124340057\n",
      "Training loss: 0.15529584884643555\n",
      "Training loss: 0.10245632380247116\n",
      "Training loss: 0.1378341168165207\n",
      "Training loss: 0.14354532957077026\n",
      "Training loss: 0.20649369060993195\n",
      "Training loss: 0.14824475347995758\n",
      "Training loss: 0.17087697982788086\n",
      "Training loss: 0.13908162713050842\n",
      "Training loss: 0.1534251719713211\n",
      "Training loss: 0.1607484221458435\n",
      "Training loss: 0.14541204273700714\n",
      "Training loss: 0.13683146238327026\n",
      "Training loss: 0.17506363987922668\n",
      "Training loss: 0.16344688832759857\n",
      "Training loss: 0.13748742640018463\n",
      "Training loss: 0.20597165822982788\n",
      "Training loss: 0.11416622996330261\n",
      "Training loss: 0.15277281403541565\n",
      "Training loss: 0.20734359323978424\n",
      "Training loss: 0.20309215784072876\n",
      "Training loss: 0.13285976648330688\n",
      "Training loss: 0.156296506524086\n",
      "Training loss: 0.1928464025259018\n",
      "Training loss: 0.13934779167175293\n",
      "Training loss: 0.11592967808246613\n",
      "Training loss: 0.16895624995231628\n",
      "Training loss: 0.14261355996131897\n",
      "Training loss: 0.18963579833507538\n",
      "Training loss: 0.1431216597557068\n",
      "Training loss: 0.21065537631511688\n",
      "Training loss: 0.14861811697483063\n",
      "Training loss: 0.1692878007888794\n",
      "Training loss: 0.11754929274320602\n",
      "Training loss: 0.15545515716075897\n",
      "Training loss: 0.1787426620721817\n",
      "Training loss: 0.1663241684436798\n",
      "Training loss: 0.16670292615890503\n",
      "Training loss: 0.19786056876182556\n",
      "Training loss: 0.15825088322162628\n",
      "Training loss: 0.12246468663215637\n",
      "Training loss: 0.1555786430835724\n",
      "Training loss: 0.19516882300376892\n",
      "Training loss: 0.13543470203876495\n",
      "Training loss: 0.13109831511974335\n",
      "Training loss: 0.16444869339466095\n",
      "Training loss: 0.12492415308952332\n",
      "Training loss: 0.1759146749973297\n",
      "Training loss: 0.16808734834194183\n",
      "Training loss: 0.17862384021282196\n",
      "Training loss: 0.15983755886554718\n",
      "Training loss: 0.14723548293113708\n",
      "Training loss: 0.1525675654411316\n",
      "Training loss: 0.14686426520347595\n",
      "Training loss: 0.12179185450077057\n",
      "Training loss: 0.1281893253326416\n",
      "Training loss: 0.15342631936073303\n",
      "Training loss: 0.0999714657664299\n",
      "Training loss: 0.13619031012058258\n",
      "Training loss: 0.1418747454881668\n",
      "Training loss: 0.20499210059642792\n",
      "Training loss: 0.14664897322654724\n",
      "Training loss: 0.169182687997818\n",
      "Training loss: 0.13732284307479858\n",
      "Training loss: 0.1514464169740677\n",
      "Training loss: 0.15891124308109283\n",
      "Training loss: 0.14354385435581207\n",
      "Training loss: 0.1349162757396698\n",
      "Training loss: 0.1735401451587677\n",
      "Training loss: 0.16168123483657837\n",
      "Training loss: 0.13603362441062927\n",
      "Training loss: 0.2047382891178131\n",
      "Training loss: 0.11204411089420319\n",
      "Training loss: 0.15127044916152954\n",
      "Training loss: 0.20623862743377686\n",
      "Training loss: 0.20174407958984375\n",
      "Training loss: 0.1307983547449112\n",
      "Training loss: 0.15481513738632202\n",
      "Training loss: 0.19144509732723236\n",
      "Training loss: 0.13758237659931183\n",
      "Training loss: 0.1139260083436966\n",
      "Training loss: 0.1668453961610794\n",
      "Training loss: 0.14090107381343842\n",
      "Training loss: 0.18831570446491241\n",
      "Training loss: 0.1407821774482727\n",
      "Training loss: 0.20984478294849396\n",
      "Training loss: 0.14684389531612396\n",
      "Training loss: 0.16768863797187805\n",
      "Training loss: 0.11582503467798233\n",
      "Training loss: 0.15386241674423218\n",
      "Training loss: 0.17675425112247467\n",
      "Training loss: 0.16540879011154175\n",
      "Training loss: 0.1657126098871231\n",
      "Training loss: 0.19668404757976532\n",
      "Training loss: 0.1566186398267746\n",
      "Training loss: 0.12084226310253143\n",
      "Training loss: 0.15435267984867096\n",
      "Training loss: 0.19406437873840332\n",
      "Training loss: 0.1336812525987625\n",
      "Training loss: 0.12921056151390076\n",
      "Training loss: 0.16292288899421692\n",
      "Training loss: 0.12309938669204712\n",
      "Training loss: 0.17489202320575714\n",
      "Training loss: 0.16625989973545074\n",
      "Training loss: 0.17698849737644196\n",
      "Training loss: 0.15775427222251892\n",
      "Training loss: 0.14539016783237457\n",
      "Training loss: 0.15099011361598969\n",
      "Training loss: 0.14532190561294556\n",
      "Training loss: 0.11954290419816971\n",
      "Training loss: 0.12573054432868958\n",
      "Training loss: 0.15170440077781677\n",
      "Training loss: 0.09764710068702698\n",
      "Training loss: 0.13467766344547272\n",
      "Training loss: 0.14033518731594086\n",
      "Training loss: 0.20361757278442383\n",
      "Training loss: 0.14517447352409363\n",
      "Training loss: 0.16762559115886688\n",
      "Training loss: 0.1356962025165558\n",
      "Training loss: 0.14960387349128723\n",
      "Training loss: 0.15720362961292267\n",
      "Training loss: 0.1418130248785019\n",
      "Training loss: 0.13314098119735718\n",
      "Training loss: 0.1721399426460266\n",
      "Training loss: 0.16005069017410278\n",
      "Training loss: 0.13469842076301575\n",
      "Training loss: 0.20362086594104767\n",
      "Training loss: 0.11006038635969162\n",
      "Training loss: 0.14988970756530762\n",
      "Training loss: 0.20524108409881592\n",
      "Training loss: 0.20050674676895142\n",
      "Training loss: 0.12887662649154663\n",
      "Training loss: 0.153453528881073\n",
      "Training loss: 0.19016925990581512\n",
      "Training loss: 0.1359410136938095\n",
      "Training loss: 0.11206046491861343\n",
      "Training loss: 0.16487973928451538\n",
      "Training loss: 0.13931550085544586\n",
      "Training loss: 0.18710996210575104\n",
      "Training loss: 0.13859476149082184\n",
      "Training loss: 0.2091323733329773\n",
      "Training loss: 0.14520299434661865\n",
      "Training loss: 0.16621103882789612\n",
      "Training loss: 0.11423318088054657\n",
      "Training loss: 0.1523859202861786\n",
      "Training loss: 0.17490175366401672\n",
      "Training loss: 0.16459301114082336\n",
      "Training loss: 0.16483335196971893\n",
      "Training loss: 0.1956225037574768\n",
      "Training loss: 0.15511375665664673\n",
      "Training loss: 0.11934596300125122\n",
      "Training loss: 0.15323957800865173\n",
      "Training loss: 0.19307102262973785\n",
      "Training loss: 0.13205945491790771\n",
      "Training loss: 0.1274583786725998\n",
      "Training loss: 0.16151106357574463\n",
      "Training loss: 0.12140528112649918\n",
      "Training loss: 0.17398208379745483\n",
      "Training loss: 0.16456368565559387\n",
      "Training loss: 0.1754828691482544\n",
      "Training loss: 0.1558104008436203\n",
      "Training loss: 0.14367365837097168\n",
      "Training loss: 0.14952896535396576\n",
      "Training loss: 0.14389894902706146\n",
      "Training loss: 0.11743347346782684\n",
      "Training loss: 0.12342134863138199\n",
      "Training loss: 0.15011608600616455\n",
      "Training loss: 0.09546859562397003\n",
      "Training loss: 0.13328306376934052\n",
      "Training loss: 0.13891375064849854\n",
      "Training loss: 0.20235773921012878\n",
      "Training loss: 0.14380931854248047\n",
      "Training loss: 0.16619254648685455\n",
      "Training loss: 0.13418899476528168\n",
      "Training loss: 0.14788472652435303\n",
      "Training loss: 0.1556134819984436\n",
      "Training loss: 0.14020651578903198\n",
      "Training loss: 0.13149240612983704\n",
      "Training loss: 0.17085111141204834\n",
      "Training loss: 0.15854258835315704\n",
      "Training loss: 0.1334700584411621\n",
      "Training loss: 0.20260784029960632\n",
      "Training loss: 0.10820230841636658\n",
      "Training loss: 0.1486186981201172\n",
      "Training loss: 0.20434004068374634\n",
      "Training loss: 0.1993694305419922\n",
      "Training loss: 0.12708163261413574\n",
      "Training loss: 0.1521999090909958\n",
      "Training loss: 0.18900662660598755\n",
      "Training loss: 0.13441191613674164\n",
      "Training loss: 0.11032034456729889\n",
      "Training loss: 0.16304603219032288\n",
      "Training loss: 0.13784462213516235\n",
      "Training loss: 0.18600736558437347\n",
      "Training loss: 0.13654577732086182\n",
      "Training loss: 0.20850783586502075\n",
      "Training loss: 0.14368273317813873\n",
      "Training loss: 0.16484332084655762\n",
      "Training loss: 0.11276093125343323\n",
      "Training loss: 0.15101458132266998\n",
      "Training loss: 0.17317306995391846\n",
      "Training loss: 0.16386637091636658\n",
      "Training loss: 0.16405361890792847\n",
      "Training loss: 0.19466450810432434\n",
      "Training loss: 0.15372410416603088\n",
      "Training loss: 0.11796371638774872\n",
      "Training loss: 0.1522279977798462\n",
      "Training loss: 0.19217771291732788\n",
      "Training loss: 0.1305570900440216\n",
      "Training loss: 0.12582936882972717\n",
      "Training loss: 0.16020238399505615\n",
      "Training loss: 0.11982976645231247\n",
      "Training loss: 0.17317324876785278\n",
      "Training loss: 0.16298672556877136\n",
      "Training loss: 0.17409482598304749\n",
      "Training loss: 0.1539936661720276\n",
      "Training loss: 0.14207425713539124\n",
      "Training loss: 0.1481732726097107\n",
      "Training loss: 0.14258386194705963\n",
      "Training loss: 0.11545120179653168\n",
      "Training loss: 0.1212487667798996\n",
      "Training loss: 0.1486489325761795\n",
      "Training loss: 0.0934230163693428\n",
      "Training loss: 0.13199521601200104\n",
      "Training loss: 0.1375991404056549\n",
      "Training loss: 0.20120179653167725\n",
      "Training loss: 0.1425430178642273\n",
      "Training loss: 0.16487203538417816\n",
      "Training loss: 0.13279007375240326\n",
      "Training loss: 0.1462777853012085\n",
      "Training loss: 0.15413008630275726\n",
      "Training loss: 0.13871267437934875\n",
      "Training loss: 0.1299588829278946\n",
      "Training loss: 0.16966308653354645\n",
      "Training loss: 0.1571456640958786\n",
      "Training loss: 0.13233809173107147\n",
      "Training loss: 0.20168915390968323\n",
      "Training loss: 0.10645860433578491\n",
      "Training loss: 0.1474469006061554\n",
      "Training loss: 0.20352603495121002\n",
      "Training loss: 0.198322594165802\n",
      "Training loss: 0.12540192902088165\n",
      "Training loss: 0.15104398131370544\n",
      "Training loss: 0.18794646859169006\n",
      "Training loss: 0.13298477232456207\n",
      "Training loss: 0.10869442671537399\n",
      "Training loss: 0.1613326370716095\n",
      "Training loss: 0.13647772371768951\n",
      "Training loss: 0.18499800562858582\n",
      "Training loss: 0.13462330400943756\n",
      "Training loss: 0.20796209573745728\n",
      "Training loss: 0.1422719657421112\n",
      "Training loss: 0.16357532143592834\n",
      "Training loss: 0.11139700561761856\n",
      "Training loss: 0.14973875880241394\n",
      "Training loss: 0.1715574413537979\n",
      "Training loss: 0.1632196307182312\n",
      "Training loss: 0.16336339712142944\n",
      "Training loss: 0.1937999129295349\n",
      "Training loss: 0.15243901312351227\n",
      "Training loss: 0.11668483912944794\n",
      "Training loss: 0.1513078808784485\n",
      "Training loss: 0.1913744956254959\n",
      "Training loss: 0.12916310131549835\n",
      "Training loss: 0.1243123859167099\n",
      "Training loss: 0.158987358212471\n",
      "Training loss: 0.11836209893226624\n",
      "Training loss: 0.17245523631572723\n",
      "Training loss: 0.16151820123195648\n",
      "Training loss: 0.17281374335289001\n",
      "Training loss: 0.1522931009531021\n",
      "Training loss: 0.14058156311511993\n",
      "Training loss: 0.14691339433193207\n",
      "Training loss: 0.1413666009902954\n",
      "Training loss: 0.11358507722616196\n",
      "Training loss: 0.11920125782489777\n",
      "Training loss: 0.14729198813438416\n",
      "Training loss: 0.09149899333715439\n",
      "Training loss: 0.13080407679080963\n",
      "Training loss: 0.1363813281059265\n",
      "Training loss: 0.20014017820358276\n",
      "Training loss: 0.14136634767055511\n",
      "Training loss: 0.16365379095077515\n",
      "Training loss: 0.13148950040340424\n",
      "Training loss: 0.14477312564849854\n",
      "Training loss: 0.15274401009082794\n",
      "Training loss: 0.137321338057518\n",
      "Training loss: 0.1285301297903061\n",
      "Training loss: 0.16856667399406433\n",
      "Training loss: 0.15584997832775116\n",
      "Training loss: 0.13129346072673798\n",
      "Training loss: 0.2008558213710785\n",
      "Training loss: 0.10481929779052734\n",
      "Training loss: 0.1463652104139328\n",
      "Training loss: 0.20279063284397125\n",
      "Training loss: 0.19735780358314514\n",
      "Training loss: 0.12382746487855911\n",
      "Training loss: 0.1499766707420349\n",
      "Training loss: 0.1869792342185974\n",
      "Training loss: 0.13165049254894257\n",
      "Training loss: 0.10717278718948364\n",
      "Training loss: 0.15972916781902313\n",
      "Training loss: 0.13520534336566925\n",
      "Training loss: 0.18407313525676727\n",
      "Training loss: 0.1328166127204895\n",
      "Training loss: 0.2074870467185974\n",
      "Training loss: 0.14096085727214813\n",
      "Training loss: 0.16239793598651886\n",
      "Training loss: 0.11013147979974747\n",
      "Training loss: 0.14854978024959564\n",
      "Training loss: 0.17004525661468506\n",
      "Training loss: 0.16264477372169495\n",
      "Training loss: 0.16275373101234436\n",
      "Training loss: 0.1930198073387146\n",
      "Training loss: 0.1512490063905716\n",
      "Training loss: 0.11549988389015198\n",
      "Training loss: 0.15047048032283783\n",
      "Training loss: 0.19065281748771667\n",
      "Training loss: 0.12786802649497986\n",
      "Training loss: 0.12289779633283615\n",
      "Training loss: 0.15785743296146393\n",
      "Training loss: 0.11699281632900238\n",
      "Training loss: 0.17181910574436188\n",
      "Training loss: 0.1601487100124359\n",
      "Training loss: 0.1716301143169403\n",
      "Training loss: 0.1506989449262619\n",
      "Training loss: 0.13918635249137878\n",
      "Training loss: 0.14574101567268372\n",
      "Training loss: 0.1402382254600525\n",
      "Training loss: 0.11182540655136108\n",
      "Training loss: 0.1172686517238617\n",
      "Training loss: 0.14603553712368011\n",
      "Training loss: 0.08968642354011536\n",
      "Training loss: 0.12970077991485596\n",
      "Training loss: 0.1352515071630478\n",
      "Training loss: 0.19916440546512604\n",
      "Training loss: 0.14027123153209686\n",
      "Training loss: 0.16252870857715607\n",
      "Training loss: 0.13027863204479218\n",
      "Training loss: 0.14336197078227997\n",
      "Training loss: 0.15144696831703186\n",
      "Training loss: 0.13602349162101746\n",
      "Training loss: 0.1271970421075821\n",
      "Training loss: 0.1675536185503006\n",
      "Training loss: 0.15464675426483154\n",
      "Training loss: 0.13032811880111694\n",
      "Training loss: 0.200099915266037\n",
      "Training loss: 0.1032756119966507\n",
      "Training loss: 0.14536535739898682\n",
      "Training loss: 0.20212647318840027\n",
      "Training loss: 0.1964678317308426\n",
      "Training loss: 0.12234926223754883\n",
      "Training loss: 0.14899000525474548\n",
      "Training loss: 0.18609647452831268\n",
      "Training loss: 0.1304011344909668\n",
      "Training loss: 0.10574669390916824\n",
      "Training loss: 0.15822646021842957\n",
      "Training loss: 0.1340191513299942\n",
      "Training loss: 0.18322508037090302\n",
      "Training loss: 0.13111639022827148\n",
      "Training loss: 0.2070758044719696\n",
      "Training loss: 0.1397407203912735\n",
      "Training loss: 0.16130319237709045\n",
      "Training loss: 0.10895554721355438\n",
      "Training loss: 0.14744015038013458\n",
      "Training loss: 0.16862809658050537\n",
      "Training loss: 0.16213470697402954\n",
      "Training loss: 0.16221681237220764\n",
      "Training loss: 0.1923162341117859\n",
      "Training loss: 0.15014581382274628\n",
      "Training loss: 0.11440053582191467\n",
      "Training loss: 0.14970800280570984\n",
      "Training loss: 0.19000482559204102\n",
      "Training loss: 0.1266632080078125\n",
      "Training loss: 0.12157687544822693\n",
      "Training loss: 0.15680532157421112\n",
      "Training loss: 0.11571354418992996\n",
      "Training loss: 0.17125676572322845\n",
      "Training loss: 0.15886978805065155\n",
      "Training loss: 0.17053556442260742\n",
      "Training loss: 0.14920258522033691\n",
      "Training loss: 0.13788048923015594\n",
      "Training loss: 0.14464858174324036\n",
      "Training loss: 0.1391909271478653\n",
      "Training loss: 0.11016358435153961\n",
      "Training loss: 0.11544181406497955\n",
      "Training loss: 0.14487093687057495\n",
      "Training loss: 0.08797634392976761\n",
      "Training loss: 0.12867745757102966\n",
      "Training loss: 0.13420191407203674\n",
      "Training loss: 0.19826698303222656\n",
      "Training loss: 0.13925044238567352\n",
      "Training loss: 0.16148874163627625\n",
      "Training loss: 0.12914970517158508\n",
      "Training loss: 0.1420365869998932\n",
      "Training loss: 0.1502315104007721\n",
      "Training loss: 0.1348111778497696\n",
      "Training loss: 0.12595157325267792\n",
      "Training loss: 0.16661660373210907\n",
      "Training loss: 0.15352822840213776\n",
      "Training loss: 0.12943492829799652\n",
      "Training loss: 0.19941435754299164\n",
      "Training loss: 0.10181982070207596\n",
      "Training loss: 0.14444024860858917\n",
      "Training loss: 0.20152686536312103\n",
      "Training loss: 0.19564583897590637\n",
      "Training loss: 0.12095950543880463\n",
      "Training loss: 0.14807675778865814\n",
      "Training loss: 0.18529070913791656\n",
      "Training loss: 0.1292293816804886\n",
      "Training loss: 0.10440822690725327\n",
      "Training loss: 0.15681636333465576\n",
      "Training loss: 0.13291174173355103\n",
      "Training loss: 0.18244697153568268\n",
      "Training loss: 0.12951399385929108\n",
      "Training loss: 0.20672191679477692\n",
      "Training loss: 0.13860367238521576\n",
      "Training loss: 0.16028395295143127\n",
      "Training loss: 0.1078614890575409\n",
      "Training loss: 0.1464030146598816\n",
      "Training loss: 0.16729827225208282\n",
      "Training loss: 0.1616831123828888\n",
      "Training loss: 0.1617456078529358\n",
      "Training loss: 0.19168226420879364\n",
      "Training loss: 0.14912188053131104\n",
      "Training loss: 0.11337938904762268\n",
      "Training loss: 0.14901357889175415\n",
      "Training loss: 0.1894238144159317\n",
      "Training loss: 0.12554113566875458\n",
      "Training loss: 0.12034192681312561\n",
      "Training loss: 0.15582416951656342\n",
      "Training loss: 0.1145167350769043\n",
      "Training loss: 0.17076125741004944\n",
      "Training loss: 0.1576739102602005\n",
      "Training loss: 0.1695224940776825\n",
      "Training loss: 0.14779621362686157\n",
      "Training loss: 0.13665655255317688\n",
      "Training loss: 0.1436295360326767\n",
      "Training loss: 0.13821764290332794\n",
      "Training loss: 0.10859188437461853\n",
      "Training loss: 0.11371257156133652\n",
      "Training loss: 0.14379043877124786\n",
      "Training loss: 0.08636074513196945\n",
      "Training loss: 0.12772707641124725\n",
      "Training loss: 0.13322553038597107\n",
      "Training loss: 0.1974412500858307\n",
      "Training loss: 0.13829760253429413\n",
      "Training loss: 0.1605266034603119\n",
      "Training loss: 0.12809579074382782\n",
      "Training loss: 0.14079000055789948\n",
      "Training loss: 0.149090975522995\n",
      "Training loss: 0.13367719948291779\n",
      "Training loss: 0.12478649616241455\n",
      "Training loss: 0.16574914753437042\n",
      "Training loss: 0.1524873673915863\n",
      "Training loss: 0.1286076307296753\n",
      "Training loss: 0.19879290461540222\n",
      "Training loss: 0.10044491291046143\n",
      "Training loss: 0.14358340203762054\n",
      "Training loss: 0.20098598301410675\n",
      "Training loss: 0.19488611817359924\n",
      "Training loss: 0.11965101957321167\n",
      "Training loss: 0.14723065495491028\n",
      "Training loss: 0.18455515801906586\n",
      "Training loss: 0.12812896072864532\n",
      "Training loss: 0.1031503677368164\n",
      "Training loss: 0.15549145638942719\n",
      "Training loss: 0.13187648355960846\n",
      "Training loss: 0.18173271417617798\n",
      "Training loss: 0.12800189852714539\n",
      "Training loss: 0.20641987025737762\n",
      "Training loss: 0.13754288852214813\n",
      "Training loss: 0.15933384001255035\n",
      "Training loss: 0.10684224963188171\n",
      "Training loss: 0.14543238282203674\n",
      "Training loss: 0.16604900360107422\n",
      "Training loss: 0.16128438711166382\n",
      "Training loss: 0.16133397817611694\n",
      "Training loss: 0.19111156463623047\n",
      "Training loss: 0.148170605301857\n",
      "Training loss: 0.11242981255054474\n",
      "Training loss: 0.1483810395002365\n",
      "Training loss: 0.18890361487865448\n",
      "Training loss: 0.12449493259191513\n",
      "Training loss: 0.11918599903583527\n",
      "Training loss: 0.15490812063217163\n",
      "Training loss: 0.11339572072029114\n",
      "Training loss: 0.17032620310783386\n",
      "Training loss: 0.15655437111854553\n",
      "Training loss: 0.16858413815498352\n",
      "Training loss: 0.14647288620471954\n",
      "Training loss: 0.13550803065299988\n",
      "Training loss: 0.14267782866954803\n",
      "Training loss: 0.13731206953525543\n",
      "Training loss: 0.10710339993238449\n",
      "Training loss: 0.112073615193367\n",
      "Training loss: 0.1427871137857437\n",
      "Training loss: 0.08483238518238068\n",
      "Training loss: 0.12684345245361328\n",
      "Training loss: 0.13231617212295532\n",
      "Training loss: 0.19668112695217133\n",
      "Training loss: 0.13740697503089905\n",
      "Training loss: 0.15963581204414368\n",
      "Training loss: 0.12711074948310852\n",
      "Training loss: 0.1396159827709198\n",
      "Training loss: 0.1480194479227066\n",
      "Training loss: 0.13261520862579346\n",
      "Training loss: 0.12369531393051147\n",
      "Training loss: 0.16494545340538025\n",
      "Training loss: 0.15151789784431458\n",
      "Training loss: 0.12784045934677124\n",
      "Training loss: 0.1982298493385315\n",
      "Training loss: 0.09914463758468628\n",
      "Training loss: 0.1427890658378601\n",
      "Training loss: 0.20049865543842316\n",
      "Training loss: 0.19418324530124664\n",
      "Training loss: 0.11841750890016556\n",
      "Training loss: 0.14644595980644226\n",
      "Training loss: 0.18388380110263824\n",
      "Training loss: 0.12709414958953857\n",
      "Training loss: 0.10196679085493088\n",
      "Training loss: 0.15424516797065735\n",
      "Training loss: 0.13090747594833374\n",
      "Training loss: 0.1810767501592636\n",
      "Training loss: 0.12657327950000763\n",
      "Training loss: 0.20616468787193298\n",
      "Training loss: 0.13655206561088562\n",
      "Training loss: 0.15844722092151642\n",
      "Training loss: 0.10589165985584259\n",
      "Training loss: 0.14452286064624786\n",
      "Training loss: 0.1648741513490677\n",
      "Training loss: 0.16093353927135468\n",
      "Training loss: 0.16097630560398102\n",
      "Training loss: 0.1905984878540039\n",
      "Training loss: 0.14728599786758423\n",
      "Training loss: 0.1115458756685257\n",
      "Training loss: 0.1478048712015152\n",
      "Training loss: 0.1884387880563736\n",
      "Training loss: 0.12351851910352707\n",
      "Training loss: 0.11810282617807388\n",
      "Training loss: 0.15405182540416718\n",
      "Training loss: 0.11234447360038757\n",
      "Training loss: 0.16994598507881165\n",
      "Training loss: 0.15550510585308075\n",
      "Training loss: 0.167714461684227\n",
      "Training loss: 0.14522632956504822\n",
      "Training loss: 0.13442906737327576\n",
      "Training loss: 0.1417880356311798\n",
      "Training loss: 0.13646867871284485\n",
      "Training loss: 0.10569190979003906\n",
      "Training loss: 0.11051835119724274\n",
      "Training loss: 0.14185470342636108\n",
      "Training loss: 0.08338478207588196\n",
      "Training loss: 0.12602104246616364\n",
      "Training loss: 0.13146831095218658\n",
      "Training loss: 0.19598111510276794\n",
      "Training loss: 0.13657349348068237\n",
      "Training loss: 0.15881063044071198\n",
      "Training loss: 0.12618903815746307\n",
      "Training loss: 0.13850896060466766\n",
      "Training loss: 0.14701160788536072\n",
      "Training loss: 0.1316194385290146\n",
      "Training loss: 0.1226721778512001\n",
      "Training loss: 0.16420020163059235\n",
      "Training loss: 0.15061406791210175\n",
      "Training loss: 0.12712842226028442\n",
      "Training loss: 0.1977202147245407\n",
      "Training loss: 0.09791339933872223\n",
      "Training loss: 0.14205209910869598\n",
      "Training loss: 0.20006006956100464\n",
      "Training loss: 0.19353261590003967\n",
      "Training loss: 0.11725322902202606\n",
      "Training loss: 0.14571760594844818\n",
      "Training loss: 0.18327125906944275\n",
      "Training loss: 0.12611982226371765\n",
      "Training loss: 0.1008518636226654\n",
      "Training loss: 0.1530715376138687\n",
      "Training loss: 0.12999941408634186\n",
      "Training loss: 0.18047426640987396\n",
      "Training loss: 0.12522196769714355\n",
      "Training loss: 0.20595192909240723\n",
      "Training loss: 0.13562563061714172\n",
      "Training loss: 0.1576189249753952\n",
      "Training loss: 0.10500413179397583\n",
      "Training loss: 0.14366957545280457\n",
      "Training loss: 0.16376812756061554\n",
      "Training loss: 0.1606261432170868\n",
      "Training loss: 0.16066765785217285\n",
      "Training loss: 0.1901380717754364\n",
      "Training loss: 0.14646269381046295\n",
      "Training loss: 0.11072226613759995\n",
      "Training loss: 0.14728021621704102\n",
      "Training loss: 0.1880243867635727\n",
      "Training loss: 0.12260635942220688\n",
      "Training loss: 0.11708685755729675\n",
      "Training loss: 0.1532505452632904\n",
      "Training loss: 0.11135756969451904\n",
      "Training loss: 0.16961553692817688\n",
      "Training loss: 0.1545206904411316\n",
      "Training loss: 0.16690793633460999\n",
      "Training loss: 0.14405089616775513\n",
      "Training loss: 0.13341432809829712\n",
      "Training loss: 0.14095547795295715\n",
      "Training loss: 0.13568241894245148\n",
      "Training loss: 0.10435189306735992\n",
      "Training loss: 0.10904085636138916\n",
      "Training loss: 0.1409875601530075\n",
      "Training loss: 0.0820121243596077\n",
      "Training loss: 0.12525467574596405\n",
      "Training loss: 0.13067689538002014\n",
      "Training loss: 0.1953364759683609\n",
      "Training loss: 0.13579252362251282\n",
      "Training loss: 0.1580456793308258\n",
      "Training loss: 0.12532566487789154\n",
      "Training loss: 0.13746395707130432\n",
      "Training loss: 0.14606264233589172\n",
      "Training loss: 0.13068479299545288\n",
      "Training loss: 0.12171192467212677\n",
      "Training loss: 0.163508802652359\n",
      "Training loss: 0.14977096021175385\n",
      "Training loss: 0.12646695971488953\n",
      "Training loss: 0.19725938141345978\n",
      "Training loss: 0.09674616903066635\n",
      "Training loss: 0.1413678526878357\n",
      "Training loss: 0.19966605305671692\n",
      "Training loss: 0.19293001294136047\n",
      "Training loss: 0.11615303158760071\n",
      "Training loss: 0.14504104852676392\n",
      "Training loss: 0.1827126443386078\n",
      "Training loss: 0.12520146369934082\n",
      "Training loss: 0.09980051219463348\n",
      "Training loss: 0.1519651859998703\n",
      "Training loss: 0.12914755940437317\n",
      "Training loss: 0.17992077767848969\n",
      "Training loss: 0.12394248694181442\n",
      "Training loss: 0.2057776004076004\n",
      "Training loss: 0.1347585916519165\n",
      "Training loss: 0.1568443775177002\n",
      "Training loss: 0.1041746735572815\n",
      "Training loss: 0.1428682506084442\n",
      "Training loss: 0.16272592544555664\n",
      "Training loss: 0.1603582203388214\n",
      "Training loss: 0.1604035496711731\n",
      "Training loss: 0.18972571194171906\n",
      "Training loss: 0.14569585025310516\n",
      "Training loss: 0.10995419323444366\n",
      "Training loss: 0.1468026340007782\n",
      "Training loss: 0.18765604496002197\n",
      "Training loss: 0.12175355106592178\n",
      "Training loss: 0.11613301187753677\n",
      "Training loss: 0.1524999737739563\n",
      "Training loss: 0.11043014377355576\n",
      "Training loss: 0.16933029890060425\n",
      "Training loss: 0.15359622240066528\n",
      "Training loss: 0.16615965962409973\n",
      "Training loss: 0.14294147491455078\n",
      "Training loss: 0.13245905935764313\n",
      "Training loss: 0.14017562568187714\n",
      "Training loss: 0.1349487453699112\n",
      "Training loss: 0.10307832807302475\n",
      "Training loss: 0.10763581097126007\n",
      "Training loss: 0.14018066227436066\n",
      "Training loss: 0.08070912957191467\n",
      "Training loss: 0.12454002350568771\n",
      "Training loss: 0.1299375295639038\n",
      "Training loss: 0.19474273920059204\n",
      "Training loss: 0.13506002724170685\n",
      "Training loss: 0.1573362946510315\n",
      "Training loss: 0.12451618164777756\n",
      "Training loss: 0.13647638261318207\n",
      "Training loss: 0.14516820013523102\n",
      "Training loss: 0.12980663776397705\n",
      "Training loss: 0.12080980837345123\n",
      "Training loss: 0.16286689043045044\n",
      "Training loss: 0.148983895778656\n",
      "Training loss: 0.12585201859474182\n",
      "Training loss: 0.19684329628944397\n",
      "Training loss: 0.09563842415809631\n",
      "Training loss: 0.14073218405246735\n",
      "Training loss: 0.1993127465248108\n",
      "Training loss: 0.1923714429140091\n",
      "Training loss: 0.11511235684156418\n",
      "Training loss: 0.144412100315094\n",
      "Training loss: 0.1822035312652588\n",
      "Training loss: 0.12433481961488724\n",
      "Training loss: 0.09880803525447845\n",
      "Training loss: 0.15092135965824127\n",
      "Training loss: 0.12834760546684265\n",
      "Training loss: 0.1794123351573944\n",
      "Training loss: 0.12272971123456955\n",
      "Training loss: 0.2056380957365036\n",
      "Training loss: 0.13394640386104584\n",
      "Training loss: 0.1561194658279419\n",
      "Training loss: 0.10339877009391785\n",
      "Training loss: 0.14211489260196686\n",
      "Training loss: 0.16174301505088806\n",
      "Training loss: 0.16012616455554962\n",
      "Training loss: 0.1601799726486206\n",
      "Training loss: 0.18935732543468475\n",
      "Training loss: 0.14498110115528107\n",
      "Training loss: 0.10923737287521362\n",
      "Training loss: 0.146368145942688\n",
      "Training loss: 0.18732978403568268\n",
      "Training loss: 0.12095557898283005\n",
      "Training loss: 0.11523670703172684\n",
      "Training loss: 0.15179617702960968\n",
      "Training loss: 0.10955779999494553\n",
      "Training loss: 0.1690862625837326\n",
      "Training loss: 0.15272726118564606\n",
      "Training loss: 0.16546514630317688\n",
      "Training loss: 0.14189337193965912\n",
      "Training loss: 0.1315588802099228\n",
      "Training loss: 0.1394447535276413\n",
      "Training loss: 0.13426360487937927\n",
      "Training loss: 0.10186658799648285\n",
      "Training loss: 0.10629822313785553\n",
      "Training loss: 0.13942934572696686\n",
      "Training loss: 0.07947105169296265\n",
      "Training loss: 0.12387286126613617\n",
      "Training loss: 0.1292460858821869\n",
      "Training loss: 0.194195955991745\n",
      "Training loss: 0.13437221944332123\n",
      "Training loss: 0.15667816996574402\n",
      "Training loss: 0.1237565129995346\n",
      "Training loss: 0.13554222881793976\n",
      "Training loss: 0.14432433247566223\n",
      "Training loss: 0.12898072600364685\n",
      "Training loss: 0.11996156722307205\n",
      "Training loss: 0.1622706949710846\n",
      "Training loss: 0.14824871718883514\n",
      "Training loss: 0.12527990341186523\n",
      "Training loss: 0.19646821916103363\n",
      "Training loss: 0.09458599984645844\n",
      "Training loss: 0.14014124870300293\n",
      "Training loss: 0.19899679720401764\n",
      "Training loss: 0.19185364246368408\n",
      "Training loss: 0.11412690579891205\n",
      "Training loss: 0.14382712543010712\n",
      "Training loss: 0.1817399263381958\n",
      "Training loss: 0.1235162764787674\n",
      "Training loss: 0.09787030518054962\n",
      "Training loss: 0.14993548393249512\n",
      "Training loss: 0.1275956779718399\n",
      "Training loss: 0.17894528806209564\n",
      "Training loss: 0.1215791255235672\n",
      "Training loss: 0.20553013682365417\n",
      "Training loss: 0.13318488001823425\n",
      "Training loss: 0.15544044971466064\n",
      "Training loss: 0.10267230868339539\n",
      "Training loss: 0.14140592515468597\n",
      "Training loss: 0.16081520915031433\n",
      "Training loss: 0.15992672741413116\n",
      "Training loss: 0.1599932760000229\n",
      "Training loss: 0.18902921676635742\n",
      "Training loss: 0.1443144828081131\n",
      "Training loss: 0.10856783390045166\n",
      "Training loss: 0.14597317576408386\n",
      "Training loss: 0.1870419830083847\n",
      "Training loss: 0.12020844966173172\n",
      "Training loss: 0.11439377814531326\n",
      "Training loss: 0.1511356681585312\n",
      "Training loss: 0.10873649269342422\n",
      "Training loss: 0.16887973248958588\n",
      "Training loss: 0.1519097536802292\n",
      "Training loss: 0.16482022404670715\n",
      "Training loss: 0.14090240001678467\n",
      "Training loss: 0.13070984184741974\n",
      "Training loss: 0.13875916600227356\n",
      "Training loss: 0.13362324237823486\n",
      "Training loss: 0.1007125973701477\n",
      "Training loss: 0.10502374917268753\n",
      "Training loss: 0.13872943818569183\n",
      "Training loss: 0.07829350978136063\n",
      "Training loss: 0.1232496127486229\n",
      "Training loss: 0.12859892845153809\n",
      "Training loss: 0.19369244575500488\n",
      "Training loss: 0.13372577726840973\n",
      "Training loss: 0.1560673862695694\n",
      "Training loss: 0.12304297834634781\n",
      "Training loss: 0.13465769588947296\n",
      "Training loss: 0.143527552485466\n",
      "Training loss: 0.12820331752300262\n",
      "Training loss: 0.11916331946849823\n",
      "Training loss: 0.16171672940254211\n",
      "Training loss: 0.14756163954734802\n",
      "Training loss: 0.12474727630615234\n",
      "Training loss: 0.19613081216812134\n",
      "Training loss: 0.09358521550893784\n",
      "Training loss: 0.1395917683839798\n",
      "Training loss: 0.19871501624584198\n",
      "Training loss: 0.19137327373027802\n",
      "Training loss: 0.11319294571876526\n",
      "Training loss: 0.14328263700008392\n",
      "Training loss: 0.18131819367408752\n",
      "Training loss: 0.12274228036403656\n",
      "Training loss: 0.0969834253191948\n",
      "Training loss: 0.14900369942188263\n",
      "Training loss: 0.12688827514648438\n",
      "Training loss: 0.1785164177417755\n",
      "Training loss: 0.12048652023077011\n",
      "Training loss: 0.20545081794261932\n",
      "Training loss: 0.13247036933898926\n",
      "Training loss: 0.15480384230613708\n",
      "Training loss: 0.10199162364006042\n",
      "Training loss: 0.14073818922042847\n",
      "Training loss: 0.1599387675523758\n",
      "Training loss: 0.15975701808929443\n",
      "Training loss: 0.1598401665687561\n",
      "Training loss: 0.18873804807662964\n",
      "Training loss: 0.14369244873523712\n",
      "Training loss: 0.10794209688901901\n",
      "Training loss: 0.14561447501182556\n",
      "Training loss: 0.18678943812847137\n",
      "Training loss: 0.11950842291116714\n",
      "Training loss: 0.11360049247741699\n",
      "Training loss: 0.15051515400409698\n",
      "Training loss: 0.10796262323856354\n",
      "Training loss: 0.16870738565921783\n",
      "Training loss: 0.15114004909992218\n",
      "Training loss: 0.16422133147716522\n",
      "Training loss: 0.13996468484401703\n",
      "Training loss: 0.12990838289260864\n",
      "Training loss: 0.13811573386192322\n",
      "Training loss: 0.13302434980869293\n",
      "Training loss: 0.09961257129907608\n",
      "Training loss: 0.10380828380584717\n",
      "Training loss: 0.13807716965675354\n",
      "Training loss: 0.07717260718345642\n",
      "Training loss: 0.12266679853200912\n",
      "Training loss: 0.1279926747083664\n",
      "Training loss: 0.1932290643453598\n",
      "Training loss: 0.13311761617660522\n",
      "Training loss: 0.15550030767917633\n",
      "Training loss: 0.12237225472927094\n",
      "Training loss: 0.13381946086883545\n",
      "Training loss: 0.1427745670080185\n",
      "Training loss: 0.12747091054916382\n",
      "Training loss: 0.11841155588626862\n",
      "Training loss: 0.16120178997516632\n",
      "Training loss: 0.14691929519176483\n",
      "Training loss: 0.12425117194652557\n",
      "Training loss: 0.19582796096801758\n",
      "Training loss: 0.09263268858194351\n",
      "Training loss: 0.13908056914806366\n",
      "Training loss: 0.19846458733081818\n",
      "Training loss: 0.19092752039432526\n",
      "Training loss: 0.11230702698230743\n",
      "Training loss: 0.14277559518814087\n",
      "Training loss: 0.1809350699186325\n",
      "Training loss: 0.12200981378555298\n",
      "Training loss: 0.09614395350217819\n",
      "Training loss: 0.14812232553958893\n",
      "Training loss: 0.12622219324111938\n",
      "Training loss: 0.17812278866767883\n",
      "Training loss: 0.11944809556007385\n",
      "Training loss: 0.20539745688438416\n",
      "Training loss: 0.13179947435855865\n",
      "Training loss: 0.15420660376548767\n",
      "Training loss: 0.10135336220264435\n",
      "Training loss: 0.14010868966579437\n",
      "Training loss: 0.15911024808883667\n",
      "Training loss: 0.15961439907550812\n",
      "Training loss: 0.15971767902374268\n",
      "Training loss: 0.18848073482513428\n",
      "Training loss: 0.14311164617538452\n",
      "Training loss: 0.10735689103603363\n",
      "Training loss: 0.1452890932559967\n",
      "Training loss: 0.18656915426254272\n",
      "Training loss: 0.11885218322277069\n",
      "Training loss: 0.11285338550806046\n",
      "Training loss: 0.14993181824684143\n",
      "Training loss: 0.10723289847373962\n",
      "Training loss: 0.16856616735458374\n",
      "Training loss: 0.15041479468345642\n",
      "Training loss: 0.16366499662399292\n",
      "Training loss: 0.13907670974731445\n",
      "Training loss: 0.12915123999118805\n",
      "Training loss: 0.1375114619731903\n",
      "Training loss: 0.13246388733386993\n",
      "Training loss: 0.09856311231851578\n",
      "Training loss: 0.10264819860458374\n",
      "Training loss: 0.13746905326843262\n",
      "Training loss: 0.07610471546649933\n",
      "Training loss: 0.12212152034044266\n",
      "Training loss: 0.12742438912391663\n",
      "Training loss: 0.19280269742012024\n",
      "Training loss: 0.13254499435424805\n",
      "Training loss: 0.15497377514839172\n",
      "Training loss: 0.12174129486083984\n",
      "Training loss: 0.1330244243144989\n",
      "Training loss: 0.14206242561340332\n",
      "Training loss: 0.1267804056406021\n",
      "Training loss: 0.11770304292440414\n",
      "Training loss: 0.1607230007648468\n",
      "Training loss: 0.14631842076778412\n",
      "Training loss: 0.12378878146409988\n",
      "Training loss: 0.19555681943893433\n",
      "Training loss: 0.09172528982162476\n",
      "Training loss: 0.13860481977462769\n",
      "Training loss: 0.1982429325580597\n",
      "Training loss: 0.19051381945610046\n",
      "Training loss: 0.11146591603755951\n",
      "Training loss: 0.1423032581806183\n",
      "Training loss: 0.1805875152349472\n",
      "Training loss: 0.1213160902261734\n",
      "Training loss: 0.0953487679362297\n",
      "Training loss: 0.14728796482086182\n",
      "Training loss: 0.12559452652931213\n",
      "Training loss: 0.17776156961917877\n",
      "Training loss: 0.11846043169498444\n",
      "Training loss: 0.20536766946315765\n",
      "Training loss: 0.13116905093193054\n",
      "Training loss: 0.15364597737789154\n",
      "Training loss: 0.10075443238019943\n",
      "Training loss: 0.13951480388641357\n",
      "Training loss: 0.15832650661468506\n",
      "Training loss: 0.15949644148349762\n",
      "Training loss: 0.15962307155132294\n",
      "Training loss: 0.1882544755935669\n",
      "Training loss: 0.1425691545009613\n",
      "Training loss: 0.10680930316448212\n",
      "Training loss: 0.1449943631887436\n",
      "Training loss: 0.1863783895969391\n",
      "Training loss: 0.11823663860559464\n",
      "Training loss: 0.11214926838874817\n",
      "Training loss: 0.14938297867774963\n",
      "Training loss: 0.10654425621032715\n",
      "Training loss: 0.1684533953666687\n",
      "Training loss: 0.14973092079162598\n",
      "Training loss: 0.16314814984798431\n",
      "Training loss: 0.13823527097702026\n",
      "Training loss: 0.1284354031085968\n",
      "Training loss: 0.13694362342357635\n",
      "Training loss: 0.13193905353546143\n",
      "Training loss: 0.0975610762834549\n",
      "Training loss: 0.10154011845588684\n",
      "Training loss: 0.13690190017223358\n",
      "Training loss: 0.0750865489244461\n",
      "Training loss: 0.12161094695329666\n",
      "Training loss: 0.12689127027988434\n",
      "Training loss: 0.1924106329679489\n",
      "Training loss: 0.13200542330741882\n",
      "Training loss: 0.1544848084449768\n",
      "Training loss: 0.12114734947681427\n",
      "Training loss: 0.13226979970932007\n",
      "Training loss: 0.141388401389122\n",
      "Training loss: 0.1261288970708847\n",
      "Training loss: 0.11703485250473022\n",
      "Training loss: 0.16027776896953583\n",
      "Training loss: 0.14575621485710144\n",
      "Training loss: 0.12335764616727829\n",
      "Training loss: 0.19531500339508057\n",
      "Training loss: 0.09086019545793533\n",
      "Training loss: 0.13816194236278534\n",
      "Training loss: 0.1980476826429367\n",
      "Training loss: 0.19012978672981262\n",
      "Training loss: 0.11066679656505585\n",
      "Training loss: 0.14186304807662964\n",
      "Training loss: 0.18027275800704956\n",
      "Training loss: 0.1206584945321083\n",
      "Training loss: 0.09459491074085236\n",
      "Training loss: 0.14649759232997894\n",
      "Training loss: 0.1250026375055313\n",
      "Training loss: 0.1774303913116455\n",
      "Training loss: 0.11752033233642578\n",
      "Training loss: 0.20535923540592194\n",
      "Training loss: 0.13057629764080048\n",
      "Training loss: 0.153119295835495\n",
      "Training loss: 0.10019208490848541\n",
      "Training loss: 0.1389540731906891\n",
      "Training loss: 0.15758460760116577\n",
      "Training loss: 0.15940098464488983\n",
      "Training loss: 0.1595539152622223\n",
      "Training loss: 0.18805678188800812\n",
      "Training loss: 0.1420622318983078\n",
      "Training loss: 0.10629663616418839\n",
      "Training loss: 0.14472785592079163\n",
      "Training loss: 0.18621477484703064\n",
      "Training loss: 0.11765903234481812\n",
      "Training loss: 0.1114853248000145\n",
      "Training loss: 0.14886623620986938\n",
      "Training loss: 0.10589396208524704\n",
      "Training loss: 0.168366476893425\n",
      "Training loss: 0.14908568561077118\n",
      "Training loss: 0.16266795992851257\n",
      "Training loss: 0.1374373733997345\n",
      "Training loss: 0.12775813043117523\n",
      "Training loss: 0.13640983402729034\n",
      "Training loss: 0.13144733011722565\n",
      "Training loss: 0.09660360217094421\n",
      "Training loss: 0.10048089176416397\n",
      "Training loss: 0.13637283444404602\n",
      "Training loss: 0.0741150826215744\n",
      "Training loss: 0.12113247066736221\n",
      "Training loss: 0.12639079988002777\n",
      "Training loss: 0.19205038249492645\n",
      "Training loss: 0.13149654865264893\n",
      "Training loss: 0.1540306657552719\n",
      "Training loss: 0.12058787047863007\n",
      "Training loss: 0.1315530240535736\n",
      "Training loss: 0.14075005054473877\n",
      "Training loss: 0.12551380693912506\n",
      "Training loss: 0.11640434712171555\n",
      "Training loss: 0.1598636507987976\n",
      "Training loss: 0.14523008465766907\n",
      "Training loss: 0.12295552343130112\n",
      "Training loss: 0.19509993493556976\n",
      "Training loss: 0.09003489464521408\n",
      "Training loss: 0.13774967193603516\n",
      "Training loss: 0.1978767216205597\n",
      "Training loss: 0.18977324664592743\n",
      "Training loss: 0.10990702360868454\n",
      "Training loss: 0.1414525806903839\n",
      "Training loss: 0.17998835444450378\n",
      "Training loss: 0.12003471702337265\n",
      "Training loss: 0.09387975186109543\n",
      "Training loss: 0.14574846625328064\n",
      "Training loss: 0.12444408982992172\n",
      "Training loss: 0.17712701857089996\n",
      "Training loss: 0.11662481725215912\n",
      "Training loss: 0.20537014305591583\n",
      "Training loss: 0.13001865148544312\n",
      "Training loss: 0.1526242196559906\n",
      "Training loss: 0.09966380149126053\n",
      "Training loss: 0.13842426240444183\n",
      "Training loss: 0.1568819284439087\n",
      "Training loss: 0.15932601690292358\n",
      "Training loss: 0.1595079004764557\n",
      "Training loss: 0.18788529932498932\n",
      "Training loss: 0.14158838987350464\n",
      "Training loss: 0.10581647604703903\n",
      "Training loss: 0.14448732137680054\n",
      "Training loss: 0.18607601523399353\n",
      "Training loss: 0.11711682379245758\n",
      "Training loss: 0.11085892468690872\n",
      "Training loss: 0.14837929606437683\n",
      "Training loss: 0.10527949035167694\n",
      "Training loss: 0.1683032065629959\n",
      "Training loss: 0.1484764963388443\n",
      "Training loss: 0.16222183406352997\n",
      "Training loss: 0.13668033480644226\n",
      "Training loss: 0.12711691856384277\n",
      "Training loss: 0.13590778410434723\n",
      "Training loss: 0.13098640739917755\n",
      "Training loss: 0.09568807482719421\n",
      "Training loss: 0.09946775436401367\n",
      "Training loss: 0.13587912917137146\n",
      "Training loss: 0.07318755984306335\n",
      "Training loss: 0.12068396061658859\n",
      "Training loss: 0.12592071294784546\n",
      "Training loss: 0.19171957671642303\n",
      "Training loss: 0.1310163289308548\n",
      "Training loss: 0.15360890328884125\n",
      "Training loss: 0.12006055563688278\n",
      "Training loss: 0.1308717131614685\n",
      "Training loss: 0.14014507830142975\n",
      "Training loss: 0.12493269145488739\n",
      "Training loss: 0.11580901592969894\n",
      "Training loss: 0.15947839617729187\n",
      "Training loss: 0.14473754167556763\n",
      "Training loss: 0.12258029729127884\n",
      "Training loss: 0.19490967690944672\n",
      "Training loss: 0.0892469584941864\n",
      "Training loss: 0.13736572861671448\n",
      "Training loss: 0.19772803783416748\n",
      "Training loss: 0.18944214284420013\n",
      "Training loss: 0.1091841384768486\n",
      "Training loss: 0.14106984436511993\n",
      "Training loss: 0.17973193526268005\n",
      "Training loss: 0.11944257467985153\n",
      "Training loss: 0.09320086985826492\n",
      "Training loss: 0.1450379490852356\n",
      "Training loss: 0.12391670048236847\n",
      "Training loss: 0.1768493354320526\n",
      "Training loss: 0.11577130109071732\n",
      "Training loss: 0.2053985744714737\n",
      "Training loss: 0.12949372828006744\n",
      "Training loss: 0.15215864777565002\n",
      "Training loss: 0.09916719794273376\n",
      "Training loss: 0.13792334496974945\n",
      "Training loss: 0.15621602535247803\n",
      "Training loss: 0.159269779920578\n",
      "Training loss: 0.15948298573493958\n",
      "Training loss: 0.18773791193962097\n",
      "Training loss: 0.14114533364772797\n",
      "Training loss: 0.105366550385952\n",
      "Training loss: 0.14427074790000916\n",
      "Training loss: 0.185960054397583\n",
      "Training loss: 0.11660757660865784\n",
      "Training loss: 0.11026757955551147\n",
      "Training loss: 0.14792022109031677\n",
      "Training loss: 0.10469852387905121\n",
      "Training loss: 0.16826149821281433\n",
      "Training loss: 0.14790107309818268\n",
      "Training loss: 0.1618073731660843\n",
      "Training loss: 0.1359616369009018\n",
      "Training loss: 0.12650948762893677\n",
      "Training loss: 0.13543541729450226\n",
      "Training loss: 0.1305541694164276\n",
      "Training loss: 0.09481208771467209\n",
      "Training loss: 0.09849806874990463\n",
      "Training loss: 0.13541841506958008\n",
      "Training loss: 0.0723014622926712\n",
      "Training loss: 0.12026321142911911\n",
      "Training loss: 0.12547887861728668\n",
      "Training loss: 0.19141614437103271\n",
      "Training loss: 0.13056284189224243\n",
      "Training loss: 0.15321721136569977\n",
      "Training loss: 0.1195632815361023\n",
      "Training loss: 0.13022375106811523\n",
      "Training loss: 0.13957148790359497\n",
      "Training loss: 0.12438338249921799\n",
      "Training loss: 0.11524660140275955\n",
      "Training loss: 0.1591200977563858\n",
      "Training loss: 0.14427632093429565\n",
      "Training loss: 0.12223006784915924\n",
      "Training loss: 0.1947421431541443\n",
      "Training loss: 0.0884942039847374\n",
      "Training loss: 0.13700824975967407\n",
      "Training loss: 0.19759991765022278\n",
      "Training loss: 0.18913476169109344\n",
      "Training loss: 0.10849589854478836\n",
      "Training loss: 0.14071282744407654\n",
      "Training loss: 0.17950136959552765\n",
      "Training loss: 0.11888013780117035\n",
      "Training loss: 0.09255599975585938\n",
      "Training loss: 0.14436368644237518\n",
      "Training loss: 0.123418428003788\n",
      "Training loss: 0.17659547924995422\n",
      "Training loss: 0.1149572879076004\n",
      "Training loss: 0.20544278621673584\n",
      "Training loss: 0.12899938225746155\n",
      "Training loss: 0.15172062814235687\n",
      "Training loss: 0.0987001582980156\n",
      "Training loss: 0.1374494582414627\n",
      "Training loss: 0.1555846631526947\n",
      "Training loss: 0.1592305302619934\n",
      "Training loss: 0.15947723388671875\n",
      "Training loss: 0.1876126527786255\n",
      "Training loss: 0.14073097705841064\n",
      "Training loss: 0.10494480282068253\n",
      "Training loss: 0.14407622814178467\n",
      "Training loss: 0.18586497008800507\n",
      "Training loss: 0.11612910032272339\n",
      "Training loss: 0.10970904678106308\n",
      "Training loss: 0.14748713374137878\n",
      "Training loss: 0.10414891690015793\n",
      "Training loss: 0.16823947429656982\n",
      "Training loss: 0.14735712110996246\n",
      "Training loss: 0.16142238676548004\n",
      "Training loss: 0.13527901470661163\n",
      "Training loss: 0.12593373656272888\n",
      "Training loss: 0.13499081134796143\n",
      "Training loss: 0.13014863431453705\n",
      "Training loss: 0.09397339820861816\n",
      "Training loss: 0.09756938368082047\n",
      "Training loss: 0.13498838245868683\n",
      "Training loss: 0.07145439833402634\n",
      "Training loss: 0.11986829340457916\n",
      "Training loss: 0.12506338953971863\n",
      "Training loss: 0.1911381483078003\n",
      "Training loss: 0.13013428449630737\n",
      "Training loss: 0.15285339951515198\n",
      "Training loss: 0.11909407377243042\n",
      "Training loss: 0.12960709631443024\n",
      "Training loss: 0.13902723789215088\n",
      "Training loss: 0.1238638311624527\n",
      "Training loss: 0.11471503227949142\n",
      "Training loss: 0.15878677368164062\n",
      "Training loss: 0.14384450018405914\n",
      "Training loss: 0.12190313637256622\n",
      "Training loss: 0.19459562003612518\n",
      "Training loss: 0.08777466416358948\n",
      "Training loss: 0.1366753727197647\n",
      "Training loss: 0.19749054312705994\n",
      "Training loss: 0.18884915113449097\n",
      "Training loss: 0.10784037411212921\n",
      "Training loss: 0.1403796374797821\n",
      "Training loss: 0.17929472029209137\n",
      "Training loss: 0.1183454617857933\n",
      "Training loss: 0.09194300323724747\n",
      "Training loss: 0.1437235325574875\n",
      "Training loss: 0.1229473352432251\n",
      "Training loss: 0.17636370658874512\n",
      "Training loss: 0.11418034881353378\n",
      "Training loss: 0.2055012434720993\n",
      "Training loss: 0.12853355705738068\n",
      "Training loss: 0.15130819380283356\n",
      "Training loss: 0.09826073795557022\n",
      "Training loss: 0.13700078427791595\n",
      "Training loss: 0.1549856960773468\n",
      "Training loss: 0.159206822514534\n",
      "Training loss: 0.1594889760017395\n",
      "Training loss: 0.187507763504982\n",
      "Training loss: 0.1403432935476303\n",
      "Training loss: 0.1045493558049202\n",
      "Training loss: 0.14390209317207336\n",
      "Training loss: 0.18578916788101196\n",
      "Training loss: 0.11567963659763336\n",
      "Training loss: 0.10918138921260834\n",
      "Training loss: 0.14707814157009125\n",
      "Training loss: 0.10362868010997772\n",
      "Training loss: 0.1682353913784027\n",
      "Training loss: 0.14684276282787323\n",
      "Training loss: 0.16106480360031128\n",
      "Training loss: 0.1346302330493927\n",
      "Training loss: 0.12538757920265198\n",
      "Training loss: 0.13457231223583221\n",
      "Training loss: 0.1297680288553238\n",
      "Training loss: 0.09316997230052948\n",
      "Training loss: 0.09667952358722687\n",
      "Training loss: 0.13458698987960815\n",
      "Training loss: 0.07064425945281982\n",
      "Training loss: 0.11949741840362549\n",
      "Training loss: 0.12467245757579803\n",
      "Training loss: 0.1908837854862213\n",
      "Training loss: 0.12972904741764069\n",
      "Training loss: 0.15251560509204865\n",
      "Training loss: 0.1186511218547821\n",
      "Training loss: 0.12901991605758667\n",
      "Training loss: 0.13851061463356018\n",
      "Training loss: 0.1233721673488617\n",
      "Training loss: 0.11421239376068115\n",
      "Training loss: 0.15847674012184143\n",
      "Training loss: 0.14344006776809692\n",
      "Training loss: 0.12159781157970428\n",
      "Training loss: 0.1944683939218521\n",
      "Training loss: 0.08708646148443222\n",
      "Training loss: 0.13636541366577148\n",
      "Training loss: 0.19739855825901031\n",
      "Training loss: 0.1885841190814972\n",
      "Training loss: 0.1072155237197876\n",
      "Training loss: 0.14006876945495605\n",
      "Training loss: 0.17911015450954437\n",
      "Training loss: 0.11783703416585922\n",
      "Training loss: 0.09136009216308594\n",
      "Training loss: 0.14311538636684418\n",
      "Training loss: 0.12250180542469025\n",
      "Training loss: 0.1761523336172104\n",
      "Training loss: 0.11343857645988464\n",
      "Training loss: 0.20557266473770142\n",
      "Training loss: 0.1280944049358368\n",
      "Training loss: 0.15091978013515472\n",
      "Training loss: 0.09784712642431259\n",
      "Training loss: 0.1365758627653122\n",
      "Training loss: 0.15441718697547913\n",
      "Training loss: 0.15919724106788635\n",
      "Training loss: 0.1595165878534317\n",
      "Training loss: 0.18742148578166962\n",
      "Training loss: 0.13998062908649445\n",
      "Training loss: 0.10417837649583817\n",
      "Training loss: 0.1437467336654663\n",
      "Training loss: 0.18573084473609924\n",
      "Training loss: 0.11525699496269226\n",
      "Training loss: 0.1086825430393219\n",
      "Training loss: 0.14669187366962433\n",
      "Training loss: 0.10313601791858673\n",
      "Training loss: 0.1682475209236145\n",
      "Training loss: 0.14635609090328217\n",
      "Training loss: 0.16073282063007355\n",
      "Training loss: 0.13401338458061218\n",
      "Training loss: 0.12486928701400757\n",
      "Training loss: 0.13417816162109375\n",
      "Training loss: 0.12941066920757294\n",
      "Training loss: 0.09239988774061203\n",
      "Training loss: 0.09582633525133133\n",
      "Training loss: 0.13421228528022766\n",
      "Training loss: 0.06986899673938751\n",
      "Training loss: 0.1191488653421402\n",
      "Training loss: 0.1243043765425682\n",
      "Training loss: 0.19065144658088684\n",
      "Training loss: 0.1293456107378006\n",
      "Training loss: 0.15220195055007935\n",
      "Training loss: 0.11823278665542603\n",
      "Training loss: 0.12846049666404724\n",
      "Training loss: 0.13801993429660797\n",
      "Training loss: 0.12290669232606888\n",
      "Training loss: 0.11373688280582428\n",
      "Training loss: 0.1581883430480957\n",
      "Training loss: 0.14306139945983887\n",
      "Training loss: 0.1213126927614212\n",
      "Training loss: 0.19435881078243256\n",
      "Training loss: 0.08642791956663132\n",
      "Training loss: 0.136076882481575\n",
      "Training loss: 0.19732242822647095\n",
      "Training loss: 0.18833789229393005\n",
      "Training loss: 0.1066197082400322\n",
      "Training loss: 0.13977858424186707\n",
      "Training loss: 0.17894607782363892\n",
      "Training loss: 0.117353156208992\n",
      "Training loss: 0.09080535918474197\n",
      "Training loss: 0.14253748953342438\n",
      "Training loss: 0.12208014726638794\n",
      "Training loss: 0.1759600043296814\n",
      "Training loss: 0.11272981762886047\n",
      "Training loss: 0.2056555151939392\n",
      "Training loss: 0.12768028676509857\n",
      "Training loss: 0.15055382251739502\n",
      "Training loss: 0.0974576398730278\n",
      "Training loss: 0.1361730843782425\n",
      "Training loss: 0.153877392411232\n",
      "Training loss: 0.15920047461986542\n",
      "Training loss: 0.15955854952335358\n",
      "Training loss: 0.18735243380069733\n",
      "Training loss: 0.13964124023914337\n",
      "Training loss: 0.10383034497499466\n",
      "Training loss: 0.14360874891281128\n",
      "Training loss: 0.18568867444992065\n",
      "Training loss: 0.11485972255468369\n",
      "Training loss: 0.10821083933115005\n",
      "Training loss: 0.14632679522037506\n",
      "Training loss: 0.10266923904418945\n",
      "Training loss: 0.16827446222305298\n",
      "Training loss: 0.14589542150497437\n",
      "Training loss: 0.16042464971542358\n",
      "Training loss: 0.13342660665512085\n",
      "Training loss: 0.12437720596790314\n",
      "Training loss: 0.13380682468414307\n",
      "Training loss: 0.1290750503540039\n",
      "Training loss: 0.09166143834590912\n",
      "Training loss: 0.09500803798437119\n",
      "Training loss: 0.1338624805212021\n",
      "Training loss: 0.06912669539451599\n",
      "Training loss: 0.11882127076387405\n",
      "Training loss: 0.12395778298377991\n",
      "Training loss: 0.19043944776058197\n",
      "Training loss: 0.12898267805576324\n",
      "Training loss: 0.1519109010696411\n",
      "Training loss: 0.11783754080533981\n",
      "Training loss: 0.12792730331420898\n",
      "Training loss: 0.13755370676517487\n",
      "Training loss: 0.12246578931808472\n",
      "Training loss: 0.11328688263893127\n",
      "Training loss: 0.1579202264547348\n",
      "Training loss: 0.14270669221878052\n",
      "Training loss: 0.12104634940624237\n",
      "Training loss: 0.194265678524971\n",
      "Training loss: 0.08579737693071365\n",
      "Training loss: 0.13580816984176636\n",
      "Training loss: 0.19726090133190155\n",
      "Training loss: 0.18810950219631195\n",
      "Training loss: 0.10605113208293915\n",
      "Training loss: 0.1395077407360077\n",
      "Training loss: 0.17880086600780487\n",
      "Training loss: 0.11689251661300659\n",
      "Training loss: 0.0902772843837738\n",
      "Training loss: 0.14198796451091766\n",
      "Training loss: 0.12168095260858536\n",
      "Training loss: 0.17578524351119995\n",
      "Training loss: 0.11205242574214935\n",
      "Training loss: 0.20574885606765747\n",
      "Training loss: 0.1272895634174347\n",
      "Training loss: 0.15020886063575745\n",
      "Training loss: 0.09709075838327408\n",
      "Training loss: 0.13579121232032776\n",
      "Training loss: 0.1533646434545517\n",
      "Training loss: 0.15921537578105927\n",
      "Training loss: 0.15961354970932007\n",
      "Training loss: 0.18729910254478455\n",
      "Training loss: 0.1393236219882965\n",
      "Training loss: 0.1035037413239479\n",
      "Training loss: 0.14348675310611725\n",
      "Training loss: 0.1856612265110016\n",
      "Training loss: 0.1144862025976181\n",
      "Training loss: 0.10776463896036148\n",
      "Training loss: 0.1459815353155136\n",
      "Training loss: 0.10222682356834412\n",
      "Training loss: 0.16831490397453308\n",
      "Training loss: 0.14545919001102448\n",
      "Training loss: 0.16013866662979126\n",
      "Training loss: 0.132868230342865\n",
      "Training loss: 0.12390972673892975\n",
      "Training loss: 0.1334570348262787\n",
      "Training loss: 0.12875977158546448\n",
      "Training loss: 0.09095294028520584\n",
      "Training loss: 0.09422274678945541\n",
      "Training loss: 0.13353592157363892\n",
      "Training loss: 0.06841568648815155\n",
      "Training loss: 0.11851316690444946\n",
      "Training loss: 0.12363117933273315\n",
      "Training loss: 0.19024649262428284\n",
      "Training loss: 0.12863890826702118\n",
      "Training loss: 0.15164075791835785\n",
      "Training loss: 0.11746391654014587\n",
      "Training loss: 0.12741884589195251\n",
      "Training loss: 0.13711054623126984\n",
      "Training loss: 0.12204799056053162\n",
      "Training loss: 0.11286088824272156\n",
      "Training loss: 0.15767095983028412\n",
      "Training loss: 0.14237463474273682\n",
      "Training loss: 0.12079758942127228\n",
      "Training loss: 0.19418753683567047\n",
      "Training loss: 0.08519349247217178\n",
      "Training loss: 0.13555823266506195\n",
      "Training loss: 0.19721278548240662\n",
      "Training loss: 0.1878974437713623\n",
      "Training loss: 0.10550850629806519\n",
      "Training loss: 0.13925495743751526\n",
      "Training loss: 0.1786731481552124\n",
      "Training loss: 0.11645373702049255\n",
      "Training loss: 0.08977434039115906\n",
      "Training loss: 0.14146529138088226\n",
      "Training loss: 0.12130285054445267\n",
      "Training loss: 0.17562679946422577\n",
      "Training loss: 0.11140468716621399\n",
      "Training loss: 0.2058514505624771\n",
      "Training loss: 0.12692078948020935\n",
      "Training loss: 0.14988362789154053\n",
      "Training loss: 0.09674499928951263\n",
      "Training loss: 0.13542893528938293\n",
      "Training loss: 0.15287740528583527\n",
      "Training loss: 0.15924084186553955\n",
      "Training loss: 0.1596803367137909\n",
      "Training loss: 0.187260240316391\n",
      "Training loss: 0.13902635872364044\n",
      "Training loss: 0.10319716483354568\n",
      "Training loss: 0.14337953925132751\n",
      "Training loss: 0.1856473982334137\n",
      "Training loss: 0.11413495242595673\n",
      "Training loss: 0.10734245926141739\n",
      "Training loss: 0.14565478265285492\n",
      "Training loss: 0.10180731117725372\n",
      "Training loss: 0.16836762428283691\n",
      "Training loss: 0.1450459212064743\n",
      "Training loss: 0.15987348556518555\n",
      "Training loss: 0.13233669102191925\n",
      "Training loss: 0.12346545606851578\n",
      "Training loss: 0.13312757015228271\n",
      "Training loss: 0.12846358120441437\n",
      "Training loss: 0.09027300029993057\n",
      "Training loss: 0.09346893429756165\n",
      "Training loss: 0.13323114812374115\n",
      "Training loss: 0.0677345022559166\n",
      "Training loss: 0.11822328716516495\n",
      "Training loss: 0.12332332879304886\n",
      "Training loss: 0.19007118046283722\n",
      "Training loss: 0.12831315398216248\n",
      "Training loss: 0.15139025449752808\n",
      "Training loss: 0.11711065471172333\n",
      "Training loss: 0.12693384289741516\n",
      "Training loss: 0.13668914139270782\n",
      "Training loss: 0.12165194749832153\n",
      "Training loss: 0.11245744675397873\n",
      "Training loss: 0.15743935108184814\n",
      "Training loss: 0.14206361770629883\n",
      "Training loss: 0.12056522816419601\n",
      "Training loss: 0.19412319362163544\n",
      "Training loss: 0.08461476117372513\n",
      "Training loss: 0.13532570004463196\n",
      "Training loss: 0.197176992893219\n",
      "Training loss: 0.1877005398273468\n",
      "Training loss: 0.1049903929233551\n",
      "Training loss: 0.13901886343955994\n",
      "Training loss: 0.17856158316135406\n",
      "Training loss: 0.11603549122810364\n",
      "Training loss: 0.08929497003555298\n",
      "Training loss: 0.14096811413764954\n",
      "Training loss: 0.1209445521235466\n",
      "Training loss: 0.17548364400863647\n",
      "Training loss: 0.11078481376171112\n",
      "Training loss: 0.20596212148666382\n",
      "Training loss: 0.1265726238489151\n",
      "Training loss: 0.1495768129825592\n",
      "Training loss: 0.09641911834478378\n",
      "Training loss: 0.13508503139019012\n",
      "Training loss: 0.1524142473936081\n",
      "Training loss: 0.15927579998970032\n",
      "Training loss: 0.15975773334503174\n",
      "Training loss: 0.18723464012145996\n",
      "Training loss: 0.13874807953834534\n",
      "Training loss: 0.10290934890508652\n",
      "Training loss: 0.14328590035438538\n",
      "Training loss: 0.18564583361148834\n",
      "Training loss: 0.11380457133054733\n",
      "Training loss: 0.1069427952170372\n",
      "Training loss: 0.14534538984298706\n",
      "Training loss: 0.10140928626060486\n",
      "Training loss: 0.16843140125274658\n",
      "Training loss: 0.14465421438217163\n",
      "Training loss: 0.15962755680084229\n",
      "Training loss: 0.1318303942680359\n",
      "Training loss: 0.12304292619228363\n",
      "Training loss: 0.13281700015068054\n",
      "Training loss: 0.12818510830402374\n",
      "Training loss: 0.08961998671293259\n",
      "Training loss: 0.09274481236934662\n",
      "Training loss: 0.1329466700553894\n",
      "Training loss: 0.06708145141601562\n",
      "Training loss: 0.11795028299093246\n",
      "Training loss: 0.12303289771080017\n",
      "Training loss: 0.18991243839263916\n",
      "Training loss: 0.12800422310829163\n",
      "Training loss: 0.15115779638290405\n",
      "Training loss: 0.1167764812707901\n",
      "Training loss: 0.12647093832492828\n",
      "Training loss: 0.13628824055194855\n",
      "Training loss: 0.12127631902694702\n",
      "Training loss: 0.11207534372806549\n",
      "Training loss: 0.15722420811653137\n",
      "Training loss: 0.14177261292934418\n",
      "Training loss: 0.12034818530082703\n",
      "Training loss: 0.19407151639461517\n",
      "Training loss: 0.08406001329421997\n",
      "Training loss: 0.13510942459106445\n",
      "Training loss: 0.19715243577957153\n",
      "Training loss: 0.1875179558992386\n",
      "Training loss: 0.1044953390955925\n",
      "Training loss: 0.13879850506782532\n",
      "Training loss: 0.17846493422985077\n",
      "Training loss: 0.11563675850629807\n",
      "Training loss: 0.08883801102638245\n",
      "Training loss: 0.14049474895000458\n",
      "Training loss: 0.12060490995645523\n",
      "Training loss: 0.17535445094108582\n",
      "Training loss: 0.11019165813922882\n",
      "Training loss: 0.20608019828796387\n",
      "Training loss: 0.12624382972717285\n",
      "Training loss: 0.14928734302520752\n",
      "Training loss: 0.09611180424690247\n",
      "Training loss: 0.13475847244262695\n",
      "Training loss: 0.15197378396987915\n",
      "Training loss: 0.15931947529315948\n",
      "Training loss: 0.15984469652175903\n",
      "Training loss: 0.18722113966941833\n",
      "Training loss: 0.13848762214183807\n",
      "Training loss: 0.10263911634683609\n",
      "Training loss: 0.14320486783981323\n",
      "Training loss: 0.18565550446510315\n",
      "Training loss: 0.11349381506443024\n",
      "Training loss: 0.10656440258026123\n",
      "Training loss: 0.14505240321159363\n",
      "Training loss: 0.10103161633014679\n",
      "Training loss: 0.16850516200065613\n",
      "Training loss: 0.14428287744522095\n",
      "Training loss: 0.15939976274967194\n",
      "Training loss: 0.1313481330871582\n",
      "Training loss: 0.12264107167720795\n",
      "Training loss: 0.13252434134483337\n",
      "Training loss: 0.1279233992099762\n",
      "Training loss: 0.08899284899234772\n",
      "Training loss: 0.0920492634177208\n",
      "Training loss: 0.1326812207698822\n",
      "Training loss: 0.06645524501800537\n",
      "Training loss: 0.11769338697195053\n",
      "Training loss: 0.12275903671979904\n",
      "Training loss: 0.18976876139640808\n",
      "Training loss: 0.12771135568618774\n",
      "Training loss: 0.15094250440597534\n",
      "Training loss: 0.11646030843257904\n",
      "Training loss: 0.12602901458740234\n",
      "Training loss: 0.13590674102306366\n",
      "Training loss: 0.12091997265815735\n",
      "Training loss: 0.1117132306098938\n",
      "Training loss: 0.15702442824840546\n",
      "Training loss: 0.1415000557899475\n",
      "Training loss: 0.12014544010162354\n",
      "Training loss: 0.19403141736984253\n",
      "Training loss: 0.08352803438901901\n",
      "Training loss: 0.13490848243236542\n",
      "Training loss: 0.19713816046714783\n",
      "Training loss: 0.18734851479530334\n",
      "Training loss: 0.10402239859104156\n",
      "Training loss: 0.13859274983406067\n",
      "Training loss: 0.1783820539712906\n",
      "Training loss: 0.11525636166334152\n",
      "Training loss: 0.08840210735797882\n",
      "Training loss: 0.140044167637825\n",
      "Training loss: 0.12028282135725021\n",
      "Training loss: 0.1752384603023529\n",
      "Training loss: 0.10962355881929398\n",
      "Training loss: 0.2062045931816101\n",
      "Training loss: 0.12593317031860352\n",
      "Training loss: 0.14901408553123474\n",
      "Training loss: 0.0958220586180687\n",
      "Training loss: 0.13444824516773224\n",
      "Training loss: 0.15155476331710815\n",
      "Training loss: 0.15937089920043945\n",
      "Training loss: 0.15994016826152802\n",
      "Training loss: 0.18721874058246613\n",
      "Training loss: 0.1382438689470291\n",
      "Training loss: 0.10238531231880188\n",
      "Training loss: 0.1431354284286499\n",
      "Training loss: 0.1856755018234253\n",
      "Training loss: 0.11320154368877411\n",
      "Training loss: 0.106206014752388\n",
      "Training loss: 0.1447746753692627\n",
      "Training loss: 0.10067305713891983\n",
      "Training loss: 0.16858790814876556\n",
      "Training loss: 0.14393062889575958\n",
      "Training loss: 0.15918870270252228\n",
      "Training loss: 0.1308884471654892\n",
      "Training loss: 0.12225856631994247\n",
      "Training loss: 0.13224855065345764\n",
      "Training loss: 0.12767727673053741\n",
      "Training loss: 0.08839012682437897\n",
      "Training loss: 0.09138064086437225\n",
      "Training loss: 0.13243354856967926\n",
      "Training loss: 0.06585454195737839\n",
      "Training loss: 0.11745110899209976\n",
      "Training loss: 0.12250038981437683\n",
      "Training loss: 0.18963958323001862\n",
      "Training loss: 0.12743328511714935\n",
      "Training loss: 0.15074284374713898\n",
      "Training loss: 0.1161610335111618\n",
      "Training loss: 0.1256069540977478\n",
      "Training loss: 0.13554354012012482\n",
      "Training loss: 0.12058183550834656\n",
      "Training loss: 0.1113700419664383\n",
      "Training loss: 0.15683889389038086\n",
      "Training loss: 0.14124524593353271\n",
      "Training loss: 0.11995608359575272\n",
      "Training loss: 0.19400203227996826\n",
      "Training loss: 0.0830177366733551\n",
      "Training loss: 0.13472166657447815\n",
      "Training loss: 0.19713333249092102\n",
      "Training loss: 0.187191441655159\n",
      "Training loss: 0.1035701185464859\n",
      "Training loss: 0.13840071856975555\n",
      "Training loss: 0.17831192910671234\n",
      "Training loss: 0.11489345878362656\n",
      "Training loss: 0.0879862904548645\n",
      "Training loss: 0.13961490988731384\n",
      "Training loss: 0.11997727304697037\n",
      "Training loss: 0.17513449490070343\n",
      "Training loss: 0.1090795025229454\n",
      "Training loss: 0.2063346803188324\n",
      "Training loss: 0.1256396323442459\n",
      "Training loss: 0.14875613152980804\n",
      "Training loss: 0.09554865956306458\n",
      "Training loss: 0.13415341079235077\n",
      "Training loss: 0.15115608274936676\n",
      "Training loss: 0.15942928194999695\n",
      "Training loss: 0.16004332900047302\n",
      "Training loss: 0.18722647428512573\n",
      "Training loss: 0.13801570236682892\n",
      "Training loss: 0.10214696079492569\n",
      "Training loss: 0.14307665824890137\n",
      "Training loss: 0.18570482730865479\n",
      "Training loss: 0.11292669177055359\n",
      "Training loss: 0.10586659610271454\n",
      "Training loss: 0.14451128244400024\n",
      "Training loss: 0.1003326028585434\n",
      "Training loss: 0.168678879737854\n",
      "Training loss: 0.14359650015830994\n",
      "Training loss: 0.15899346768856049\n",
      "Training loss: 0.1304503083229065\n",
      "Training loss: 0.1218944564461708\n",
      "Training loss: 0.13198870420455933\n",
      "Training loss: 0.12744590640068054\n",
      "Training loss: 0.08781090378761292\n",
      "Training loss: 0.0907379686832428\n",
      "Training loss: 0.13220250606536865\n",
      "Training loss: 0.06527820229530334\n",
      "Training loss: 0.11722294986248016\n",
      "Training loss: 0.1222563311457634\n",
      "Training loss: 0.18952348828315735\n",
      "Training loss: 0.1271694153547287\n",
      "Training loss: 0.15055805444717407\n",
      "Training loss: 0.11587770283222198\n",
      "Training loss: 0.12520377337932587\n",
      "Training loss: 0.1351977437734604\n",
      "Training loss: 0.12026084959506989\n",
      "Training loss: 0.11104471236467361\n",
      "Training loss: 0.15666690468788147\n",
      "Training loss: 0.1410067230463028\n",
      "Training loss: 0.1197793036699295\n",
      "Training loss: 0.1939823031425476\n",
      "Training loss: 0.08252804726362228\n",
      "Training loss: 0.13454830646514893\n",
      "Training loss: 0.19713708758354187\n",
      "Training loss: 0.18704582750797272\n",
      "Training loss: 0.10313776880502701\n",
      "Training loss: 0.1382213830947876\n",
      "Training loss: 0.17825351655483246\n",
      "Training loss: 0.11454693228006363\n",
      "Training loss: 0.0875893235206604\n",
      "Training loss: 0.13920612633228302\n",
      "Training loss: 0.11968734115362167\n",
      "Training loss: 0.17504188418388367\n",
      "Training loss: 0.10855810344219208\n",
      "Training loss: 0.2064695805311203\n",
      "Training loss: 0.12536215782165527\n",
      "Training loss: 0.14851249754428864\n",
      "Training loss: 0.09529075026512146\n",
      "Training loss: 0.1338731050491333\n",
      "Training loss: 0.15077659487724304\n",
      "Training loss: 0.15949395298957825\n",
      "Training loss: 0.16015326976776123\n",
      "Training loss: 0.18724343180656433\n",
      "Training loss: 0.13780216872692108\n",
      "Training loss: 0.10192307829856873\n",
      "Training loss: 0.14302770793437958\n",
      "Training loss: 0.18574264645576477\n",
      "Training loss: 0.11266811192035675\n",
      "Training loss: 0.10554491728544235\n",
      "Training loss: 0.1442614644765854\n",
      "Training loss: 0.10000917315483093\n",
      "Training loss: 0.16877701878547668\n",
      "Training loss: 0.14327935874462128\n",
      "Training loss: 0.15881289541721344\n",
      "Training loss: 0.1300324648618698\n",
      "Training loss: 0.12154759466648102\n",
      "Training loss: 0.13174374401569366\n",
      "Training loss: 0.12722823023796082\n",
      "Training loss: 0.08725389093160629\n",
      "Training loss: 0.09011983871459961\n",
      "Training loss: 0.13198703527450562\n",
      "Training loss: 0.06472501158714294\n",
      "Training loss: 0.11700773984193802\n",
      "Training loss: 0.12202572077512741\n",
      "Training loss: 0.18941974639892578\n",
      "Training loss: 0.126918762922287\n",
      "Training loss: 0.1503869593143463\n",
      "Training loss: 0.11560937017202377\n",
      "Training loss: 0.12481845170259476\n",
      "Training loss: 0.1348683387041092\n",
      "Training loss: 0.11995605379343033\n",
      "Training loss: 0.11073624342679977\n",
      "Training loss: 0.15650738775730133\n",
      "Training loss: 0.14078371226787567\n",
      "Training loss: 0.11961419880390167\n",
      "Training loss: 0.19397151470184326\n",
      "Training loss: 0.08205803483724594\n",
      "Training loss: 0.13438734412193298\n",
      "Training loss: 0.19714871048927307\n",
      "Training loss: 0.18691100180149078\n",
      "Training loss: 0.102724090218544\n",
      "Training loss: 0.13805408775806427\n",
      "Training loss: 0.17820590734481812\n",
      "Training loss: 0.114216148853302\n",
      "Training loss: 0.08721040189266205\n",
      "Training loss: 0.1388164609670639\n",
      "Training loss: 0.1194121465086937\n",
      "Training loss: 0.17495959997177124\n",
      "Training loss: 0.10805852711200714\n",
      "Training loss: 0.20660875737667084\n",
      "Training loss: 0.12509986758232117\n",
      "Training loss: 0.14828252792358398\n",
      "Training loss: 0.09504728019237518\n",
      "Training loss: 0.13360653817653656\n",
      "Training loss: 0.15041539072990417\n",
      "Training loss: 0.15956412255764008\n",
      "Training loss: 0.16026918590068817\n",
      "Training loss: 0.18726879358291626\n",
      "Training loss: 0.1376023143529892\n",
      "Training loss: 0.10171281546354294\n",
      "Training loss: 0.14298777282238007\n",
      "Training loss: 0.18578806519508362\n",
      "Training loss: 0.11242477595806122\n",
      "Training loss: 0.10523999482393265\n",
      "Training loss: 0.14402461051940918\n",
      "Training loss: 0.0997018963098526\n",
      "Training loss: 0.1688816398382187\n",
      "Training loss: 0.14297837018966675\n",
      "Training loss: 0.15864601731300354\n",
      "Training loss: 0.12963402271270752\n",
      "Training loss: 0.12121730297803879\n",
      "Training loss: 0.1315128058195114\n",
      "Training loss: 0.1270235776901245\n",
      "Training loss: 0.08671833574771881\n",
      "Training loss: 0.08952544629573822\n",
      "Training loss: 0.13178613781929016\n",
      "Training loss: 0.0641939714550972\n",
      "Training loss: 0.1168050616979599\n",
      "Training loss: 0.12180805951356888\n",
      "Training loss: 0.18932725489139557\n",
      "Training loss: 0.12668077647686005\n",
      "Training loss: 0.15022890269756317\n",
      "Training loss: 0.1153552383184433\n",
      "Training loss: 0.12445017695426941\n",
      "Training loss: 0.13455452024936676\n",
      "Training loss: 0.11966654658317566\n",
      "Training loss: 0.11044363677501678\n",
      "Training loss: 0.15635962784290314\n",
      "Training loss: 0.14057496190071106\n",
      "Training loss: 0.11946006864309311\n",
      "Training loss: 0.19396895170211792\n",
      "Training loss: 0.08160670101642609\n",
      "Training loss: 0.1342380791902542\n",
      "Training loss: 0.19716748595237732\n",
      "Training loss: 0.18678611516952515\n",
      "Training loss: 0.1023283451795578\n",
      "Training loss: 0.13789793848991394\n",
      "Training loss: 0.17816822230815887\n",
      "Training loss: 0.11390016973018646\n",
      "Training loss: 0.0868484228849411\n",
      "Training loss: 0.13844512403011322\n",
      "Training loss: 0.11915085464715958\n",
      "Training loss: 0.1748870611190796\n",
      "Training loss: 0.10757946223020554\n",
      "Training loss: 0.20675139129161835\n",
      "Training loss: 0.12485183030366898\n",
      "Training loss: 0.14806510508060455\n",
      "Training loss: 0.09481751918792725\n",
      "Training loss: 0.13335290551185608\n",
      "Training loss: 0.15007136762142181\n",
      "Training loss: 0.15963929891586304\n",
      "Training loss: 0.16039034724235535\n",
      "Training loss: 0.18730179965496063\n",
      "Training loss: 0.13741537928581238\n",
      "Training loss: 0.10151530057191849\n",
      "Training loss: 0.1429561972618103\n",
      "Training loss: 0.18584048748016357\n",
      "Training loss: 0.11219610273838043\n",
      "Training loss: 0.10495114326477051\n",
      "Training loss: 0.14379949867725372\n",
      "Training loss: 0.09940987825393677\n",
      "Training loss: 0.16899198293685913\n",
      "Training loss: 0.14269253611564636\n",
      "Training loss: 0.15849201381206512\n",
      "Training loss: 0.1292538344860077\n",
      "Training loss: 0.12090237438678741\n",
      "Training loss: 0.13129527866840363\n",
      "Training loss: 0.1268310397863388\n",
      "Training loss: 0.08620309829711914\n",
      "Training loss: 0.08895346522331238\n",
      "Training loss: 0.13159891963005066\n",
      "Training loss: 0.06368406116962433\n",
      "Training loss: 0.11661355942487717\n",
      "Training loss: 0.12160217761993408\n",
      "Training loss: 0.18924564123153687\n",
      "Training loss: 0.12645447254180908\n",
      "Training loss: 0.15008260309696198\n",
      "Training loss: 0.11511445790529251\n",
      "Training loss: 0.12409807741641998\n",
      "Training loss: 0.13425549864768982\n",
      "Training loss: 0.119391530752182\n",
      "Training loss: 0.11016619205474854\n",
      "Training loss: 0.1562228500843048\n",
      "Training loss: 0.1403800994157791\n",
      "Training loss: 0.11931625008583069\n",
      "Training loss: 0.19397366046905518\n",
      "Training loss: 0.0811733603477478\n",
      "Training loss: 0.1340997815132141\n",
      "Training loss: 0.1971927285194397\n",
      "Training loss: 0.1866704821586609\n",
      "Training loss: 0.10194959491491318\n",
      "Training loss: 0.1377522349357605\n",
      "Training loss: 0.17813974618911743\n",
      "Training loss: 0.11359826475381851\n",
      "Training loss: 0.08650262653827667\n",
      "Training loss: 0.1380910724401474\n",
      "Training loss: 0.11890271306037903\n",
      "Training loss: 0.17482344806194305\n",
      "Training loss: 0.10712011903524399\n",
      "Training loss: 0.2068970650434494\n",
      "Training loss: 0.12461721897125244\n",
      "Training loss: 0.14785976707935333\n",
      "Training loss: 0.09460058808326721\n",
      "Training loss: 0.13311153650283813\n",
      "Training loss: 0.14974376559257507\n",
      "Training loss: 0.15971876680850983\n",
      "Training loss: 0.16051611304283142\n",
      "Training loss: 0.18734171986579895\n",
      "Training loss: 0.13724039494991302\n",
      "Training loss: 0.10132976621389389\n",
      "Training loss: 0.1429322361946106\n",
      "Training loss: 0.1858990490436554\n",
      "Training loss: 0.11198090016841888\n",
      "Training loss: 0.10467714816331863\n",
      "Training loss: 0.1435859054327011\n",
      "Training loss: 0.09913226962089539\n",
      "Training loss: 0.1691073775291443\n",
      "Training loss: 0.14242109656333923\n",
      "Training loss: 0.1583499014377594\n",
      "Training loss: 0.12889112532138824\n",
      "Training loss: 0.1206023320555687\n",
      "Training loss: 0.13109014928340912\n",
      "Training loss: 0.12664997577667236\n",
      "Training loss: 0.08570744842290878\n",
      "Training loss: 0.08840318024158478\n",
      "Training loss: 0.1314244270324707\n",
      "Training loss: 0.0631943866610527\n",
      "Training loss: 0.11643321812152863\n",
      "Training loss: 0.12140778452157974\n",
      "Training loss: 0.18917354941368103\n",
      "Training loss: 0.12623952329158783\n",
      "Training loss: 0.14994768798351288\n",
      "Training loss: 0.1148863211274147\n",
      "Training loss: 0.12376139312982559\n",
      "Training loss: 0.13397052884101868\n",
      "Training loss: 0.11913023889064789\n",
      "Training loss: 0.10990291833877563\n",
      "Training loss: 0.15609627962112427\n",
      "Training loss: 0.14019787311553955\n",
      "Training loss: 0.1191820353269577\n",
      "Training loss: 0.19398528337478638\n",
      "Training loss: 0.08075711131095886\n",
      "Training loss: 0.1339716613292694\n",
      "Training loss: 0.19722379744052887\n",
      "Training loss: 0.1865636706352234\n",
      "Training loss: 0.10158705711364746\n",
      "Training loss: 0.1376163214445114\n",
      "Training loss: 0.17811961472034454\n",
      "Training loss: 0.11330978572368622\n",
      "Training loss: 0.08617222309112549\n",
      "Training loss: 0.13775351643562317\n",
      "Training loss: 0.11866700649261475\n",
      "Training loss: 0.17476814985275269\n",
      "Training loss: 0.1066795215010643\n",
      "Training loss: 0.20704509317874908\n",
      "Training loss: 0.12439532577991486\n",
      "Training loss: 0.14766573905944824\n",
      "Training loss: 0.09439574927091599\n",
      "Training loss: 0.1328817456960678\n",
      "Training loss: 0.14943170547485352\n",
      "Training loss: 0.15980206429958344\n",
      "Training loss: 0.16064582765102386\n",
      "Training loss: 0.18738782405853271\n",
      "Training loss: 0.13707678020000458\n",
      "Training loss: 0.10115550458431244\n",
      "Training loss: 0.14291523396968842\n",
      "Training loss: 0.1859632134437561\n",
      "Training loss: 0.11177856475114822\n",
      "Training loss: 0.10441739857196808\n",
      "Training loss: 0.14338290691375732\n",
      "Training loss: 0.09886829555034637\n",
      "Training loss: 0.16922718286514282\n",
      "Training loss: 0.1421632021665573\n",
      "Training loss: 0.15821899473667145\n",
      "Training loss: 0.1285449117422104\n",
      "Training loss: 0.12031607329845428\n",
      "Training loss: 0.13089697062969208\n",
      "Training loss: 0.12647958099842072\n",
      "Training loss: 0.08523045480251312\n",
      "Training loss: 0.08787348121404648\n",
      "Training loss: 0.131261944770813\n",
      "Training loss: 0.06272397935390472\n",
      "Training loss: 0.11626268178224564\n",
      "Training loss: 0.1212237998843193\n",
      "Training loss: 0.18911081552505493\n",
      "Training loss: 0.12603498995304108\n",
      "Training loss: 0.14982302486896515\n",
      "Training loss: 0.11467006802558899\n",
      "Training loss: 0.12343933433294296\n",
      "Training loss: 0.13369879126548767\n",
      "Training loss: 0.11888188868761063\n",
      "Training loss: 0.10965320467948914\n",
      "Training loss: 0.15597930550575256\n",
      "Training loss: 0.14002785086631775\n",
      "Training loss: 0.11905688792467117\n",
      "Training loss: 0.19400285184383392\n",
      "Training loss: 0.08035722374916077\n",
      "Training loss: 0.1338532567024231\n",
      "Training loss: 0.19726014137268066\n",
      "Training loss: 0.18646468222141266\n",
      "Training loss: 0.10123996436595917\n",
      "Training loss: 0.137489452958107\n",
      "Training loss: 0.17810720205307007\n",
      "Training loss: 0.11303382366895676\n",
      "Training loss: 0.08585619926452637\n",
      "Training loss: 0.13743165135383606\n",
      "Training loss: 0.11844298988580704\n",
      "Training loss: 0.1747206449508667\n",
      "Training loss: 0.10625650733709335\n",
      "Training loss: 0.20719490945339203\n",
      "Training loss: 0.12418530136346817\n",
      "Training loss: 0.14748218655586243\n",
      "Training loss: 0.09420238435268402\n",
      "Training loss: 0.132662832736969\n",
      "Training loss: 0.14913421869277954\n",
      "Training loss: 0.15988868474960327\n",
      "Training loss: 0.1607789248228073\n",
      "Training loss: 0.18743959069252014\n",
      "Training loss: 0.13692373037338257\n",
      "Training loss: 0.10099177807569504\n",
      "Training loss: 0.14290477335453033\n",
      "Training loss: 0.18603256344795227\n",
      "Training loss: 0.11158864945173264\n",
      "Training loss: 0.1041712835431099\n",
      "Training loss: 0.1431894600391388\n",
      "Training loss: 0.09861724823713303\n",
      "Training loss: 0.16935104131698608\n",
      "Training loss: 0.14191804826259613\n",
      "Training loss: 0.15809845924377441\n",
      "Training loss: 0.1282142698764801\n",
      "Training loss: 0.12004277110099792\n",
      "Training loss: 0.13071531057357788\n",
      "Training loss: 0.1263192743062973\n",
      "Training loss: 0.0847712978720665\n",
      "Training loss: 0.08736345171928406\n",
      "Training loss: 0.13111068308353424\n",
      "Training loss: 0.0622720941901207\n",
      "Training loss: 0.11610139161348343\n",
      "Training loss: 0.12104960530996323\n",
      "Training loss: 0.18905682861804962\n",
      "Training loss: 0.12584033608436584\n",
      "Training loss: 0.14970791339874268\n",
      "Training loss: 0.11446507275104523\n",
      "Training loss: 0.12313126027584076\n",
      "Training loss: 0.13343968987464905\n",
      "Training loss: 0.11864578723907471\n",
      "Training loss: 0.10941629111766815\n",
      "Training loss: 0.15587128698825836\n",
      "Training loss: 0.1398693472146988\n",
      "Training loss: 0.11894018948078156\n",
      "Training loss: 0.19402587413787842\n",
      "Training loss: 0.07997303456068039\n",
      "Training loss: 0.13374382257461548\n",
      "Training loss: 0.1973012089729309\n",
      "Training loss: 0.1863732784986496\n",
      "Training loss: 0.1009075790643692\n",
      "Training loss: 0.13737112283706665\n",
      "Training loss: 0.17810189723968506\n",
      "Training loss: 0.1127699464559555\n",
      "Training loss: 0.0855540782213211\n",
      "Training loss: 0.13712464272975922\n",
      "Training loss: 0.11823011934757233\n",
      "Training loss: 0.1746802031993866\n",
      "Training loss: 0.10585065186023712\n",
      "Training loss: 0.20734627544879913\n",
      "Training loss: 0.12398658692836761\n",
      "Training loss: 0.14730866253376007\n",
      "Training loss: 0.09401974827051163\n",
      "Training loss: 0.13245433568954468\n",
      "Training loss: 0.14885076880455017\n",
      "Training loss: 0.15997810661792755\n",
      "Training loss: 0.160914808511734\n",
      "Training loss: 0.1874963790178299\n",
      "Training loss: 0.13678058981895447\n",
      "Training loss: 0.10083802789449692\n",
      "Training loss: 0.14290006458759308\n",
      "Training loss: 0.18610623478889465\n",
      "Training loss: 0.11140990257263184\n",
      "Training loss: 0.10393769294023514\n",
      "Training loss: 0.1430058777332306\n",
      "Training loss: 0.09837843477725983\n",
      "Training loss: 0.16947804391384125\n",
      "Training loss: 0.1416851431131363\n",
      "Training loss: 0.15798769891262054\n",
      "Training loss: 0.1278986930847168\n",
      "Training loss: 0.11978219449520111\n",
      "Training loss: 0.13054391741752625\n",
      "Training loss: 0.12616848945617676\n",
      "Training loss: 0.08432933688163757\n",
      "Training loss: 0.08687250316143036\n",
      "Training loss: 0.13096986711025238\n",
      "Training loss: 0.06183791533112526\n",
      "Training loss: 0.11594930291175842\n",
      "Training loss: 0.1208849623799324\n",
      "Training loss: 0.1890103965997696\n",
      "Training loss: 0.12565523386001587\n",
      "Training loss: 0.14960195124149323\n",
      "Training loss: 0.11427070200443268\n",
      "Training loss: 0.12283647805452347\n",
      "Training loss: 0.13319259881973267\n",
      "Training loss: 0.1184212863445282\n",
      "Training loss: 0.10919143259525299\n",
      "Training loss: 0.15577170252799988\n",
      "Training loss: 0.13972121477127075\n",
      "Training loss: 0.11883140355348587\n",
      "Training loss: 0.19405385851860046\n",
      "Training loss: 0.07960376143455505\n",
      "Training loss: 0.13364282250404358\n",
      "Training loss: 0.1973465383052826\n",
      "Training loss: 0.18628865480422974\n",
      "Training loss: 0.10058925300836563\n",
      "Training loss: 0.13726076483726501\n",
      "Training loss: 0.1781029850244522\n",
      "Training loss: 0.1125175878405571\n",
      "Training loss: 0.08526510745286942\n",
      "Training loss: 0.13683173060417175\n",
      "Training loss: 0.11802773177623749\n",
      "Training loss: 0.1746463179588318\n",
      "Training loss: 0.10546107590198517\n",
      "Training loss: 0.20749855041503906\n",
      "Training loss: 0.12379851937294006\n",
      "Training loss: 0.14714466035366058\n",
      "Training loss: 0.09384716302156448\n",
      "Training loss: 0.13225562870502472\n",
      "Training loss: 0.1485806554555893\n",
      "Training loss: 0.16006992757320404\n",
      "Training loss: 0.16105306148529053\n",
      "Training loss: 0.18755769729614258\n",
      "Training loss: 0.13664670288562775\n",
      "Training loss: 0.10069359838962555\n",
      "Training loss: 0.14290070533752441\n",
      "Training loss: 0.18618375062942505\n",
      "Training loss: 0.11124186217784882\n",
      "Training loss: 0.10371605306863785\n",
      "Training loss: 0.14283117651939392\n",
      "Training loss: 0.09815119206905365\n",
      "Training loss: 0.16960789263248444\n",
      "Training loss: 0.14146369695663452\n",
      "Training loss: 0.15788592398166656\n",
      "Training loss: 0.12759734690189362\n",
      "Training loss: 0.11953352391719818\n",
      "Training loss: 0.13038237392902374\n",
      "Training loss: 0.12602660059928894\n",
      "Training loss: 0.08390381932258606\n",
      "Training loss: 0.08639977872371674\n",
      "Training loss: 0.13083887100219727\n",
      "Training loss: 0.06142060458660126\n",
      "Training loss: 0.11580585688352585\n",
      "Training loss: 0.12072926014661789\n",
      "Training loss: 0.18897101283073425\n",
      "Training loss: 0.12547913193702698\n",
      "Training loss: 0.14950455725193024\n",
      "Training loss: 0.11408640444278717\n",
      "Training loss: 0.12255433201789856\n",
      "Training loss: 0.13295693695545197\n",
      "Training loss: 0.11820779740810394\n",
      "Training loss: 0.10897795855998993\n",
      "Training loss: 0.1556798666715622\n",
      "Training loss: 0.13958309590816498\n",
      "Training loss: 0.11873000860214233\n",
      "Training loss: 0.19408655166625977\n",
      "Training loss: 0.07924884557723999\n",
      "Training loss: 0.13354961574077606\n",
      "Training loss: 0.1973957121372223\n",
      "Training loss: 0.18621084094047546\n",
      "Training loss: 0.10028427839279175\n",
      "Training loss: 0.13715794682502747\n",
      "Training loss: 0.17810995876789093\n",
      "Training loss: 0.11227618157863617\n",
      "Training loss: 0.08498869091272354\n",
      "Training loss: 0.13655224442481995\n",
      "Training loss: 0.11783532053232193\n",
      "Training loss: 0.17461848258972168\n",
      "Training loss: 0.10508707910776138\n",
      "Training loss: 0.20765148103237152\n",
      "Training loss: 0.12362051010131836\n",
      "Training loss: 0.14698955416679382\n",
      "Training loss: 0.09368415921926498\n",
      "Training loss: 0.1320662647485733\n",
      "Training loss: 0.14832311868667603\n",
      "Training loss: 0.1601637601852417\n",
      "Training loss: 0.1611931025981903\n",
      "Training loss: 0.18762291967868805\n",
      "Training loss: 0.1365216076374054\n",
      "Training loss: 0.10055795311927795\n",
      "Training loss: 0.14290620386600494\n",
      "Training loss: 0.18626472353935242\n",
      "Training loss: 0.11108391731977463\n",
      "Training loss: 0.10350580513477325\n",
      "Training loss: 0.14266493916511536\n",
      "Training loss: 0.09793493896722794\n",
      "Training loss: 0.16974008083343506\n",
      "Training loss: 0.14125309884548187\n",
      "Training loss: 0.15779264271259308\n",
      "Training loss: 0.12730950117111206\n",
      "Training loss: 0.11929602175951004\n",
      "Training loss: 0.1302303969860077\n",
      "Training loss: 0.12589311599731445\n",
      "Training loss: 0.08349408209323883\n",
      "Training loss: 0.08594446629285812\n",
      "Training loss: 0.13071708381175995\n",
      "Training loss: 0.06101957708597183\n",
      "Training loss: 0.11566999554634094\n",
      "Training loss: 0.1205817312002182\n",
      "Training loss: 0.18893858790397644\n",
      "Training loss: 0.12531140446662903\n",
      "Training loss: 0.14941483736038208\n",
      "Training loss: 0.11391159147024155\n",
      "Training loss: 0.12228432297706604\n",
      "Training loss: 0.132732093334198\n",
      "Training loss: 0.11800473928451538\n",
      "Training loss: 0.10877547413110733\n",
      "Training loss: 0.15559542179107666\n",
      "Training loss: 0.13945451378822327\n",
      "Training loss: 0.1186356246471405\n",
      "Training loss: 0.19412297010421753\n",
      "Training loss: 0.07890767604112625\n",
      "Training loss: 0.13346387445926666\n",
      "Training loss: 0.19744819402694702\n",
      "Training loss: 0.18613888323307037\n",
      "Training loss: 0.09999212622642517\n",
      "Training loss: 0.13706201314926147\n",
      "Training loss: 0.17812226712703705\n",
      "Training loss: 0.11204507946968079\n",
      "Training loss: 0.08472409099340439\n",
      "Training loss: 0.13628555834293365\n",
      "Training loss: 0.11765231192111969\n",
      "Training loss: 0.17459623515605927\n",
      "Training loss: 0.10472779721021652\n",
      "Training loss: 0.2078045904636383\n",
      "Training loss: 0.12345192581415176\n",
      "Training loss: 0.14684276282787323\n",
      "Training loss: 0.09353014081716537\n",
      "Training loss: 0.13188567757606506\n",
      "Training loss: 0.14807754755020142\n",
      "Training loss: 0.16025923192501068\n",
      "Training loss: 0.1613345742225647\n",
      "Training loss: 0.18769173324108124\n",
      "Training loss: 0.13640464842319489\n",
      "Training loss: 0.10043054074048996\n",
      "Training loss: 0.14291617274284363\n",
      "Training loss: 0.18634870648384094\n",
      "Training loss: 0.11093564331531525\n",
      "Training loss: 0.1033063605427742\n",
      "Training loss: 0.14250655472278595\n",
      "Training loss: 0.09772911667823792\n",
      "Training loss: 0.16987420618534088\n",
      "Training loss: 0.14105281233787537\n",
      "Training loss: 0.15770724415779114\n",
      "Training loss: 0.12703447043895721\n",
      "Training loss: 0.11906914412975311\n",
      "Training loss: 0.13008736073970795\n",
      "Training loss: 0.1257675141096115\n",
      "Training loss: 0.08309949934482574\n",
      "Training loss: 0.0855059027671814\n",
      "Training loss: 0.13060390949249268\n",
      "Training loss: 0.060634102672338486\n",
      "Training loss: 0.1155414804816246\n",
      "Training loss: 0.12044201046228409\n",
      "Training loss: 0.18891237676143646\n",
      "Training loss: 0.1251516193151474\n",
      "Training loss: 0.14933232963085175\n",
      "Training loss: 0.11374577134847641\n",
      "Training loss: 0.12202584743499756\n",
      "Training loss: 0.13251757621765137\n",
      "Training loss: 0.11781151592731476\n",
      "Training loss: 0.10858321934938431\n",
      "Training loss: 0.1555177867412567\n",
      "Training loss: 0.1393347531557083\n",
      "Training loss: 0.11854776740074158\n",
      "Training loss: 0.19416292011737823\n",
      "Training loss: 0.07857967168092728\n",
      "Training loss: 0.1333850622177124\n",
      "Training loss: 0.19750359654426575\n",
      "Training loss: 0.18607237935066223\n",
      "Training loss: 0.09971220791339874\n",
      "Training loss: 0.1369725465774536\n",
      "Training loss: 0.17813941836357117\n",
      "Training loss: 0.11182381957769394\n",
      "Training loss: 0.08447079360485077\n",
      "Training loss: 0.13603109121322632\n",
      "Training loss: 0.11747822165489197\n",
      "Training loss: 0.1745792031288147\n",
      "Training loss: 0.10438263416290283\n",
      "Training loss: 0.20795753598213196\n",
      "Training loss: 0.12329228967428207\n",
      "Training loss: 0.14670386910438538\n",
      "Training loss: 0.09338465332984924\n",
      "Training loss: 0.1317133754491806\n",
      "Training loss: 0.14784328639507294\n",
      "Training loss: 0.1603560447692871\n",
      "Training loss: 0.16147708892822266\n",
      "Training loss: 0.18776360154151917\n",
      "Training loss: 0.1362953782081604\n",
      "Training loss: 0.10031087696552277\n",
      "Training loss: 0.14293017983436584\n",
      "Training loss: 0.18643535673618317\n",
      "Training loss: 0.11079651862382889\n",
      "Training loss: 0.10311725735664368\n",
      "Training loss: 0.14235539734363556\n",
      "Training loss: 0.09753316640853882\n",
      "Training loss: 0.17000994086265564\n",
      "Training loss: 0.14086222648620605\n",
      "Training loss: 0.15762916207313538\n",
      "Training loss: 0.12677164375782013\n",
      "Training loss: 0.11885228753089905\n",
      "Training loss: 0.12995295226573944\n",
      "Training loss: 0.1256493180990219\n",
      "Training loss: 0.08271947503089905\n",
      "Training loss: 0.08508339524269104\n",
      "Training loss: 0.13049881160259247\n",
      "Training loss: 0.06026361137628555\n",
      "Training loss: 0.11541977524757385\n",
      "Training loss: 0.12030954658985138\n",
      "Training loss: 0.18889200687408447\n",
      "Training loss: 0.12499940395355225\n",
      "Training loss: 0.14925658702850342\n",
      "Training loss: 0.11358846724033356\n",
      "Training loss: 0.12177839875221252\n",
      "Training loss: 0.1323128491640091\n",
      "Training loss: 0.11762773245573044\n",
      "Training loss: 0.10840079188346863\n",
      "Training loss: 0.15544646978378296\n",
      "Training loss: 0.1392233669757843\n",
      "Training loss: 0.11846598237752914\n",
      "Training loss: 0.194206103682518\n",
      "Training loss: 0.07826429605484009\n",
      "Training loss: 0.13331255316734314\n",
      "Training loss: 0.197561576962471\n",
      "Training loss: 0.18601137399673462\n",
      "Training loss: 0.09944385290145874\n",
      "Training loss: 0.1368892937898636\n",
      "Training loss: 0.17816096544265747\n",
      "Training loss: 0.11161209642887115\n",
      "Training loss: 0.08422838151454926\n",
      "Training loss: 0.13578812777996063\n",
      "Training loss: 0.11731262505054474\n",
      "Training loss: 0.1745668202638626\n",
      "Training loss: 0.10405124723911285\n",
      "Training loss: 0.2081102430820465\n",
      "Training loss: 0.12314111739397049\n",
      "Training loss: 0.1465725600719452\n",
      "Training loss: 0.09324715286493301\n",
      "Training loss: 0.13154909014701843\n",
      "Training loss: 0.14761991798877716\n",
      "Training loss: 0.16045381128787994\n",
      "Training loss: 0.16162024438381195\n",
      "Training loss: 0.1878381073474884\n",
      "Training loss: 0.13619332015514374\n",
      "Training loss: 0.10019848495721817\n",
      "Training loss: 0.1429477483034134\n",
      "Training loss: 0.18652404844760895\n",
      "Training loss: 0.11066574603319168\n",
      "Training loss: 0.10293768346309662\n",
      "Training loss: 0.14221155643463135\n",
      "Training loss: 0.09734660387039185\n",
      "Training loss: 0.17014667391777039\n",
      "Training loss: 0.1406809240579605\n",
      "Training loss: 0.157557874917984\n",
      "Training loss: 0.1265205442905426\n",
      "Training loss: 0.11864515393972397\n",
      "Training loss: 0.12982627749443054\n",
      "Training loss: 0.12553812563419342\n",
      "Training loss: 0.08235345035791397\n",
      "Training loss: 0.08467643707990646\n",
      "Training loss: 0.13040117919445038\n",
      "Training loss: 0.05990740284323692\n",
      "Training loss: 0.11530479043722153\n",
      "Training loss: 0.12018413841724396\n",
      "Training loss: 0.18887671828269958\n",
      "Training loss: 0.12485440075397491\n",
      "Training loss: 0.14918719232082367\n",
      "Training loss: 0.11343921720981598\n",
      "Training loss: 0.12154145538806915\n",
      "Training loss: 0.13211746513843536\n",
      "Training loss: 0.11745283007621765\n",
      "Training loss: 0.10822755843400955\n",
      "Training loss: 0.15538111329078674\n",
      "Training loss: 0.13911980390548706\n",
      "Training loss: 0.11838992685079575\n",
      "Training loss: 0.19425195455551147\n",
      "Training loss: 0.07796107977628708\n",
      "Training loss: 0.13324618339538574\n",
      "Training loss: 0.19762179255485535\n",
      "Training loss: 0.18595507740974426\n",
      "Training loss: 0.09918670356273651\n",
      "Training loss: 0.13681168854236603\n",
      "Training loss: 0.17818646132946014\n",
      "Training loss: 0.11140932887792587\n",
      "Training loss: 0.08399622142314911\n",
      "Training loss: 0.13555628061294556\n",
      "Training loss: 0.117155060172081\n",
      "Training loss: 0.1745588183403015\n",
      "Training loss: 0.10373272746801376\n",
      "Training loss: 0.208262100815773\n",
      "Training loss: 0.1229979619383812\n",
      "Training loss: 0.14644820988178253\n",
      "Training loss: 0.09311722964048386\n",
      "Training loss: 0.13139228522777557\n",
      "Training loss: 0.14740687608718872\n",
      "Training loss: 0.16055217385292053\n",
      "Training loss: 0.16176369786262512\n",
      "Training loss: 0.18791493773460388\n",
      "Training loss: 0.1360979676246643\n",
      "Training loss: 0.10009303689002991\n",
      "Training loss: 0.14296859502792358\n",
      "Training loss: 0.18661460280418396\n",
      "Training loss: 0.11054303497076035\n",
      "Training loss: 0.10276725888252258\n",
      "Training loss: 0.14207445085048676\n",
      "Training loss: 0.09716896712779999\n",
      "Training loss: 0.17028428614139557\n",
      "Training loss: 0.14050841331481934\n",
      "Training loss: 0.15749293565750122\n",
      "Training loss: 0.12628065049648285\n",
      "Training loss: 0.11844727396965027\n",
      "Training loss: 0.12970715761184692\n",
      "Training loss: 0.1254335641860962\n",
      "Training loss: 0.0820009633898735\n",
      "Training loss: 0.0842844545841217\n",
      "Training loss: 0.13031063973903656\n",
      "Training loss: 0.059565093368291855\n",
      "Training loss: 0.11519600450992584\n",
      "Training loss: 0.12006530910730362\n",
      "Training loss: 0.1888662725687027\n",
      "Training loss: 0.12471625953912735\n",
      "Training loss: 0.14912360906600952\n",
      "Training loss: 0.11329761892557144\n",
      "Training loss: 0.1213146448135376\n",
      "Training loss: 0.13193106651306152\n",
      "Training loss: 0.11728636920452118\n",
      "Training loss: 0.10806315392255783\n",
      "Training loss: 0.1553214192390442\n",
      "Training loss: 0.13902336359024048\n",
      "Training loss: 0.11831924319267273\n",
      "Training loss: 0.19430018961429596\n",
      "Training loss: 0.07766947150230408\n",
      "Training loss: 0.1331854611635208\n",
      "Training loss: 0.19768396019935608\n",
      "Training loss: 0.18590322136878967\n",
      "Training loss: 0.09894029051065445\n",
      "Training loss: 0.13673943281173706\n",
      "Training loss: 0.17821545898914337\n",
      "Training loss: 0.11121521890163422\n",
      "Training loss: 0.08377397060394287\n",
      "Training loss: 0.13533490896224976\n",
      "Training loss: 0.1170051097869873\n",
      "Training loss: 0.1745547652244568\n",
      "Training loss: 0.10342688858509064\n",
      "Training loss: 0.2084130048751831\n",
      "Training loss: 0.12286242842674255\n",
      "Training loss: 0.14633065462112427\n",
      "Training loss: 0.09299434721469879\n",
      "Training loss: 0.13124269247055054\n",
      "Training loss: 0.14720375835895538\n",
      "Training loss: 0.16065099835395813\n",
      "Training loss: 0.16190709173679352\n",
      "Training loss: 0.18799370527267456\n",
      "Training loss: 0.1360088586807251\n",
      "Training loss: 0.09999401867389679\n",
      "Training loss: 0.142992302775383\n",
      "Training loss: 0.1867065131664276\n",
      "Training loss: 0.11042775213718414\n",
      "Training loss: 0.1026054248213768\n",
      "Training loss: 0.14194385707378387\n",
      "Training loss: 0.09699979424476624\n",
      "Training loss: 0.17042233049869537\n",
      "Training loss: 0.1403442919254303\n",
      "Training loss: 0.15743395686149597\n",
      "Training loss: 0.12605144083499908\n",
      "Training loss: 0.11825814843177795\n",
      "Training loss: 0.1295951008796692\n",
      "Training loss: 0.1253352165222168\n",
      "Training loss: 0.08166145533323288\n",
      "Training loss: 0.08390688896179199\n",
      "Training loss: 0.13022667169570923\n",
      "Training loss: 0.05923604592680931\n",
      "Training loss: 0.11509323865175247\n",
      "Training loss: 0.11995276063680649\n",
      "Training loss: 0.18886004388332367\n",
      "Training loss: 0.12458467483520508\n",
      "Training loss: 0.14906558394432068\n",
      "Training loss: 0.11316326260566711\n",
      "Training loss: 0.12109746038913727\n",
      "Training loss: 0.13175319135189056\n",
      "Training loss: 0.1171279326081276\n",
      "Training loss: 0.10790707170963287\n",
      "Training loss: 0.15526697039604187\n",
      "Training loss: 0.13893373310565948\n",
      "Training loss: 0.11825358867645264\n",
      "Training loss: 0.19435051083564758\n",
      "Training loss: 0.07738901674747467\n",
      "Training loss: 0.1331300139427185\n",
      "Training loss: 0.19774776697158813\n",
      "Training loss: 0.1858557015657425\n",
      "Training loss: 0.09870411455631256\n",
      "Training loss: 0.13667212426662445\n",
      "Training loss: 0.1782475858926773\n",
      "Training loss: 0.11102929711341858\n",
      "Training loss: 0.08356104791164398\n",
      "Training loss: 0.13512355089187622\n",
      "Training loss: 0.11686240136623383\n",
      "Training loss: 0.17455433309078217\n",
      "Training loss: 0.10313296318054199\n",
      "Training loss: 0.2085626721382141\n",
      "Training loss: 0.12273402512073517\n",
      "Training loss: 0.146219402551651\n",
      "Training loss: 0.09287817031145096\n",
      "Training loss: 0.1310998499393463\n",
      "Training loss: 0.14700999855995178\n",
      "Training loss: 0.1607499122619629\n",
      "Training loss: 0.16205015778541565\n",
      "Training loss: 0.18807411193847656\n",
      "Training loss: 0.13592566549777985\n",
      "Training loss: 0.09990108013153076\n",
      "Training loss: 0.14301860332489014\n",
      "Training loss: 0.18679960072040558\n",
      "Training loss: 0.1103196069598198\n",
      "Training loss: 0.10245177894830704\n",
      "Training loss: 0.14181923866271973\n",
      "Training loss: 0.09683863073587418\n",
      "Training loss: 0.17056065797805786\n",
      "Training loss: 0.14018799364566803\n",
      "Training loss: 0.15738044679164886\n",
      "Training loss: 0.12583231925964355\n",
      "Training loss: 0.11807730793952942\n",
      "Training loss: 0.1294897347688675\n",
      "Training loss: 0.125242680311203\n",
      "Training loss: 0.08133438974618912\n",
      "Training loss: 0.0835430771112442\n",
      "Training loss: 0.13014887273311615\n",
      "Training loss: 0.05891970545053482\n",
      "Training loss: 0.11499594897031784\n",
      "Training loss: 0.11984603106975555\n",
      "Training loss: 0.18885788321495056\n",
      "Training loss: 0.12445920705795288\n",
      "Training loss: 0.14901256561279297\n",
      "Training loss: 0.11303573846817017\n",
      "Training loss: 0.12088945508003235\n",
      "Training loss: 0.13158337771892548\n",
      "Training loss: 0.11697707325220108\n",
      "Training loss: 0.10775886476039886\n",
      "Training loss: 0.15521736443042755\n",
      "Training loss: 0.13885043561458588\n",
      "Training loss: 0.11819262057542801\n",
      "Training loss: 0.19440264999866486\n",
      "Training loss: 0.07711923867464066\n",
      "Training loss: 0.13307945430278778\n",
      "Training loss: 0.19781291484832764\n",
      "Training loss: 0.1858120560646057\n",
      "Training loss: 0.09847765415906906\n",
      "Training loss: 0.1366094946861267\n",
      "Training loss: 0.17828251421451569\n",
      "Training loss: 0.1108512133359909\n",
      "Training loss: 0.0833570659160614\n",
      "Training loss: 0.13492175936698914\n",
      "Training loss: 0.11672651767730713\n",
      "Training loss: 0.17455720901489258\n",
      "Training loss: 0.10285048186779022\n",
      "Training loss: 0.2087109237909317\n",
      "Training loss: 0.12261242419481277\n",
      "Training loss: 0.14611412584781647\n",
      "Training loss: 0.09276830404996872\n",
      "Training loss: 0.1309635043144226\n",
      "Training loss: 0.1468251645565033\n",
      "Training loss: 0.1608487069606781\n",
      "Training loss: 0.1621926724910736\n",
      "Training loss: 0.18815584480762482\n",
      "Training loss: 0.13584797084331512\n",
      "Training loss: 0.09981384873390198\n",
      "Training loss: 0.14304718375205994\n",
      "Training loss: 0.18689346313476562\n",
      "Training loss: 0.1102181077003479\n",
      "Training loss: 0.10230587422847748\n",
      "Training loss: 0.14170034229755402\n",
      "Training loss: 0.09668509662151337\n",
      "Training loss: 0.17069882154464722\n",
      "Training loss: 0.14003922045230865\n",
      "Training loss: 0.15733198821544647\n",
      "Training loss: 0.12562286853790283\n",
      "Training loss: 0.11790437996387482\n",
      "Training loss: 0.1293906569480896\n",
      "Training loss: 0.12515562772750854\n",
      "Training loss: 0.081019327044487\n",
      "Training loss: 0.08319258689880371\n",
      "Training loss: 0.13007678091526031\n",
      "Training loss: 0.058615587651729584\n",
      "Training loss: 0.11490385979413986\n",
      "Training loss: 0.11974481493234634\n",
      "Training loss: 0.18885938823223114\n",
      "Training loss: 0.12433955818414688\n",
      "Training loss: 0.14896422624588013\n",
      "Training loss: 0.11291468888521194\n",
      "Training loss: 0.12069026380777359\n",
      "Training loss: 0.1314212828874588\n",
      "Training loss: 0.11683348566293716\n",
      "Training loss: 0.10761822760105133\n",
      "Training loss: 0.15517227351665497\n",
      "Training loss: 0.13877329230308533\n",
      "Training loss: 0.11813609302043915\n",
      "Training loss: 0.19445635378360748\n",
      "Training loss: 0.07685983180999756\n",
      "Training loss: 0.13303354382514954\n",
      "Training loss: 0.1978791058063507\n",
      "Training loss: 0.18577182292938232\n",
      "Training loss: 0.09826061129570007\n",
      "Training loss: 0.1365511417388916\n",
      "Training loss: 0.17831993103027344\n",
      "Training loss: 0.11068053543567657\n",
      "Training loss: 0.08316159248352051\n",
      "Training loss: 0.13472908735275269\n",
      "Training loss: 0.11659715324640274\n",
      "Training loss: 0.17456309497356415\n",
      "Training loss: 0.10257893055677414\n",
      "Training loss: 0.2088574916124344\n",
      "Training loss: 0.12249718606472015\n",
      "Training loss: 0.14601440727710724\n",
      "Training loss: 0.09266441315412521\n",
      "Training loss: 0.130833238363266\n",
      "Training loss: 0.1466488391160965\n",
      "Training loss: 0.1609472632408142\n",
      "Training loss: 0.16233433783054352\n",
      "Training loss: 0.18823859095573425\n",
      "Training loss: 0.13577546179294586\n",
      "Training loss: 0.09973199665546417\n",
      "Training loss: 0.14307783544063568\n",
      "Training loss: 0.18698802590370178\n",
      "Training loss: 0.11012297868728638\n",
      "Training loss: 0.10216736048460007\n",
      "Training loss: 0.14158675074577332\n",
      "Training loss: 0.09653881192207336\n",
      "Training loss: 0.1708366870880127\n",
      "Training loss: 0.13989752531051636\n",
      "Training loss: 0.1572882980108261\n",
      "Training loss: 0.1254226118326187\n",
      "Training loss: 0.11773895472288132\n",
      "Training loss: 0.12929752469062805\n",
      "Training loss: 0.12507373094558716\n",
      "Training loss: 0.08071582019329071\n",
      "Training loss: 0.08285487443208694\n",
      "Training loss: 0.13001005351543427\n",
      "Training loss: 0.05832315608859062\n",
      "Training loss: 0.1148165836930275\n",
      "Training loss: 0.11964882165193558\n",
      "Training loss: 0.18886421620845795\n",
      "Training loss: 0.12422546744346619\n",
      "Training loss: 0.1489202082157135\n",
      "Training loss: 0.11279977858066559\n",
      "Training loss: 0.12049948424100876\n",
      "Training loss: 0.1312665194272995\n",
      "Training loss: 0.11669671535491943\n",
      "Training loss: 0.10748464614152908\n",
      "Training loss: 0.15513135492801666\n",
      "Training loss: 0.13870179653167725\n",
      "Training loss: 0.1180836483836174\n",
      "Training loss: 0.19451114535331726\n",
      "Training loss: 0.07661033421754837\n",
      "Training loss: 0.13299190998077393\n",
      "Training loss: 0.19794616103172302\n",
      "Training loss: 0.18573492765426636\n",
      "Training loss: 0.0980524942278862\n",
      "Training loss: 0.13649682700634003\n",
      "Training loss: 0.1783595085144043\n",
      "Training loss: 0.1105170026421547\n",
      "Training loss: 0.08297427743673325\n",
      "Training loss: 0.13454510271549225\n",
      "Training loss: 0.11647393554449081\n",
      "Training loss: 0.17457173764705658\n",
      "Training loss: 0.1023179143667221\n",
      "Training loss: 0.2090023010969162\n",
      "Training loss: 0.12238800525665283\n",
      "Training loss: 0.1459200531244278\n",
      "Training loss: 0.09256616234779358\n",
      "Training loss: 0.13070881366729736\n",
      "Training loss: 0.14648056030273438\n",
      "Training loss: 0.16104531288146973\n",
      "Training loss: 0.16247497498989105\n",
      "Training loss: 0.18832212686538696\n",
      "Training loss: 0.1357077807188034\n",
      "Training loss: 0.0996551513671875\n",
      "Training loss: 0.1431102305650711\n",
      "Training loss: 0.18708278238773346\n",
      "Training loss: 0.11003363132476807\n",
      "Training loss: 0.10203570127487183\n",
      "Training loss: 0.14147847890853882\n",
      "Training loss: 0.09639938920736313\n",
      "Training loss: 0.17097392678260803\n",
      "Training loss: 0.13976259529590607\n",
      "Training loss: 0.15724892914295197\n",
      "Training loss: 0.12523117661476135\n",
      "Training loss: 0.11758076399564743\n",
      "Training loss: 0.12920993566513062\n",
      "Training loss: 0.12499672174453735\n",
      "Training loss: 0.08042339980602264\n",
      "Training loss: 0.0825294703245163\n",
      "Training loss: 0.12994837760925293\n",
      "Training loss: 0.0580420047044754\n",
      "Training loss: 0.11473410576581955\n",
      "Training loss: 0.11955779045820236\n",
      "Training loss: 0.18887190520763397\n",
      "Training loss: 0.1241166889667511\n",
      "Training loss: 0.1488802582025528\n",
      "Training loss: 0.11269065737724304\n",
      "Training loss: 0.12031668424606323\n",
      "Training loss: 0.13111872971057892\n",
      "Training loss: 0.11656643450260162\n",
      "Training loss: 0.10735775530338287\n",
      "Training loss: 0.15509440004825592\n",
      "Training loss: 0.13863524794578552\n",
      "Training loss: 0.11803503334522247\n",
      "Training loss: 0.19456709921360016\n",
      "Training loss: 0.07637017965316772\n",
      "Training loss: 0.13295413553714752\n",
      "Training loss: 0.19801393151283264\n",
      "Training loss: 0.1857013702392578\n",
      "Training loss: 0.09785289317369461\n",
      "Training loss: 0.13644635677337646\n",
      "Training loss: 0.17840096354484558\n",
      "Training loss: 0.11036030203104019\n",
      "Training loss: 0.08279469609260559\n",
      "Training loss: 0.13436931371688843\n",
      "Training loss: 0.11635656654834747\n",
      "Training loss: 0.17458277940750122\n",
      "Training loss: 0.1020670160651207\n",
      "Training loss: 0.20914514362812042\n",
      "Training loss: 0.12228454649448395\n",
      "Training loss: 0.14583076536655426\n",
      "Training loss: 0.09247317910194397\n",
      "Training loss: 0.13058993220329285\n",
      "Training loss: 0.14632001519203186\n",
      "Training loss: 0.1611427366733551\n",
      "Training loss: 0.1626143604516983\n",
      "Training loss: 0.18840615451335907\n",
      "Training loss: 0.13564462959766388\n",
      "Training loss: 0.09958307445049286\n",
      "Training loss: 0.1431441605091095\n",
      "Training loss: 0.18717770278453827\n",
      "Training loss: 0.10994993895292282\n",
      "Training loss: 0.10191066563129425\n",
      "Training loss: 0.14137503504753113\n",
      "Training loss: 0.09626650810241699\n",
      "Training loss: 0.17111040651798248\n",
      "Training loss: 0.13963404297828674\n",
      "Training loss: 0.15721362829208374\n",
      "Training loss: 0.12504811584949493\n",
      "Training loss: 0.11742934584617615\n",
      "Training loss: 0.12912768125534058\n",
      "Training loss: 0.12492427974939346\n",
      "Training loss: 0.08014170080423355\n",
      "Training loss: 0.08221596479415894\n",
      "Training loss: 0.12989135086536407\n",
      "Training loss: 0.0577717199921608\n",
      "Training loss: 0.11465592682361603\n",
      "Training loss: 0.11947141587734222\n",
      "Training loss: 0.18888239562511444\n",
      "Training loss: 0.12401288002729416\n",
      "Training loss: 0.14884406328201294\n",
      "Training loss: 0.11258705705404282\n",
      "Training loss: 0.12014161050319672\n",
      "Training loss: 0.13097764551639557\n",
      "Training loss: 0.11644235253334045\n",
      "Training loss: 0.10723735392093658\n",
      "Training loss: 0.15506109595298767\n",
      "Training loss: 0.13857388496398926\n",
      "Training loss: 0.11799002438783646\n",
      "Training loss: 0.19462363421916962\n",
      "Training loss: 0.07613926380872726\n",
      "Training loss: 0.13292016088962555\n",
      "Training loss: 0.19808202981948853\n",
      "Training loss: 0.185670405626297\n",
      "Training loss: 0.09766155481338501\n",
      "Training loss: 0.13639935851097107\n",
      "Training loss: 0.1784440577030182\n",
      "Training loss: 0.11021009087562561\n",
      "Training loss: 0.08262255787849426\n",
      "Training loss: 0.13420145213603973\n",
      "Training loss: 0.11624477803707123\n",
      "Training loss: 0.17459608614444733\n",
      "Training loss: 0.1018257886171341\n",
      "Training loss: 0.2092859447002411\n",
      "Training loss: 0.12218644469976425\n",
      "Training loss: 0.1457461416721344\n",
      "Training loss: 0.09238528460264206\n",
      "Training loss: 0.13047632575035095\n",
      "Training loss: 0.14616671204566956\n",
      "Training loss: 0.161239355802536\n",
      "Training loss: 0.16275233030319214\n",
      "Training loss: 0.18849049508571625\n",
      "Training loss: 0.1355857402086258\n",
      "Training loss: 0.09951542317867279\n",
      "Training loss: 0.14317944645881653\n",
      "Training loss: 0.1872725784778595\n",
      "Training loss: 0.1098715215921402\n",
      "Training loss: 0.10179196298122406\n",
      "Training loss: 0.14127610623836517\n",
      "Training loss: 0.09613984823226929\n",
      "Training loss: 0.17124587297439575\n",
      "Training loss: 0.1395116001367569\n",
      "Training loss: 0.15718205273151398\n",
      "Training loss: 0.12487304210662842\n",
      "Training loss: 0.11728443205356598\n",
      "Training loss: 0.12905049324035645\n",
      "Training loss: 0.12485617399215698\n",
      "Training loss: 0.07987033575773239\n",
      "Training loss: 0.08191388845443726\n",
      "Training loss: 0.12983867526054382\n",
      "Training loss: 0.05751186981797218\n",
      "Training loss: 0.11458178609609604\n",
      "Training loss: 0.11938939243555069\n",
      "Training loss: 0.1888953298330307\n",
      "Training loss: 0.12391383945941925\n",
      "Training loss: 0.1488112211227417\n",
      "Training loss: 0.11248869448900223\n",
      "Training loss: 0.11997390538454056\n",
      "Training loss: 0.1308429092168808\n",
      "Training loss: 0.11632415652275085\n",
      "Training loss: 0.1071229949593544\n",
      "Training loss: 0.15503115952014923\n",
      "Training loss: 0.1385170966386795\n",
      "Training loss: 0.11794840544462204\n",
      "Training loss: 0.19468076527118683\n",
      "Training loss: 0.07591714709997177\n",
      "Training loss: 0.13288965821266174\n",
      "Training loss: 0.19815035164356232\n",
      "Training loss: 0.18564215302467346\n",
      "Training loss: 0.09747807681560516\n",
      "Training loss: 0.13635565340518951\n",
      "Training loss: 0.17848850786685944\n",
      "Training loss: 0.11006611585617065\n",
      "Training loss: 0.0824575275182724\n",
      "Training loss: 0.13404110074043274\n",
      "Training loss: 0.1161382719874382\n",
      "Training loss: 0.1746113896369934\n",
      "Training loss: 0.10159385949373245\n",
      "Training loss: 0.2094244509935379\n",
      "Training loss: 0.12209352850914001\n",
      "Training loss: 0.14566601812839508\n",
      "Training loss: 0.0923021212220192\n",
      "Training loss: 0.130367711186409\n",
      "Training loss: 0.14602047204971313\n",
      "Training loss: 0.1613350212574005\n",
      "Training loss: 0.16288869082927704\n",
      "Training loss: 0.18857495486736298\n",
      "Training loss: 0.13553078472614288\n",
      "Training loss: 0.0994519591331482\n",
      "Training loss: 0.1432158648967743\n",
      "Training loss: 0.18736718595027924\n",
      "Training loss: 0.10979818552732468\n",
      "Training loss: 0.1016792356967926\n",
      "Training loss: 0.14118143916130066\n",
      "Training loss: 0.09601911902427673\n",
      "Training loss: 0.17138034105300903\n",
      "Training loss: 0.1393948793411255\n",
      "Training loss: 0.15715397894382477\n",
      "Training loss: 0.12470565736293793\n",
      "Training loss: 0.11714562028646469\n",
      "Training loss: 0.1289781630039215\n",
      "Training loss: 0.12479209154844284\n",
      "Training loss: 0.0796089693903923\n",
      "Training loss: 0.08162286877632141\n",
      "Training loss: 0.12979009747505188\n",
      "Training loss: 0.05726218596100807\n",
      "Training loss: 0.1145113855600357\n",
      "Training loss: 0.11931145936250687\n",
      "Training loss: 0.1889106184244156\n",
      "Training loss: 0.12381928414106369\n",
      "Training loss: 0.14878153800964355\n",
      "Training loss: 0.1123952865600586\n",
      "Training loss: 0.11981327831745148\n",
      "Training loss: 0.1307142674922943\n",
      "Training loss: 0.11621153354644775\n",
      "Training loss: 0.10701445490121841\n",
      "Training loss: 0.15500442683696747\n",
      "Training loss: 0.13846451044082642\n",
      "Training loss: 0.11790992319583893\n",
      "Training loss: 0.19473834335803986\n",
      "Training loss: 0.07570340484380722\n",
      "Training loss: 0.13286234438419342\n",
      "Training loss: 0.19821889698505402\n",
      "Training loss: 0.1856163889169693\n",
      "Training loss: 0.09730211645364761\n",
      "Training loss: 0.1363150179386139\n",
      "Training loss: 0.17853409051895142\n",
      "Training loss: 0.10992808640003204\n",
      "Training loss: 0.08229926973581314\n",
      "Training loss: 0.13388793170452118\n",
      "Training loss: 0.1160367876291275\n",
      "Training loss: 0.17462845146656036\n",
      "Training loss: 0.1013709008693695\n",
      "Training loss: 0.2095605731010437\n",
      "Training loss: 0.12200545519590378\n",
      "Training loss: 0.14559021592140198\n",
      "Training loss: 0.09222333133220673\n",
      "Training loss: 0.13026385009288788\n",
      "Training loss: 0.14588096737861633\n",
      "Training loss: 0.1614294946193695\n",
      "Training loss: 0.16302327811717987\n",
      "Training loss: 0.18865936994552612\n",
      "Training loss: 0.13547948002815247\n",
      "Training loss: 0.09939248859882355\n",
      "Training loss: 0.1432531774044037\n",
      "Training loss: 0.1874612271785736\n",
      "Training loss: 0.10972941666841507\n",
      "Training loss: 0.10157200694084167\n",
      "Training loss: 0.14109109342098236\n",
      "Training loss: 0.09590400755405426\n",
      "Training loss: 0.1715133786201477\n",
      "Training loss: 0.1392837017774582\n",
      "Training loss: 0.15712907910346985\n",
      "Training loss: 0.12454560399055481\n",
      "Training loss: 0.1170128583908081\n",
      "Training loss: 0.1289101541042328\n",
      "Training loss: 0.12473184615373611\n",
      "Training loss: 0.07935719192028046\n",
      "Training loss: 0.08134253323078156\n",
      "Training loss: 0.12974530458450317\n",
      "Training loss: 0.057022154331207275\n",
      "Training loss: 0.11444490402936935\n",
      "Training loss: 0.11923754960298538\n",
      "Training loss: 0.18892768025398254\n",
      "Training loss: 0.12372909486293793\n",
      "Training loss: 0.14875485002994537\n",
      "Training loss: 0.11230659484863281\n",
      "Training loss: 0.1196594312787056\n",
      "Training loss: 0.1305914670228958\n",
      "Training loss: 0.11610424518585205\n",
      "Training loss: 0.10691136866807938\n",
      "Training loss: 0.1549805849790573\n",
      "Training loss: 0.13841590285301208\n",
      "Training loss: 0.11787434667348862\n",
      "Training loss: 0.19479620456695557\n",
      "Training loss: 0.0754978358745575\n",
      "Training loss: 0.1328379511833191\n",
      "Training loss: 0.1982872635126114\n",
      "Training loss: 0.1855929046869278\n",
      "Training loss: 0.09713336080312729\n",
      "Training loss: 0.13627730309963226\n",
      "Training loss: 0.17858058214187622\n",
      "Training loss: 0.10979586839675903\n",
      "Training loss: 0.08214761316776276\n",
      "Training loss: 0.13374152779579163\n",
      "Training loss: 0.11594009399414062\n",
      "Training loss: 0.1746470034122467\n",
      "Training loss: 0.10115672647953033\n",
      "Training loss: 0.20969435572624207\n",
      "Training loss: 0.12192201614379883\n",
      "Training loss: 0.1455184370279312\n",
      "Training loss: 0.09214877337217331\n",
      "Training loss: 0.13016465306282043\n",
      "Training loss: 0.14574787020683289\n",
      "Training loss: 0.161522775888443\n",
      "Training loss: 0.1631559282541275\n",
      "Training loss: 0.1887434422969818\n",
      "Training loss: 0.1354316920042038\n",
      "Training loss: 0.09933672100305557\n",
      "Training loss: 0.14329123497009277\n",
      "Training loss: 0.18755453824996948\n",
      "Training loss: 0.1096649020910263\n",
      "Training loss: 0.10147007554769516\n",
      "Training loss: 0.14100487530231476\n",
      "Training loss: 0.09579426795244217\n",
      "Training loss: 0.17164486646652222\n",
      "Training loss: 0.13917776942253113\n",
      "Training loss: 0.1571071296930313\n",
      "Training loss: 0.12439258396625519\n",
      "Training loss: 0.11688578128814697\n",
      "Training loss: 0.12884631752967834\n",
      "Training loss: 0.1246752217411995\n",
      "Training loss: 0.07911475747823715\n",
      "Training loss: 0.08107256889343262\n",
      "Training loss: 0.12970402836799622\n",
      "Training loss: 0.05679149553179741\n",
      "Training loss: 0.11438185721635818\n",
      "Training loss: 0.11916738748550415\n",
      "Training loss: 0.18894647061824799\n",
      "Training loss: 0.12364303320646286\n",
      "Training loss: 0.14873087406158447\n",
      "Training loss: 0.11222237348556519\n",
      "Training loss: 0.11951209604740143\n",
      "Training loss: 0.13047422468662262\n",
      "Training loss: 0.11600205302238464\n",
      "Training loss: 0.10681353509426117\n",
      "Training loss: 0.15495949983596802\n",
      "Training loss: 0.13837110996246338\n",
      "Training loss: 0.11784154921770096\n",
      "Training loss: 0.19485393166542053\n",
      "Training loss: 0.0753001868724823\n",
      "Training loss: 0.1328163743019104\n",
      "Training loss: 0.19835533201694489\n",
      "Training loss: 0.18557144701480865\n",
      "Training loss: 0.09697164595127106\n",
      "Training loss: 0.13624227046966553\n",
      "Training loss: 0.17862781882286072\n",
      "Training loss: 0.10966910421848297\n",
      "Training loss: 0.082002192735672\n",
      "Training loss: 0.13360172510147095\n",
      "Training loss: 0.11584795266389847\n",
      "Training loss: 0.17466695606708527\n",
      "Training loss: 0.10095082223415375\n",
      "Training loss: 0.2098255455493927\n",
      "Training loss: 0.12184294313192368\n",
      "Training loss: 0.14545059204101562\n",
      "Training loss: 0.09207811951637268\n",
      "Training loss: 0.13006983697414398\n",
      "Training loss: 0.14562095701694489\n",
      "Training loss: 0.16161470115184784\n",
      "Training loss: 0.16328653693199158\n",
      "Training loss: 0.18882717192173004\n",
      "Training loss: 0.13538706302642822\n",
      "Training loss: 0.09928444027900696\n",
      "Training loss: 0.14332979917526245\n",
      "Training loss: 0.18764697015285492\n",
      "Training loss: 0.10960439592599869\n",
      "Training loss: 0.10137306153774261\n",
      "Training loss: 0.14092260599136353\n",
      "Training loss: 0.09568959474563599\n",
      "Training loss: 0.1717747300863266\n",
      "Training loss: 0.139076828956604\n",
      "Training loss: 0.15708790719509125\n",
      "Training loss: 0.1242462620139122\n",
      "Training loss: 0.11676420271396637\n",
      "Training loss: 0.1287863403558731\n",
      "Training loss: 0.12462195754051208\n",
      "Training loss: 0.078881174325943\n",
      "Training loss: 0.08081245422363281\n",
      "Training loss: 0.1296660304069519\n",
      "Training loss: 0.056569792330265045\n",
      "Training loss: 0.11432213336229324\n",
      "Training loss: 0.11910070478916168\n",
      "Training loss: 0.1889667510986328\n",
      "Training loss: 0.1235608384013176\n",
      "Training loss: 0.14870929718017578\n",
      "Training loss: 0.11214232444763184\n",
      "Training loss: 0.11937098950147629\n",
      "Training loss: 0.13036230206489563\n",
      "Training loss: 0.11590460687875748\n",
      "Training loss: 0.1067207008600235\n",
      "Training loss: 0.15494100749492645\n",
      "Training loss: 0.13832977414131165\n",
      "Training loss: 0.11781133711338043\n",
      "Training loss: 0.1949116736650467\n",
      "Training loss: 0.07511001080274582\n",
      "Training loss: 0.13279742002487183\n",
      "Training loss: 0.1984231173992157\n",
      "Training loss: 0.1855519413948059\n",
      "Training loss: 0.0968165472149849\n",
      "Training loss: 0.1362096071243286\n",
      "Training loss: 0.178675577044487\n",
      "Training loss: 0.10954748094081879\n",
      "Training loss: 0.08186262845993042\n",
      "Training loss: 0.13346822559833527\n",
      "Training loss: 0.11576009541749954\n",
      "Training loss: 0.1746882200241089\n",
      "Training loss: 0.10075271874666214\n",
      "Training loss: 0.20995402336120605\n",
      "Training loss: 0.12176796793937683\n",
      "Training loss: 0.145386204123497\n",
      "Training loss: 0.09201128035783768\n",
      "Training loss: 0.12997904419898987\n",
      "Training loss: 0.14549978077411652\n",
      "Training loss: 0.16170521080493927\n",
      "Training loss: 0.16341502964496613\n",
      "Training loss: 0.18891029059886932\n",
      "Training loss: 0.135345458984375\n",
      "Training loss: 0.0992354154586792\n",
      "Training loss: 0.1433688998222351\n",
      "Training loss: 0.18773861229419708\n",
      "Training loss: 0.10954798758029938\n",
      "Training loss: 0.10128099471330643\n",
      "Training loss: 0.14084358513355255\n",
      "Training loss: 0.095589779317379\n",
      "Training loss: 0.17190298438072205\n",
      "Training loss: 0.13898055255413055\n",
      "Training loss: 0.15707111358642578\n",
      "Training loss: 0.1241062730550766\n",
      "Training loss: 0.1166476383805275\n",
      "Training loss: 0.1287301778793335\n",
      "Training loss: 0.12457191944122314\n",
      "Training loss: 0.07865624129772186\n",
      "Training loss: 0.08056189119815826\n",
      "Training loss: 0.12963107228279114\n",
      "Training loss: 0.056356653571128845\n",
      "Training loss: 0.1142655536532402\n",
      "Training loss: 0.11903741955757141\n",
      "Training loss: 0.18898829817771912\n",
      "Training loss: 0.12348241358995438\n",
      "Training loss: 0.14869017899036407\n",
      "Training loss: 0.11206631362438202\n",
      "Training loss: 0.11923573911190033\n",
      "Training loss: 0.1302553415298462\n",
      "Training loss: 0.11581170558929443\n",
      "Training loss: 0.10663241893053055\n",
      "Training loss: 0.15492479503154755\n",
      "Training loss: 0.13829146325588226\n",
      "Training loss: 0.11778344959020615\n",
      "Training loss: 0.19496923685073853\n",
      "Training loss: 0.0749269425868988\n",
      "Training loss: 0.1327807754278183\n",
      "Training loss: 0.19849041104316711\n",
      "Training loss: 0.18553416430950165\n",
      "Training loss: 0.09666776657104492\n",
      "Training loss: 0.13617931306362152\n",
      "Training loss: 0.17872373759746552\n",
      "Training loss: 0.10943087190389633\n",
      "Training loss: 0.08172875642776489\n",
      "Training loss: 0.13334056735038757\n",
      "Training loss: 0.11567629873752594\n",
      "Training loss: 0.1747104823589325\n",
      "Training loss: 0.10056223720312119\n",
      "Training loss: 0.21007995307445526\n",
      "Training loss: 0.12169686704874039\n",
      "Training loss: 0.1453252136707306\n",
      "Training loss: 0.09194794297218323\n",
      "Training loss: 0.12989220023155212\n",
      "Training loss: 0.1453840732574463\n",
      "Training loss: 0.1617942601442337\n",
      "Training loss: 0.16354133188724518\n",
      "Training loss: 0.18899273872375488\n",
      "Training loss: 0.13530667126178741\n",
      "Training loss: 0.09918945282697678\n",
      "Training loss: 0.14340831339359283\n",
      "Training loss: 0.18782919645309448\n",
      "Training loss: 0.10949511080980301\n",
      "Training loss: 0.10119342058897018\n",
      "Training loss: 0.14076806604862213\n",
      "Training loss: 0.09549456834793091\n",
      "Training loss: 0.17202933132648468\n",
      "Training loss: 0.13888873159885406\n",
      "Training loss: 0.15705667436122894\n",
      "Training loss: 0.12397238612174988\n",
      "Training loss: 0.11653600633144379\n",
      "Training loss: 0.12867756187915802\n",
      "Training loss: 0.12452483177185059\n",
      "Training loss: 0.07843958586454391\n",
      "Training loss: 0.08032052963972092\n",
      "Training loss: 0.1295989751815796\n",
      "Training loss: 0.0561518669128418\n",
      "Training loss: 0.11421186476945877\n",
      "Training loss: 0.11897720396518707\n",
      "Training loss: 0.18901103734970093\n",
      "Training loss: 0.1234075129032135\n",
      "Training loss: 0.14867308735847473\n",
      "Training loss: 0.11199408024549484\n",
      "Training loss: 0.11910618841648102\n",
      "Training loss: 0.13015322387218475\n",
      "Training loss: 0.11572317779064178\n",
      "Training loss: 0.10654860734939575\n",
      "Training loss: 0.15491071343421936\n",
      "Training loss: 0.13825635612010956\n",
      "Training loss: 0.11775779724121094\n",
      "Training loss: 0.1950264722108841\n",
      "Training loss: 0.07475095242261887\n",
      "Training loss: 0.13276632130146027\n",
      "Training loss: 0.19855716824531555\n",
      "Training loss: 0.18551793694496155\n",
      "Training loss: 0.09652510285377502\n",
      "Training loss: 0.13615116477012634\n",
      "Training loss: 0.17877216637134552\n",
      "Training loss: 0.1093190610408783\n",
      "Training loss: 0.0816003680229187\n",
      "Training loss: 0.13321861624717712\n",
      "Training loss: 0.11559642851352692\n",
      "Training loss: 0.17473366856575012\n",
      "Training loss: 0.10037914663553238\n",
      "Training loss: 0.2102031409740448\n",
      "Training loss: 0.12162946909666061\n",
      "Training loss: 0.14526748657226562\n",
      "Training loss: 0.09188796579837799\n",
      "Training loss: 0.12980912625789642\n",
      "Training loss: 0.14527368545532227\n",
      "Training loss: 0.1618817150592804\n",
      "Training loss: 0.1636652946472168\n",
      "Training loss: 0.18907438218593597\n",
      "Training loss: 0.13527053594589233\n",
      "Training loss: 0.09914636611938477\n",
      "Training loss: 0.1434479057788849\n",
      "Training loss: 0.18791860342025757\n",
      "Training loss: 0.10944566130638123\n",
      "Training loss: 0.10111011564731598\n",
      "Training loss: 0.14069581031799316\n",
      "Training loss: 0.095403753221035\n",
      "Training loss: 0.17215381562709808\n",
      "Training loss: 0.1388012021780014\n",
      "Training loss: 0.15704435110092163\n",
      "Training loss: 0.12384434044361115\n",
      "Training loss: 0.11642907559871674\n",
      "Training loss: 0.12862828373908997\n",
      "Training loss: 0.12448058277368546\n",
      "Training loss: 0.07823098450899124\n",
      "Training loss: 0.08008813112974167\n",
      "Training loss: 0.12956953048706055\n",
      "Training loss: 0.05595514923334122\n",
      "Training loss: 0.11416085809469223\n",
      "Training loss: 0.11891996115446091\n",
      "Training loss: 0.1890348643064499\n",
      "Training loss: 0.12333595007658005\n",
      "Training loss: 0.148657888174057\n",
      "Training loss: 0.11192548274993896\n",
      "Training loss: 0.11898218095302582\n",
      "Training loss: 0.1300557404756546\n",
      "Training loss: 0.11563877761363983\n",
      "Training loss: 0.10646913945674896\n",
      "Training loss: 0.15489868819713593\n",
      "Training loss: 0.13822409510612488\n",
      "Training loss: 0.11773426085710526\n",
      "Training loss: 0.1950831115245819\n",
      "Training loss: 0.07458174228668213\n",
      "Training loss: 0.13275392353534698\n",
      "Training loss: 0.19862310588359833\n",
      "Training loss: 0.18550334870815277\n",
      "Training loss: 0.09638839215040207\n",
      "Training loss: 0.13612502813339233\n",
      "Training loss: 0.1788206249475479\n",
      "Training loss: 0.10921188443899155\n",
      "Training loss: 0.08147723227739334\n",
      "Training loss: 0.13310211896896362\n",
      "Training loss: 0.11552029848098755\n",
      "Training loss: 0.17475755512714386\n",
      "Training loss: 0.10020323097705841\n",
      "Training loss: 0.2103235125541687\n",
      "Training loss: 0.12156562507152557\n",
      "Training loss: 0.14521288871765137\n",
      "Training loss: 0.09183106571435928\n",
      "Training loss: 0.12972970306873322\n",
      "Training loss: 0.1451684683561325\n",
      "Training loss: 0.1619674265384674\n",
      "Training loss: 0.16378682851791382\n",
      "Training loss: 0.18915510177612305\n",
      "Training loss: 0.1352367401123047\n",
      "Training loss: 0.09910605102777481\n",
      "Training loss: 0.14348751306533813\n",
      "Training loss: 0.18800665438175201\n",
      "Training loss: 0.10939928889274597\n",
      "Training loss: 0.10103076696395874\n",
      "Training loss: 0.14062689244747162\n",
      "Training loss: 0.09531709551811218\n",
      "Training loss: 0.17227616906166077\n",
      "Training loss: 0.13871777057647705\n",
      "Training loss: 0.15703390538692474\n",
      "Training loss: 0.1237219050526619\n",
      "Training loss: 0.116326704621315\n",
      "Training loss: 0.1285819411277771\n",
      "Training loss: 0.12443898618221283\n",
      "Training loss: 0.07803007960319519\n",
      "Training loss: 0.07986427843570709\n",
      "Training loss: 0.1295425146818161\n",
      "Training loss: 0.05576608330011368\n",
      "Training loss: 0.11411271244287491\n",
      "Training loss: 0.11886560171842575\n",
      "Training loss: 0.18905934691429138\n",
      "Training loss: 0.12326762825250626\n",
      "Training loss: 0.14864449203014374\n",
      "Training loss: 0.11186027526855469\n",
      "Training loss: 0.11886337399482727\n",
      "Training loss: 0.12996265292167664\n",
      "Training loss: 0.11555831134319305\n",
      "Training loss: 0.10639368742704391\n",
      "Training loss: 0.1548885703086853\n",
      "Training loss: 0.13819438219070435\n",
      "Training loss: 0.11771267652511597\n",
      "Training loss: 0.19513913989067078\n",
      "Training loss: 0.07441893219947815\n",
      "Training loss: 0.1327434629201889\n",
      "Training loss: 0.1986883580684662\n",
      "Training loss: 0.18548986315727234\n",
      "Training loss: 0.0962572991847992\n",
      "Training loss: 0.1361006498336792\n",
      "Training loss: 0.1788690835237503\n",
      "Training loss: 0.10910897701978683\n",
      "Training loss: 0.08135897666215897\n",
      "Training loss: 0.13299082219600677\n",
      "Training loss: 0.11544764041900635\n",
      "Training loss: 0.17478221654891968\n",
      "Training loss: 0.10003385692834854\n",
      "Training loss: 0.21044102311134338\n",
      "Training loss: 0.12150503695011139\n",
      "Training loss: 0.14516103267669678\n",
      "Training loss: 0.09177720546722412\n",
      "Training loss: 0.12965355813503265\n",
      "Training loss: 0.14506793022155762\n",
      "Training loss: 0.16205158829689026\n",
      "Training loss: 0.1639060080051422\n",
      "Training loss: 0.18923479318618774\n",
      "Training loss: 0.13520537316799164\n",
      "Training loss: 0.09906820207834244\n",
      "Training loss: 0.14352715015411377\n",
      "Training loss: 0.18809351325035095\n",
      "Training loss: 0.10935620218515396\n",
      "Training loss: 0.10095547139644623\n",
      "Training loss: 0.14056050777435303\n",
      "Training loss: 0.09523442387580872\n",
      "Training loss: 0.17239657044410706\n",
      "Training loss: 0.13863815367221832\n",
      "Training loss: 0.15702521800994873\n",
      "Training loss: 0.12360471487045288\n",
      "Training loss: 0.11622846126556396\n",
      "Training loss: 0.1285388022661209\n",
      "Training loss: 0.12439989298582077\n",
      "Training loss: 0.07783665508031845\n",
      "Training loss: 0.0796487033367157\n",
      "Training loss: 0.12951777875423431\n",
      "Training loss: 0.05558445677161217\n",
      "Training loss: 0.11406677961349487\n",
      "Training loss: 0.11881382763385773\n",
      "Training loss: 0.18908463418483734\n",
      "Training loss: 0.1232023611664772\n",
      "Training loss: 0.1486327052116394\n",
      "Training loss: 0.11179831624031067\n",
      "Training loss: 0.11874951422214508\n",
      "Training loss: 0.12987370789051056\n",
      "Training loss: 0.11548156291246414\n",
      "Training loss: 0.10632191598415375\n",
      "Training loss: 0.15488003194332123\n",
      "Training loss: 0.13816705346107483\n",
      "Training loss: 0.11769281327724457\n",
      "Training loss: 0.1951947808265686\n",
      "Training loss: 0.07426227629184723\n",
      "Training loss: 0.13273464143276215\n",
      "Training loss: 0.19875270128250122\n",
      "Training loss: 0.1854778528213501\n",
      "Training loss: 0.09613150358200073\n",
      "Training loss: 0.13607807457447052\n",
      "Training loss: 0.17891739308834076\n",
      "Training loss: 0.10901033878326416\n",
      "Training loss: 0.081245556473732\n",
      "Training loss: 0.13288439810276031\n",
      "Training loss: 0.11537833511829376\n",
      "Training loss: 0.17480731010437012\n",
      "Training loss: 0.09987113624811172\n",
      "Training loss: 0.2105558216571808\n",
      "Training loss: 0.1214476078748703\n",
      "Training loss: 0.14511197805404663\n",
      "Training loss: 0.09172610938549042\n",
      "Training loss: 0.12958072125911713\n",
      "Training loss: 0.14497201144695282\n",
      "Training loss: 0.16213390231132507\n",
      "Training loss: 0.16402263939380646\n",
      "Training loss: 0.18931342661380768\n",
      "Training loss: 0.13517600297927856\n",
      "Training loss: 0.09903279691934586\n",
      "Training loss: 0.14356662333011627\n",
      "Training loss: 0.18817880749702454\n",
      "Training loss: 0.10931573063135147\n",
      "Training loss: 0.10088367760181427\n",
      "Training loss: 0.14049723744392395\n",
      "Training loss: 0.0951555147767067\n",
      "Training loss: 0.17251472175121307\n",
      "Training loss: 0.13856221735477448\n",
      "Training loss: 0.15701811015605927\n",
      "Training loss: 0.12349268794059753\n",
      "Training loss: 0.11613448709249496\n",
      "Training loss: 0.1284981667995453\n",
      "Training loss: 0.12436313182115555\n",
      "Training loss: 0.07765042036771774\n",
      "Training loss: 0.07944117486476898\n",
      "Training loss: 0.12949512898921967\n",
      "Training loss: 0.05540994182229042\n",
      "Training loss: 0.11402346193790436\n",
      "Training loss: 0.1187647357583046\n",
      "Training loss: 0.18911033868789673\n",
      "Training loss: 0.12314005941152573\n",
      "Training loss: 0.1486225426197052\n",
      "Training loss: 0.11173944920301437\n",
      "Training loss: 0.11864049732685089\n",
      "Training loss: 0.12978880107402802\n",
      "Training loss: 0.11540838330984116\n",
      "Training loss: 0.10625381767749786\n",
      "Training loss: 0.15487311780452728\n",
      "Training loss: 0.13814187049865723\n",
      "Training loss: 0.11767463386058807\n",
      "Training loss: 0.19524957239627838\n",
      "Training loss: 0.07411165535449982\n",
      "Training loss: 0.13272735476493835\n",
      "Training loss: 0.19881613552570343\n",
      "Training loss: 0.18546722829341888\n",
      "Training loss: 0.09601090103387833\n",
      "Training loss: 0.13605724275112152\n",
      "Training loss: 0.17896543443202972\n",
      "Training loss: 0.10891587287187576\n",
      "Training loss: 0.08113687485456467\n",
      "Training loss: 0.13278263807296753\n",
      "Training loss: 0.11531225591897964\n",
      "Training loss: 0.17483271658420563\n",
      "Training loss: 0.09971493482589722\n",
      "Training loss: 0.21066784858703613\n",
      "Training loss: 0.12139318138360977\n",
      "Training loss: 0.14506560564041138\n",
      "Training loss: 0.09167756140232086\n",
      "Training loss: 0.12951114773750305\n",
      "Training loss: 0.14488062262535095\n",
      "Training loss: 0.1622144877910614\n",
      "Training loss: 0.16413669288158417\n",
      "Training loss: 0.18939077854156494\n",
      "Training loss: 0.13514865934848785\n",
      "Training loss: 0.09899961948394775\n",
      "Training loss: 0.14360582828521729\n",
      "Training loss: 0.18826250731945038\n",
      "Training loss: 0.10927771031856537\n",
      "Training loss: 0.10081522911787033\n",
      "Training loss: 0.14043693244457245\n",
      "Training loss: 0.09508021920919418\n",
      "Training loss: 0.1726304590702057\n",
      "Training loss: 0.13848982751369476\n",
      "Training loss: 0.15701249241828918\n",
      "Training loss: 0.12338556349277496\n",
      "Training loss: 0.11604456603527069\n",
      "Training loss: 0.12845994532108307\n",
      "Training loss: 0.1243285983800888\n",
      "Training loss: 0.07747109234333038\n",
      "Training loss: 0.07924138009548187\n",
      "Training loss: 0.1294744312763214\n",
      "Training loss: 0.05524223670363426\n",
      "Training loss: 0.11398249864578247\n",
      "Training loss: 0.11871806532144547\n",
      "Training loss: 0.1891363561153412\n",
      "Training loss: 0.1230805367231369\n",
      "Training loss: 0.14861370623111725\n",
      "Training loss: 0.1116834431886673\n",
      "Training loss: 0.11853605508804321\n",
      "Training loss: 0.1297077238559723\n",
      "Training loss: 0.1153385266661644\n",
      "Training loss: 0.10618907958269119\n",
      "Training loss: 0.15486769378185272\n",
      "Training loss: 0.13811857998371124\n",
      "Training loss: 0.11765798926353455\n",
      "Training loss: 0.19530372321605682\n",
      "Training loss: 0.07396664470434189\n",
      "Training loss: 0.13272152841091156\n",
      "Training loss: 0.1988787055015564\n",
      "Training loss: 0.18545742332935333\n",
      "Training loss: 0.09589522331953049\n",
      "Training loss: 0.13603778183460236\n",
      "Training loss: 0.17901311814785004\n",
      "Training loss: 0.10882510244846344\n",
      "Training loss: 0.08103238046169281\n",
      "Training loss: 0.13268539309501648\n",
      "Training loss: 0.1152491495013237\n",
      "Training loss: 0.17485857009887695\n",
      "Training loss: 0.09956445544958115\n",
      "Training loss: 0.2107769250869751\n",
      "Training loss: 0.1213415265083313\n",
      "Training loss: 0.14502160251140594\n",
      "Training loss: 0.09163153916597366\n",
      "Training loss: 0.12944439053535461\n",
      "Training loss: 0.144793301820755\n",
      "Training loss: 0.1622932404279709\n",
      "Training loss: 0.1642482578754425\n",
      "Training loss: 0.18946701288223267\n",
      "Training loss: 0.1351231038570404\n",
      "Training loss: 0.09896855801343918\n",
      "Training loss: 0.1436447948217392\n",
      "Training loss: 0.1883447766304016\n",
      "Training loss: 0.10924239456653595\n",
      "Training loss: 0.10075020045042038\n",
      "Training loss: 0.14037878811359406\n",
      "Training loss: 0.09500835090875626\n",
      "Training loss: 0.17274421453475952\n",
      "Training loss: 0.1384207159280777\n",
      "Training loss: 0.15700815618038177\n",
      "Training loss: 0.1232830137014389\n",
      "Training loss: 0.11595817655324936\n",
      "Training loss: 0.12842443585395813\n",
      "Training loss: 0.12429609894752502\n",
      "Training loss: 0.07729847729206085\n",
      "Training loss: 0.07904896140098572\n",
      "Training loss: 0.1294555813074112\n",
      "Training loss: 0.055081259459257126\n",
      "Training loss: 0.11394333839416504\n",
      "Training loss: 0.11867353320121765\n",
      "Training loss: 0.18916290998458862\n",
      "Training loss: 0.1230236291885376\n",
      "Training loss: 0.14860600233078003\n",
      "Training loss: 0.11163024604320526\n",
      "Training loss: 0.11843601614236832\n",
      "Training loss: 0.12963026762008667\n",
      "Training loss: 0.11527195572853088\n",
      "Training loss: 0.10612766444683075\n",
      "Training loss: 0.15486349165439606\n",
      "Training loss: 0.13809755444526672\n",
      "Training loss: 0.11764280498027802\n",
      "Training loss: 0.1953568160533905\n",
      "Training loss: 0.0738273561000824\n",
      "Training loss: 0.1327170729637146\n",
      "Training loss: 0.19894006848335266\n",
      "Training loss: 0.1854485273361206\n",
      "Training loss: 0.09578434377908707\n",
      "Training loss: 0.1360197365283966\n",
      "Training loss: 0.17906036972999573\n",
      "Training loss: 0.10873807966709137\n",
      "Training loss: 0.08093220740556717\n",
      "Training loss: 0.13259245455265045\n",
      "Training loss: 0.11518895626068115\n",
      "Training loss: 0.17488455772399902\n",
      "Training loss: 0.09942000359296799\n",
      "Training loss: 0.2108832746744156\n",
      "Training loss: 0.12129256874322891\n",
      "Training loss: 0.14497996866703033\n",
      "Training loss: 0.0915878489613533\n",
      "Training loss: 0.12938052415847778\n",
      "Training loss: 0.1447100043296814\n",
      "Training loss: 0.16237014532089233\n",
      "Training loss: 0.16435721516609192\n",
      "Training loss: 0.18954187631607056\n",
      "Training loss: 0.13509929180145264\n",
      "Training loss: 0.09893947839736938\n",
      "Training loss: 0.14368335902690887\n",
      "Training loss: 0.18842534720897675\n",
      "Training loss: 0.10920922458171844\n",
      "Training loss: 0.10068812966346741\n",
      "Training loss: 0.1403234899044037\n",
      "Training loss: 0.09493972361087799\n",
      "Training loss: 0.1728554666042328\n",
      "Training loss: 0.1383548080921173\n",
      "Training loss: 0.15700502693653107\n",
      "Training loss: 0.12318497896194458\n",
      "Training loss: 0.11587563157081604\n",
      "Training loss: 0.12839087843894958\n",
      "Training loss: 0.12426561117172241\n",
      "Training loss: 0.07713228464126587\n",
      "Training loss: 0.07886377722024918\n",
      "Training loss: 0.12943829596042633\n",
      "Training loss: 0.054926514625549316\n",
      "Training loss: 0.11390659958124161\n",
      "Training loss: 0.11863132566213608\n",
      "Training loss: 0.18918941915035248\n",
      "Training loss: 0.12296931445598602\n",
      "Training loss: 0.14859959483146667\n",
      "Training loss: 0.11157964915037155\n",
      "Training loss: 0.11834019422531128\n",
      "Training loss: 0.1295563131570816\n",
      "Training loss: 0.11520836502313614\n",
      "Training loss: 0.10606928169727325\n",
      "Training loss: 0.15486061573028564\n",
      "Training loss: 0.1380779892206192\n",
      "Training loss: 0.11762892454862595\n",
      "Training loss: 0.19540920853614807\n",
      "Training loss: 0.07369328290224075\n",
      "Training loss: 0.13271379470825195\n",
      "Training loss: 0.19900047779083252\n",
      "Training loss: 0.18544073402881622\n",
      "Training loss: 0.09567797929048538\n",
      "Training loss: 0.13600300252437592\n",
      "Training loss: 0.17910711467266083\n",
      "Training loss: 0.10865465551614761\n",
      "Training loss: 0.08083608001470566\n",
      "Training loss: 0.13250359892845154\n",
      "Training loss: 0.11513152718544006\n",
      "Training loss: 0.17491067945957184\n",
      "Training loss: 0.09928111732006073\n",
      "Training loss: 0.21098674833774567\n",
      "Training loss: 0.12124614417552948\n",
      "Training loss: 0.14494051039218903\n",
      "Training loss: 0.09154638648033142\n",
      "Training loss: 0.12931941449642181\n",
      "Training loss: 0.1446305513381958\n",
      "Training loss: 0.16244524717330933\n",
      "Training loss: 0.16446352005004883\n",
      "Training loss: 0.18961535394191742\n",
      "Training loss: 0.1350770890712738\n",
      "Training loss: 0.09891220182180405\n",
      "Training loss: 0.14372150599956512\n",
      "Training loss: 0.18850427865982056\n",
      "Training loss: 0.10917828977108002\n",
      "Training loss: 0.10062915086746216\n",
      "Training loss: 0.14027032256126404\n",
      "Training loss: 0.09487424045801163\n",
      "Training loss: 0.17296439409255981\n",
      "Training loss: 0.1382918655872345\n",
      "Training loss: 0.15700307488441467\n",
      "Training loss: 0.12309116125106812\n",
      "Training loss: 0.1157962754368782\n",
      "Training loss: 0.12835979461669922\n",
      "Training loss: 0.12423691898584366\n",
      "Training loss: 0.0769723579287529\n",
      "Training loss: 0.07868549227714539\n",
      "Training loss: 0.12942267954349518\n",
      "Training loss: 0.05477798357605934\n",
      "Training loss: 0.1138714849948883\n",
      "Training loss: 0.1185910701751709\n",
      "Training loss: 0.1892162263393402\n",
      "Training loss: 0.12291737645864487\n",
      "Training loss: 0.14859415590763092\n",
      "Training loss: 0.111531563103199\n",
      "Training loss: 0.11824840307235718\n",
      "Training loss: 0.12948568165302277\n",
      "Training loss: 0.11514770984649658\n",
      "Training loss: 0.10601381957530975\n",
      "Training loss: 0.15485873818397522\n",
      "Training loss: 0.13806016743183136\n",
      "Training loss: 0.11761630326509476\n",
      "Training loss: 0.19546067714691162\n",
      "Training loss: 0.07356435060501099\n",
      "Training loss: 0.13271164894104004\n",
      "Training loss: 0.19905975461006165\n",
      "Training loss: 0.18543362617492676\n",
      "Training loss: 0.09557601064443588\n",
      "Training loss: 0.13598746061325073\n",
      "Training loss: 0.17915324866771698\n",
      "Training loss: 0.10857459157705307\n",
      "Training loss: 0.08074378222227097\n",
      "Training loss: 0.13241860270500183\n",
      "Training loss: 0.11507667601108551\n",
      "Training loss: 0.17493686079978943\n",
      "Training loss: 0.09914753586053848\n",
      "Training loss: 0.2110873907804489\n",
      "Training loss: 0.12120205909013748\n",
      "Training loss: 0.14490309357643127\n",
      "Training loss: 0.09150703251361847\n",
      "Training loss: 0.1292608082294464\n",
      "Training loss: 0.14455467462539673\n",
      "Training loss: 0.1625184267759323\n",
      "Training loss: 0.16456730663776398\n",
      "Training loss: 0.18968747556209564\n",
      "Training loss: 0.13505633175373077\n",
      "Training loss: 0.09888672083616257\n",
      "Training loss: 0.14375919103622437\n",
      "Training loss: 0.18858155608177185\n",
      "Training loss: 0.10914938896894455\n",
      "Training loss: 0.10057283937931061\n",
      "Training loss: 0.14021942019462585\n",
      "Training loss: 0.09481168538331985\n",
      "Training loss: 0.17307105660438538\n",
      "Training loss: 0.1382318139076233\n",
      "Training loss: 0.1570020467042923\n",
      "Training loss: 0.12300143390893936\n",
      "Training loss: 0.11572034657001495\n",
      "Training loss: 0.1283305585384369\n",
      "Training loss: 0.12420996278524399\n",
      "Training loss: 0.07681841403245926\n",
      "Training loss: 0.07851390540599823\n",
      "Training loss: 0.12940841913223267\n",
      "Training loss: 0.05463535711169243\n",
      "Training loss: 0.11383827030658722\n",
      "Training loss: 0.11855275928974152\n",
      "Training loss: 0.18924298882484436\n",
      "Training loss: 0.12286777794361115\n",
      "Training loss: 0.1485896110534668\n",
      "Training loss: 0.11148584634065628\n",
      "Training loss: 0.11816056072711945\n",
      "Training loss: 0.1294182538986206\n",
      "Training loss: 0.11508986353874207\n",
      "Training loss: 0.10596124082803726\n",
      "Training loss: 0.15485796332359314\n",
      "Training loss: 0.13804389536380768\n",
      "Training loss: 0.11760486662387848\n",
      "Training loss: 0.19551092386245728\n",
      "Training loss: 0.07344043999910355\n",
      "Training loss: 0.1327105164527893\n",
      "Training loss: 0.1991177499294281\n",
      "Training loss: 0.18542718887329102\n",
      "Training loss: 0.09547830373048782\n",
      "Training loss: 0.1359730213880539\n",
      "Training loss: 0.1791987419128418\n",
      "Training loss: 0.10849779099225998\n",
      "Training loss: 0.08065522462129593\n",
      "Training loss: 0.13233746588230133\n",
      "Training loss: 0.11502435803413391\n",
      "Training loss: 0.17496304214000702\n",
      "Training loss: 0.09901925176382065\n",
      "Training loss: 0.21118518710136414\n",
      "Training loss: 0.12116030603647232\n",
      "Training loss: 0.14486771821975708\n",
      "Training loss: 0.09146962314844131\n",
      "Training loss: 0.12920479476451874\n",
      "Training loss: 0.14448240399360657\n",
      "Training loss: 0.16258971393108368\n",
      "Training loss: 0.16466832160949707\n",
      "Training loss: 0.1897580921649933\n",
      "Training loss: 0.13503699004650116\n",
      "Training loss: 0.09886282682418823\n",
      "Training loss: 0.14379626512527466\n",
      "Training loss: 0.1886569857597351\n",
      "Training loss: 0.10912232100963593\n",
      "Training loss: 0.10051923245191574\n",
      "Training loss: 0.14017081260681152\n",
      "Training loss: 0.0947519987821579\n",
      "Training loss: 0.17317509651184082\n",
      "Training loss: 0.13817453384399414\n",
      "Training loss: 0.15700189769268036\n",
      "Training loss: 0.12291562557220459\n",
      "Training loss: 0.11564762145280838\n",
      "Training loss: 0.12830311059951782\n",
      "Training loss: 0.12418462336063385\n",
      "Training loss: 0.0766703188419342\n",
      "Training loss: 0.07834884524345398\n",
      "Training loss: 0.1293954849243164\n",
      "Training loss: 0.05449836328625679\n",
      "Training loss: 0.1138068214058876\n",
      "Training loss: 0.11851637810468674\n",
      "Training loss: 0.18926960229873657\n",
      "Training loss: 0.12282038480043411\n",
      "Training loss: 0.14858600497245789\n",
      "Training loss: 0.11144235730171204\n",
      "Training loss: 0.11807633936405182\n",
      "Training loss: 0.1293538212776184\n",
      "Training loss: 0.11503463983535767\n",
      "Training loss: 0.10591118782758713\n",
      "Training loss: 0.1548580378293991\n",
      "Training loss: 0.13802891969680786\n",
      "Training loss: 0.11759442836046219\n",
      "Training loss: 0.19556038081645966\n",
      "Training loss: 0.07332120090723038\n",
      "Training loss: 0.1327102929353714\n",
      "Training loss: 0.19917461276054382\n",
      "Training loss: 0.18542152643203735\n",
      "Training loss: 0.09538456052541733\n",
      "Training loss: 0.13595956563949585\n",
      "Training loss: 0.1792435646057129\n",
      "Training loss: 0.10842406004667282\n",
      "Training loss: 0.08057015389204025\n",
      "Training loss: 0.13225983083248138\n",
      "Training loss: 0.11497433483600616\n",
      "Training loss: 0.17498916387557983\n",
      "Training loss: 0.09889578819274902\n",
      "Training loss: 0.2112802267074585\n",
      "Training loss: 0.1211206465959549\n",
      "Training loss: 0.14483414590358734\n",
      "Training loss: 0.09143412113189697\n",
      "Training loss: 0.12915101647377014\n",
      "Training loss: 0.1444132924079895\n",
      "Training loss: 0.16265922784805298\n",
      "Training loss: 0.16476686298847198\n",
      "Training loss: 0.1898273080587387\n",
      "Training loss: 0.13501891493797302\n",
      "Training loss: 0.09884047508239746\n",
      "Training loss: 0.14383284747600555\n",
      "Training loss: 0.18873082101345062\n",
      "Training loss: 0.10909710824489594\n",
      "Training loss: 0.10046810656785965\n",
      "Training loss: 0.1401241421699524\n",
      "Training loss: 0.09469497203826904\n",
      "Training loss: 0.1732769012451172\n",
      "Training loss: 0.13811981678009033\n",
      "Training loss: 0.15700247883796692\n",
      "Training loss: 0.1228335052728653\n",
      "Training loss: 0.1155778244137764\n",
      "Training loss: 0.1282775104045868\n",
      "Training loss: 0.12416083365678787\n",
      "Training loss: 0.07652775943279266\n",
      "Training loss: 0.07818993180990219\n",
      "Training loss: 0.12938380241394043\n",
      "Training loss: 0.0543668307363987\n",
      "Training loss: 0.11377698183059692\n",
      "Training loss: 0.11848170310258865\n",
      "Training loss: 0.1892961859703064\n",
      "Training loss: 0.12277505546808243\n",
      "Training loss: 0.14858312904834747\n",
      "Training loss: 0.11140097677707672\n",
      "Training loss: 0.1179957240819931\n",
      "Training loss: 0.1292923390865326\n",
      "Training loss: 0.11498190462589264\n",
      "Training loss: 0.10586366802453995\n",
      "Training loss: 0.15485896170139313\n",
      "Training loss: 0.13801521062850952\n",
      "Training loss: 0.11758499592542648\n",
      "Training loss: 0.19560876488685608\n",
      "Training loss: 0.0732065886259079\n",
      "Training loss: 0.13271084427833557\n",
      "Training loss: 0.19923031330108643\n",
      "Training loss: 0.18541650474071503\n",
      "Training loss: 0.09529468417167664\n",
      "Training loss: 0.13594719767570496\n",
      "Training loss: 0.17928756773471832\n",
      "Training loss: 0.10835348814725876\n",
      "Training loss: 0.0804886594414711\n",
      "Training loss: 0.13218554854393005\n",
      "Training loss: 0.11492668092250824\n",
      "Training loss: 0.17501506209373474\n",
      "Training loss: 0.0987773984670639\n",
      "Training loss: 0.21137261390686035\n",
      "Training loss: 0.12108305841684341\n",
      "Training loss: 0.14480237662792206\n",
      "Training loss: 0.09140031039714813\n",
      "Training loss: 0.12909963726997375\n",
      "Training loss: 0.14434751868247986\n",
      "Training loss: 0.1627267748117447\n",
      "Training loss: 0.16486269235610962\n",
      "Training loss: 0.189894899725914\n",
      "Training loss: 0.13500209152698517\n",
      "Training loss: 0.09881953150033951\n",
      "Training loss: 0.14386874437332153\n",
      "Training loss: 0.1888027787208557\n",
      "Training loss: 0.10907342284917831\n",
      "Training loss: 0.10041939467191696\n",
      "Training loss: 0.1400795876979828\n",
      "Training loss: 0.09464052319526672\n",
      "Training loss: 0.17337606847286224\n",
      "Training loss: 0.1380676031112671\n",
      "Training loss: 0.15700381994247437\n",
      "Training loss: 0.12275496125221252\n",
      "Training loss: 0.11551102250814438\n",
      "Training loss: 0.1282534897327423\n",
      "Training loss: 0.12413843721151352\n",
      "Training loss: 0.07639062404632568\n",
      "Training loss: 0.07803704589605331\n",
      "Training loss: 0.1293731927871704\n",
      "Training loss: 0.05424048751592636\n",
      "Training loss: 0.11374859511852264\n",
      "Training loss: 0.1184486597776413\n",
      "Training loss: 0.18932271003723145\n",
      "Training loss: 0.12273174524307251\n",
      "Training loss: 0.14858096837997437\n",
      "Training loss: 0.11136163771152496\n",
      "Training loss: 0.11791849136352539\n",
      "Training loss: 0.12923353910446167\n",
      "Training loss: 0.11493159085512161\n",
      "Training loss: 0.10581851005554199\n",
      "Training loss: 0.15486057102680206\n",
      "Training loss: 0.13800281286239624\n",
      "Training loss: 0.1175764724612236\n",
      "Training loss: 0.19565612077713013\n",
      "Training loss: 0.0730963796377182\n",
      "Training loss: 0.1327122002840042\n",
      "Training loss: 0.199284628033638\n",
      "Training loss: 0.18541190028190613\n",
      "Training loss: 0.09520851075649261\n",
      "Training loss: 0.13593560457229614\n",
      "Training loss: 0.17933085560798645\n",
      "Training loss: 0.10828564316034317\n",
      "Training loss: 0.08041028678417206\n",
      "Training loss: 0.13211454451084137\n",
      "Training loss: 0.11488104611635208\n",
      "Training loss: 0.1750408411026001\n",
      "Training loss: 0.0986633226275444\n",
      "Training loss: 0.21146222949028015\n",
      "Training loss: 0.12104735523462296\n",
      "Training loss: 0.1447722166776657\n",
      "Training loss: 0.0913681909441948\n",
      "Training loss: 0.12905025482177734\n",
      "Training loss: 0.14428459107875824\n",
      "Training loss: 0.16279253363609314\n",
      "Training loss: 0.1649560034275055\n",
      "Training loss: 0.18996110558509827\n",
      "Training loss: 0.1349862962961197\n",
      "Training loss: 0.09879995882511139\n",
      "Training loss: 0.14390406012535095\n",
      "Training loss: 0.1888730823993683\n",
      "Training loss: 0.1090514287352562\n",
      "Training loss: 0.10037288814783096\n",
      "Training loss: 0.14003674685955048\n",
      "Training loss: 0.09458848088979721\n",
      "Training loss: 0.17347308993339539\n",
      "Training loss: 0.13801772892475128\n",
      "Training loss: 0.15700580179691315\n",
      "Training loss: 0.12267978489398956\n",
      "Training loss: 0.11544691026210785\n",
      "Training loss: 0.12823103368282318\n",
      "Training loss: 0.12411738932132721\n",
      "Training loss: 0.0762585997581482\n",
      "Training loss: 0.07788991928100586\n",
      "Training loss: 0.12936364114284515\n",
      "Training loss: 0.05411924049258232\n",
      "Training loss: 0.1137218102812767\n",
      "Training loss: 0.11841725558042526\n",
      "Training loss: 0.18934877216815948\n",
      "Training loss: 0.1226903647184372\n",
      "Training loss: 0.1485794186592102\n",
      "Training loss: 0.11132418364286423\n",
      "Training loss: 0.11784452944993973\n",
      "Training loss: 0.12917743623256683\n",
      "Training loss: 0.11488352715969086\n",
      "Training loss: 0.10577557235956192\n",
      "Training loss: 0.15486297011375427\n",
      "Training loss: 0.1379912942647934\n",
      "Training loss: 0.11756876856088638\n",
      "Training loss: 0.19570237398147583\n",
      "Training loss: 0.07299038767814636\n",
      "Training loss: 0.1327141374349594\n",
      "Training loss: 0.19933784008026123\n",
      "Training loss: 0.18540814518928528\n",
      "Training loss: 0.09512588381767273\n",
      "Training loss: 0.13592493534088135\n",
      "Training loss: 0.17937326431274414\n",
      "Training loss: 0.1082206591963768\n",
      "Training loss: 0.08033516258001328\n",
      "Training loss: 0.1320466548204422\n",
      "Training loss: 0.11483755707740784\n",
      "Training loss: 0.17506632208824158\n",
      "Training loss: 0.09855394810438156\n",
      "Training loss: 0.21154916286468506\n",
      "Training loss: 0.12101352214813232\n",
      "Training loss: 0.14474375545978546\n",
      "Training loss: 0.09133763611316681\n",
      "Training loss: 0.1290031373500824\n",
      "Training loss: 0.14422470331192017\n",
      "Training loss: 0.16285644471645355\n",
      "Training loss: 0.16504669189453125\n",
      "Training loss: 0.1900256872177124\n",
      "Training loss: 0.13497160375118256\n",
      "Training loss: 0.09878165274858475\n",
      "Training loss: 0.14393863081932068\n",
      "Training loss: 0.18894144892692566\n",
      "Training loss: 0.10903073847293854\n",
      "Training loss: 0.10032858699560165\n",
      "Training loss: 0.13999585807323456\n",
      "Training loss: 0.09453880786895752\n",
      "Training loss: 0.17356744408607483\n",
      "Training loss: 0.13797014951705933\n",
      "Training loss: 0.1570083200931549\n",
      "Training loss: 0.12260791659355164\n",
      "Training loss: 0.115385502576828\n",
      "Training loss: 0.12821006774902344\n",
      "Training loss: 0.12409761548042297\n",
      "Training loss: 0.07613168656826019\n",
      "Training loss: 0.07774841785430908\n",
      "Training loss: 0.12935501337051392\n",
      "Training loss: 0.05400284752249718\n",
      "Training loss: 0.11369628459215164\n",
      "Training loss: 0.11838731169700623\n",
      "Training loss: 0.18937471508979797\n",
      "Training loss: 0.12265076488256454\n",
      "Training loss: 0.14857842028141022\n",
      "Training loss: 0.11128857731819153\n",
      "Training loss: 0.11777368932962418\n",
      "Training loss: 0.12912382185459137\n",
      "Training loss: 0.1148376539349556\n",
      "Training loss: 0.10573482513427734\n",
      "Training loss: 0.15486589074134827\n",
      "Training loss: 0.13798095285892487\n",
      "Training loss: 0.11756184697151184\n",
      "Training loss: 0.195747509598732\n",
      "Training loss: 0.07288852334022522\n",
      "Training loss: 0.13271677494049072\n",
      "Training loss: 0.1993897408246994\n",
      "Training loss: 0.18540450930595398\n",
      "Training loss: 0.09504666924476624\n",
      "Training loss: 0.13591496646404266\n",
      "Training loss: 0.17941483855247498\n",
      "Training loss: 0.10815824568271637\n",
      "Training loss: 0.08026296645402908\n",
      "Training loss: 0.1319817155599594\n",
      "Training loss: 0.11479596048593521\n",
      "Training loss: 0.17509159445762634\n",
      "Training loss: 0.09844867140054703\n",
      "Training loss: 0.21163345873355865\n",
      "Training loss: 0.1209813728928566\n",
      "Training loss: 0.14471666514873505\n",
      "Training loss: 0.09130855649709702\n",
      "Training loss: 0.1289578676223755\n",
      "Training loss: 0.1441674679517746\n",
      "Training loss: 0.16291847825050354\n",
      "Training loss: 0.16513486206531525\n",
      "Training loss: 0.19008879363536835\n",
      "Training loss: 0.13495780527591705\n",
      "Training loss: 0.09876449406147003\n",
      "Training loss: 0.1439725160598755\n",
      "Training loss: 0.18900814652442932\n",
      "Training loss: 0.10901141166687012\n",
      "Training loss: 0.10028620809316635\n",
      "Training loss: 0.13995671272277832\n",
      "Training loss: 0.09449129551649094\n",
      "Training loss: 0.17365947365760803\n",
      "Training loss: 0.1379246711730957\n",
      "Training loss: 0.15701128542423248\n",
      "Training loss: 0.12253913283348083\n",
      "Training loss: 0.11532669514417648\n",
      "Training loss: 0.12819018959999084\n",
      "Training loss: 0.12407903373241425\n",
      "Training loss: 0.07600947469472885\n",
      "Training loss: 0.07761228084564209\n",
      "Training loss: 0.12934724986553192\n",
      "Training loss: 0.053890980780124664\n",
      "Training loss: 0.11367245018482208\n",
      "Training loss: 0.11835896968841553\n",
      "Training loss: 0.18939998745918274\n",
      "Training loss: 0.12261303514242172\n",
      "Training loss: 0.14857815206050873\n",
      "Training loss: 0.11125466227531433\n",
      "Training loss: 0.11770579218864441\n",
      "Training loss: 0.12907260656356812\n",
      "Training loss: 0.11479378491640091\n",
      "Training loss: 0.10569588840007782\n",
      "Training loss: 0.1548694521188736\n",
      "Training loss: 0.13797102868556976\n",
      "Training loss: 0.11755557358264923\n",
      "Training loss: 0.19579178094863892\n",
      "Training loss: 0.07279036939144135\n",
      "Training loss: 0.13271969556808472\n",
      "Training loss: 0.19944043457508087\n",
      "Training loss: 0.18540173768997192\n",
      "Training loss: 0.09497057646512985\n",
      "Training loss: 0.13590586185455322\n",
      "Training loss: 0.1794554889202118\n",
      "Training loss: 0.10809852927923203\n",
      "Training loss: 0.08019381761550903\n",
      "Training loss: 0.13191944360733032\n",
      "Training loss: 0.11475624144077301\n",
      "Training loss: 0.17511636018753052\n",
      "Training loss: 0.09834780544042587\n",
      "Training loss: 0.21171534061431885\n",
      "Training loss: 0.12095092982053757\n",
      "Training loss: 0.14469113945960999\n",
      "Training loss: 0.09128082543611526\n",
      "Training loss: 0.12891465425491333\n",
      "Training loss: 0.1441129744052887\n",
      "Training loss: 0.16297879815101624\n",
      "Training loss: 0.1652204394340515\n",
      "Training loss: 0.1901502013206482\n",
      "Training loss: 0.13494494557380676\n",
      "Training loss: 0.09874842315912247\n",
      "Training loss: 0.1440056413412094\n",
      "Training loss: 0.18907290697097778\n",
      "Training loss: 0.10899323225021362\n",
      "Training loss: 0.10024575889110565\n",
      "Training loss: 0.13991937041282654\n",
      "Training loss: 0.09444594383239746\n",
      "Training loss: 0.17374897003173828\n",
      "Training loss: 0.1378812938928604\n",
      "Training loss: 0.15701471269130707\n",
      "Training loss: 0.1224733516573906\n",
      "Training loss: 0.11527030915021896\n",
      "Training loss: 0.12817175686359406\n",
      "Training loss: 0.12406157702207565\n",
      "Training loss: 0.07589204609394073\n",
      "Training loss: 0.07748136669397354\n",
      "Training loss: 0.1293403059244156\n",
      "Training loss: 0.05378377065062523\n",
      "Training loss: 0.11364953219890594\n",
      "Training loss: 0.11833178251981735\n",
      "Training loss: 0.18942521512508392\n",
      "Training loss: 0.12257684022188187\n",
      "Training loss: 0.14857804775238037\n",
      "Training loss: 0.11122241616249084\n",
      "Training loss: 0.11764083057641983\n",
      "Training loss: 0.1290237158536911\n",
      "Training loss: 0.11475196480751038\n",
      "Training loss: 0.1056591048836708\n",
      "Training loss: 0.15487343072891235\n",
      "Training loss: 0.13796250522136688\n",
      "Training loss: 0.11755003780126572\n",
      "Training loss: 0.19583463668823242\n",
      "Training loss: 0.0726962685585022\n",
      "Training loss: 0.1327233761548996\n",
      "Training loss: 0.19948969781398773\n",
      "Training loss: 0.1853988915681839\n",
      "Training loss: 0.09489783644676208\n",
      "Training loss: 0.1358971744775772\n",
      "Training loss: 0.17949530482292175\n",
      "Training loss: 0.10804103314876556\n",
      "Training loss: 0.08012723177671432\n",
      "Training loss: 0.13186015188694\n",
      "Training loss: 0.11471827328205109\n",
      "Training loss: 0.17514100670814514\n",
      "Training loss: 0.09825049340724945\n",
      "Training loss: 0.21179433166980743\n",
      "Training loss: 0.12092199921607971\n",
      "Training loss: 0.14466679096221924\n",
      "Training loss: 0.09125450253486633\n",
      "Training loss: 0.12887302041053772\n",
      "Training loss: 0.1440608650445938\n",
      "Training loss: 0.16303721070289612\n",
      "Training loss: 0.16530361771583557\n",
      "Training loss: 0.1902102828025818\n",
      "Training loss: 0.13493289053440094\n",
      "Training loss: 0.09873343259096146\n",
      "Training loss: 0.1440380960702896\n",
      "Training loss: 0.1891360580921173\n",
      "Training loss: 0.10897643864154816\n",
      "Training loss: 0.10020722448825836\n",
      "Training loss: 0.139883354306221\n",
      "Training loss: 0.09440256655216217\n",
      "Training loss: 0.17383626103401184\n",
      "Training loss: 0.13783980906009674\n",
      "Training loss: 0.1570184975862503\n",
      "Training loss: 0.12241040170192719\n",
      "Training loss: 0.1152162104845047\n",
      "Training loss: 0.128154456615448\n",
      "Training loss: 0.12404513359069824\n",
      "Training loss: 0.07577899843454361\n",
      "Training loss: 0.07735537737607956\n",
      "Training loss: 0.12933409214019775\n",
      "Training loss: 0.05368076637387276\n"
     ]
    }
   ],
   "source": [
    "from valuation.models.pytorch_model import PyTorchSupervisedModel, PyTorchOptimizer\n",
    "from valuation.models.binary_logistic_regression import BinaryLogisticRegressionTorchModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model = PyTorchSupervisedModel(\n",
    "    model=BinaryLogisticRegressionTorchModel(num_features),\n",
    "    objective=F.binary_cross_entropy,\n",
    "    num_epochs=100,\n",
    "    batch_size=128,\n",
    "    optimizer=PyTorchOptimizer.ADAM_W,\n",
    "    optimizer_kwargs={\n",
    "        \"lr\": 0.005,\n",
    "        \"weight_decay\": 0.005\n",
    "    },\n",
    ")\n",
    "model.fit(\n",
    "    dataset.x_train,\n",
    "    dataset.y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def6f02",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is important that the model converges to a point near the optimum, or otherwise the influence values will be of bad quality. Next the influences with respect to the previously fitted logistic regression model are calculated. A influence function\n",
    "\n",
    "$$I(x_1, y_1, x_2, y_2) \\colon \\mathbb{R}^d \\times \\mathbb{R}^d \\to \\mathbb{R}$$\n",
    "\n",
    "measures the influence of the data point $x_1$ onto $x_2$ conditioned on the training targets $y_1$ and $y_2$ trough some model parameters $\\theta$. As long as the loss function L is differentiable (or can be approximated by a surrogate objective) the influences\n",
    "\n",
    "$$\n",
    "I(x_1, x_2) = \\nabla_\\theta\\; L(x_1, y_1) ^\\mathsf{T} \\; H_\\theta^{-1} \\; \\nabla_\\theta \\; L(x_2, y_2)\n",
    "$$\n",
    "\n",
    "can be linearly approximated. Using the pyDVL library the influences can be estimated by the following snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa37f65c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from valuation.influence.general import influences\n",
    "from valuation.influence.types import InfluenceTypes\n",
    "train_influences = influences(\n",
    "    model,\n",
    "    dataset.x_train,\n",
    "    dataset.y_train,\n",
    "    dataset.x_test,\n",
    "    dataset.y_test,\n",
    "    influence_type=InfluenceTypes.Up\n",
    ")\n",
    "test_influences = influences(\n",
    "    model,\n",
    "    dataset.x_test,\n",
    "    dataset.y_test,\n",
    "    influence_type=InfluenceTypes.Up\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ecfc98",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "using autograd and explicit construction of the Hessian. Recall the train influences have shape [NxM] where N is the number of test samples and M is the number of training samples. The keene reader notices that, in order to obtain a valid ranking for the training data, each column of the aforementioned matrix has to be reduced to a single value, resulting overall in an vector of size [M]. There are various different choices in order to select  A few strictly positive sample metrics\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{MAI}(x) &= \\frac{1}{N} \\sum_{i=1}^N | I(x, x_i) | \\\\\n",
    "\\text{PMAI}(x) &= \\frac{1}{N} \\sum_{i=1}^N \\max(0, I(x, x_i))  \\\\\n",
    "\\text{NMAI}(x) &= \\frac{1}{N} \\sum_{i=1}^N \\max(0, -I(x, x_i))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "are given. This notebook restricts to use the mean absolute influence to filter for the wrong data labels. It is suggested to try other selection rules when applying this to real world data. After calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "920a792e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mean_influences = lambda arr: np.mean(np.abs(arr), axis=0)\n",
    "mean_train_influences = mean_influences(train_influences)\n",
    "mean_test_influences = mean_influences(test_influences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380ef2d0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "the data is again visualized along with their influences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "515809bf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 864x288 with 3 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqwAAAEICAYAAACap6ZVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAEAAElEQVR4nOydZ4AT1dqAnzOTvo2y9I6IgBQVBBULoCJg77333sv1s/feCyBW7AUEURQRUDrSpffOAruwLX1mzvdjkmyySZYFFliWeby57GbOnDnJZt68561CSomFhYWFhYWFhYVFdUXZ3wuwsLCwsLCwsLCwqAhLYbWwsLCwsLCwsKjWWAqrhYWFhYWFhYVFtcZSWC0sLCwsLCwsLKo1lsJqYWFhYWFhYWFRrbEUVgsLCwsLCwsLi2qNpbBa1GiEEAOFEI/v73VYWFhYWFhY7D6WwmpRrRFCrBFCnLK750spb5FSPluVa7KwsLA4kNhTORqZ4xohxKSqWpOFxa5iKawWByxCCNv+XoOFhYWFhYXF3sdSWC2qLUKIoUBz4BchRKkQ4iEhhBRCXC+EWAeMi4z7QQiRJ4QoEkL8I4Q4PG6Oz4QQz0V+7iWE2CCEuF8IsVUIsVkIce1+eXEWFhYW+4A0cvQYIcQUIUShEGKeEKJX3PhrhBCrhBAlQojVQojLhRDtgYHAsZE5CvfLi7E4qLEUVotqi5TySmAdcKaUMhP4PnLoJKA9cFrk99HAoUB9YDbwVQXTNgRygCbA9cD7QojaVb96CwsLi/1PCjn6FfAr8BxQB3gA+EkIUU8IkQG8A/SXUmYBxwFzpZSLgVuAqVLKTCllrf3wUiwOciyF1eJA5CkppVdK6QeQUn4ipSyRUgaBp4AuQoicNOeGgWeklGEp5W9AKXDYPlm1hYWFxf7nCuA3KeVvUkpDSvknMBMYEDluAB2FEG4p5WYp5cL9tlILizgshdXiQGR99AchhCqEeEkIsVIIUQysiRzKTXNugZRSi/vdB2TunWVaWFhYVDtaABdGwgEKI+7944FGUkovcDGmNXWzEOJXIUS7/bhWC4sYlsJqUd2RO3nuMuBs4BRMV3/LyPNi7y7LwsLC4oAhXmauB4ZKKWvFPTKklC8BSCn/kFKeCjQClgAfpZjDwmKfYymsFtWdLUDrCo5nAUGgAPAAL+yLRVlYWFgcQMTL0S+BM4UQp0U8VK5IQmpTIUQDIcTZkVjWIGbIlBE3R1MhhGPfL9/CwlJYLao/LwKPRdxWF6Q4/gWwFtgILAKm7bulWVhYWBwQxMvRizG9Uo8C2zAtrg9i6gMKcB+wCdiOmeB6a2SOccBCIE8Ikb8vF29hASCktKz8FhYWFhYWFhYW1RfLwmphYWFhYWFhYVGt2WOFNRL/MiNSfHihEOLpqliYhYWFhcXewZLbFhYWBxp7HBIghBBAhpSyVAhhByYBd0sprVhCCwsLi2qIJbctLCwONPa4F7s0Nd7SyK/2yMMKjLWwsLCoplhy28LC4kBjjxVWMIu3A7OANsD7UsrpKcbcBNwEkJGR0bVdO6sWsYWFxYHHrFmz8qWU9fb3OvaUncltS2ZbWNRM9kSGndLXIwsK9EqNnTs79IeUst/uXCcVVVolQAhRCxgO3CmlXJBuXLdu3eTMmTOr7LoWFhYW+wohxCwpZbf9vY6qojJy25LZFhY1hz2RYUd2dckJk5tVamwt94oqlZVVYmGNIqUsFEKMB/oBaRVWCwsLC4vqgSW3LSwsKo0EYVRdI0khxBqgBNABrSIFtyqqBNSL7NARQriBUzHbuVlYWFhYVEMsuW1hYbHbSFG5R+XpLaU8YmfW2KqwsDYCPo/EQynA91LKUVUwr4WFhYXF3sGS2xYWFruMoGotrLtCVVQJmA8cWQVrsbCwsLDYB1hy28KiZhAOh9mwYQOBQCDpmMvlomnTptjt9qq7oAShVXp0rhAiPvh9sJRycPKMjBFCSGBQiuMxqjSG1cLCwsLCwsLCYt+wYcMGsrKyaNmyJWZ5ZRMpJQUFBWzYsIFWrVpV3QUliMrn6udXIunqeCnlRiFEfeBPIcQSKeU/qQZarVktLCwsLCwsLA5AAoEAdevWTVBWAYQQ1K1bN6XldU8RRuUelUFKuTHy71bMaiXd0421FFYLCwsLCwsLiwOU8srqzp7fYwxZucdOEEJkCCGyoj8DfamgUokVEmBhYWFhYWFhYbFzdi0kYGc0AIZHFGsb8LWU8vd0gy2F1cLCwsLCwsLConJU0t2/M6SUq4AulR1vKawWFhYWFhYWFgcoUsqU7v+q7GQaRUgQWtXPWxmsGFYLCwsLCwsLiwMQl8tFQUFBknIarRLgcrmq/JpCVu5R1VgWVgsLCwsLCwuLA5CmTZuyYcMGtm3blnQsWoe1yqmikIBdxVJYLSwsLCwsLCwOQOx2e9XWWd0ZsvIlq6oaS2G1sLCwsLCwsLCoHHshNrYyWAqrhYWFhYWFhYXFztm11qxViqWwWlhYWFhYWFhYVIq9kVBVGSyF1cLCwsLCwsLConJYMawWFhYWFhYWFhbVFomlsFpYWFhYWFhYWFRfBCBkcpOCfYGlsFpYWFhYWFhYWFSO/WRhtTpdWVjUMLbklbJixXZ0fT9JFQsLi4OWHTsCzJmdx/bt/v29FIu9gQT0Sj6qGMvCamFRjdA0gyGDZ/Ppx3MJhXQuvKgDd97Tnaws507P3brFy9VX/MzMfzeh2hQyPHbeHzSAfv3b7IOVW1hYHMzousGD9/3Jl1/Mx+5QCYV0Lru8E6+/1RebzbKN1SSEsX9CAqxPkYXFXmLpknzG/LGSzZtKKn3OVZcP58nHJ7B4UT4rV+zgrTem07f3l4RCFW9XpZScc+Z3TJ+2kWBQx+cNs22bj6uv+Jkli/P39KVYWFhYVMjrr0zl6y//IxDQKSkOEQzofPv1Al56YdL+XppFVSJ34VHFWAqrRY3i+28X0uXwgeTmvEr7Q9/nwnN/4L13ZlBQsO/cU4WFAU47+UtOPO4zrrtqJJ07DOTO20ZjGBXfwfPnbeGvP1fj95VVZQ4ENNasLeSXEcsqPHfe3C2sWrUDTUsMAwgFdQZ+MHP3X4yFhYVFJfjg/Zn4fIkV5f1+jYHvz9pPK7LYaxiico8qxlJYLWoMn30yl7tu/53VqwoJhXQ2bijhj99X8tTjE+jS4UP+m79lj+bfklfKVZcNJ7fWq9Sv/So3XDuSgnxf0rjbb/mNWTM34/drFBcHCQZ1fvhuER8Nml3h/P/O2JTyeW9pmMmT1lV47uZNJdjUZAGh65K1a4oqPNfCwsJiTykqDKR8vqQkiNxPrTwt9hJGJR9VjKWwWtQIpJQ88+Q/+HzhpGOhkEFxcYibrh+12/MHgxq9T/yCUb8sIxTUCQR0hv+0hFN6D02wapaWhvhj9MokF77PF+aD9/6t8BqNGmWipoj1cjpVmjXPrvDcI49qRDCYHDbgdtvofXLLCs+1sLCw2FO6HNEw5fMdO9VHiP0T82ixF7BCAiwsUpO/zccfv69kzuy8CnfpXm+YwjQ7/CjLl21PaRGtDL+MWEbhDj+aVraGcNhgS56XP/9YGXvOn0JhjlJcHKzwGqee1pqMDDuKkijcg0Gdf6dvqjCOtWGjTK674Qg8HnvsOYdDoXYdN1df26XC61pYWFhUxKaNJSxetC0p5CieV14/BY+nTH4pisDtsfHSKyczeOAsTjzuM07q+RlDBs8mHN4LKeQW+wgBulK5RxVjKawW1RIpJc889Tft277P9VePZEDfr+jR9WM2bUydwOTx2MnMtKc8Fj+nzb57H/kli/MpLU1WRgOBMEsWF8R+z63noVHjzKRxqiro2/eQCq9ht6v8PvYK6tX3JB37a+xqnnxsQoXnv/TqKbz57ml0OaIBrVrV4qZbujJp6rXk5LgqPM/CwsIiFVu3eOl3ypd06TiQk08ayiHN32HE8CUpx3bv0YRx/1zFeRe047DD6nLuee0YO/5KXn1lKk/83wTmzsljzuw8HvvfeC4+/0crTOBARYIwKveoaqyyVhbVkl9GLOODd2cSDOgEA+ZufPmyAi696Cf+nnxN0nhFETzw8HG88OyklGEBqirocUyT3Vbe2h5Wl8xMe5LS6nLZaXtYndjvQgje+7A/F53/I6Ggjq5LnE6VzEwHjz15Qtr5ly7JZ+yYVcyenZfSCuz3a3z68VxeeLlPzL22aWMJb70xjUkT19GiRS3uua8Hl17WkUsv67hbr9HCwsIinvPO/o5FC/PRNINgpLDmTdePolXr2nTu0iBpfIfD6/HJ52cD4PeHGfjBTKZNWU8gUGZR9fnCTJmygWlTNnBsz2a7tB4pJXPnbKGgwEfXbo2pXXvvb8Z13WDMH6sYP241DRtmcullHWnUOGuvX7das5/KWlkKq0W1xMw4TVQOdV2yeHE+a9YU0rJlraRz7ry7O6qq8MpLk9mxPQACbDaBy2WnTm0XH31y5i6vQ9MMtuSVcupprcnOceH3a+i6aRmw2RTq1/fQqUsDRv+2gmbNsunYqT4n9WrJxCnX8sF7/7Ji+XaO69mMm27pSm69ZMupYUhuuv4XRgxfmjIGNR6/P4yuS2w2wbq1RRx/zKd4vSHCYYOFC7YxbtxqPhg0gPMv6LDLr9PCwqJmUlISZNiPS1i3roiuXRtxWv9DUNWde5oW/LeVFcuTK48EgzofvjeTDz86Pe25v41azvXXjiQcNgilkGvBgMaUXVRY160t4twzv2PTphJUVRAKGfzvseO59/5jKnX+f/O3MGnieurV83D6mYfidlfskQMIhXTOGvAt8+bl4S0N43SqvPLiZL754QJ692lZ6bXXOPaTcdxSWC2qJYVpuqTYbErabFQhBLffeTS33dGNUEjnv/lbmTM7jxYtczj5lFYphXRxcZA1qwtp1jwnabf+0aBZPPPkP7HY0Qsv7kD+Nh9j/liFEHD6mYfi8djp2nkwTqcNTTNo174uP424mLaH1eWtd/slXW/e3DxG/bIcu13hvAvaM3niOn4ZsWynyirA4R3rxwpwv/j8JEpKgjHlWUrw+zQeuOdPzjm3XaW+kCwsLGo2Sxbnc9rJX5q1mX1hMjLttG5Vm9//unynzUjy8kpTFvw3DMm6dekrj2zaWMK1V43A79fSjnG6bNSPhD4ZhmT58u08+dh4/h6/lowMBzfcfCT3P3gsdrsKmJbVC879gZUrdySUB3z5hcl0OaIBfU5ulfZahiG5/pqR/DZqOYYhsTtU7rtnDKNGX5rSShzP55/OY86czbFSg1E5fe1VI1ix5s6DsyGCxLKwWljEc8ZZbVm5ckeSIqcqgg6H16vwXCEETqeNbkc3ptvRjVOOMQzJY4+OY8igOaiqIBDQOKRNbZ59oTen9WvDiOFLefzRCQlW3h++W8QNNx1FwQ/nA2YZrf89NI5gUI+t87/5W7nxul8YNuKipGv+3yPjGDJ4NsGgjqoKXnt5KnVz3RUKdjDDGex2leOPb8arL0/htH6HMGHcmpiyGo/XG2ba1I30PH7XXG0WFhY1j+uvGUlhYYBouKi3NMyyZQW89vJUnn6uV4XnHnFEQ4LBZNnkctk4+ZT0CuJPPywiXEFyFoBNVeh9cktuuHYkw39aQjhcNt7rDfPma9NYuriAT4ea4QWLF+Wzbm1RUi1rny/Mh+/PrFBh/ebrBYz+dUVMzkZl9aUX/cSCJbdWWMHg268XJNTFjhIK6cybu4Wu3RpV+DprLLrV6crCIsbtd3WnYcNM3G5zT6UoArfbxtvv94vtuveE99+ZwScfzSUQ0PB6TVf7sqXbufziYVxywY+89EJyLKzfr0UyXA2EEHzwXnLYQjhs8M+EtezYkWgFnjVzM0M+mo3fr2EYknDYIBDQ2Lih4i5YQkCPY5ogpeTTT+bywrMTObX3UIJpKgYEAhrnnPEtfU78nLzNpbvxzlhYWNQE8rf5WLqkgPK5TcGgzvffLdzp+bn1PNx2Rzc8GYmVR+rUdXPtDUemPW/TphK0cGqFVVUFTZtlM+LXi7n84uH8PGxpgrIaxe/XGDVqGWvXFgJmjVfVllpJisb8BwIaK1fuwOsNJRz//JO5KfMathf4+enHJVxywY907jCQSy/8iblz8hLGOBypv2ukIbHvZgLvgY8AWclHFWNZWC2qJbVru5g84zo+/3Quf45ZTdOmWdxyW7cKXThr1hTy1uvT+HfGJg5rV5d77jsm7fh33pqRUohpmuTvCWsQSmphZBiS4qIgufU8actUKarAWxpKCDH4efgSgoHknboQJH2hxOZR4Jzz2jH61xUJlma/X0M3zGSuVKEEwaDOnNl5nHvWd0yZcR1CCHTd4J+/17F5cwndezShTZs6SedZWFjUHCoqfapUsi7qU8/2ossRDXn/3X/Zvt3PGWe25Z77esRk29TJ63nw/rH8N38LObVcdO/RmAnj1qSdr1HjTBYsuYVZM/NYtrSgwlJ9ToeNRQvzadGiFl2ObJjSo+Ry2TjjrLa89vIUXn91KmDmOlx9XRdefPlkbDaFUCi18mwYkltvGkUoqCMlrF1TyLhxqxk24iJ6Ht8cgGuuO4I5s/OSvitq13bRqXP9Ct+7GosEaYUEWNRUFi/axqMP/MXUKRvIynJy4y1Hcd/Dx+40/ic728mdd/fgzrt7JB3bvKmEB+77kz9Gr0RR4KTeLZk0cR3BgI6mmUlIv45azrdpguPLW0Dj8ft16tS14/MmK7RZ2U7q1HUDcFr/Nnz5+fykpIS6ddw0aZpFMGhaU91uOzZVibieEoVuRZVdDmuXyymntmbMH6uSjoWCOkce1ZAli/MJh42kNei6ZPXqQv6bv5XsHCen9/2aHTsCSCnRdcl5F7Tng0EDkmq+WlhY1Azq5nro1Lk+c2bnJbjSXS4bl11RuUoiQgjOu6A9513QPunYf/O3cO5Z38eUucIdAcb8niyr4snb7GXqlA2sW1e802YC4bDOIYfUBsyyha+8dgoP3v8nAb+GlGZTlMZNssjIcPDkY4nhW198Ng+Px87Tz/bi0ss7snjxtiTXfiikJyjB0TyAhx/4i0nTrgXMvIVxY1fz8/AlSAk2u4LNpvDtjxcc3M0Q9oL1tDIcrDZti33EurVF9D3pSyaMW0swoJO/zcdbr03jrltG7/acfn+Y3id8wW+jlhMKmV2nxvy+Em9pOKa4GYbE79O4587fU9b7O6xd3Qqvsb0gkKRQuz02nnuxd0zJe/T/jqdOXTcul7nvs9kEHo+d517qwyUX/ESj3DdoXO8NTun1BV27NUrrXkpFTo6TKTOuw+22I43UWm3HTvVYsfZODu+UOqbXpip89+1CunYezPr1xZSWhvB6wwQCGj8PW8JXQ+dXai2bNpYweOAsBn4wk3VrrTavFhYHCkM+O4u6uW4yM+3YbAoZGXY6d2nAfQ8eu8dzv/LSFPz+9I1SUqHrBr/+spwOHXIxjPRxrk6nytE9mtD2sDI5fdU1XRg1+jIuvLgDJ/ZqzpPPnMTEqdfy3jvJ3jK/T2Pwh7PQdYNrruvCkUc2IiNSp9vpUvFk2FNabMGsjhBFUQSDPj6D8ROv5rkXe/PeB/1ZuvL2nSZr1Xj2U2tWy8JqsVf54N1/k1zhfr/GsB8W8/gzJ+5WPbufhy+lqDiQtDtOxfp1xZSUhMjOTsyILdlJ1ykwXfL1G2QgpaR58xwe+b/jOa1fWfH/ho0ymTH7Bj4ZMoeJ/6yjdeva3HxrVy6/ZBirVxXGlOd/Z2zitpt/48prOjP4w9mVeo12h4qqKpzSt1XapCxvaZisLCfnnd+epYsLCJR7n32+MIM+nJkyRsznC/PRoNlceXXFXbA++2QuD90/FgQgJU/833iefq4Xt95+dKVeh4WFxf7jkENqs3DpbYwauYz164o5qltDTjypRZVYBxcu2FqhhygVNptCZpaDLkc0pOvRjZkxbWNSWJPdrnD+hR147c1Tk84/untjju5+VsJz27al7l7o84VZvXoHbdrU5dc/LuXPMav4Z8JaGjTM5JJLD6f7UUNSetpqpajt2uHwejtN9j1okOw3C6ulsFrsVWb/uzmlwuR02Vi2pGC3FNbFi7bhTdF1KhWqKhg7ZhX/zd9Ky1a1OO+Cdvi8YTZXIiEpFDIoLgowa95NNGuek3BsS14p27b5aHNoHR546DgeeOg4AMaPW8PmTaUJLnopIRjUWLu6sNKv0VtqJg643fa0sWgTJqwF4LobjuCjQbPZutUbq3no8ZjneVOENcSuUcExgI0binno/j8Tin4DPPnY35x62iFWHKyFxQGAy2XjgouqvjZzx04NWLWyMClzvyJUm8LFl5rhCD8Ov5BnnvyHr4b+RzCoccqprXjy2V60bl2bcFhn3NjV+Hwavfu0jNWwllIy7q81fP/tQlRFcMnlHenSpQFTp2xIupaU0LP7p3z6xdkMOONQ+vVvQ7/+bWLHb7/raN54dSq+uFABj8fGnXdbm/GKEXul7WplsBRWi73K4R3rM2fWZjQtUaiFgjqtIvFJu0q79rnYbCJpzvI4XSoul407bh1NaWkIT4adJx+bwHc/nl9py4BQBCtW7IgprMXFQa67egR/j1+Lw6EiJdx6R1dWLNvBv/9uwuFQCYWSLaJ+v0Zenrdy1xRwXKSgdkVfBtsL/DTMfZ3T+h3Cb2Mu4/NP5/HrL8uoXcfNzbd25ZorRlR4nTPPalvh8V9GLsM0rSai6wYjhi/l/ipwK1pYWFQ/fL4wa1YX0qBhJnUjMfvleeiR4xjz+8okd3z5RFKnS0UgCAY1ggGNs0//lieeOpGLLjmcF185mRdfOTnh/CmT1nPheT8AZix+MKjRslUtzr+wPatWFvLbr8tj+QXDflrCgNPb4PbYUpaf8vs1rr92JKvW3ZXUKOCBh45je4GfT4bMxW5XCIcNrr3+CO6935JrFbIf67CKPe3nK4RoBnwBNMB8KYOllG9XdE63bt3kzJkz9+i6FgcGK5Zv56QenyUINZdLpc+prfnqh/N2a06/P0zTBm+mtNya89uQSBo3zmLD+uKkce075LJtq4/8FC1QUzFp2rWxmKVzzvyWiX+vS5qzomz/KMef2IyZMzYlWSzLIwT8MPxC+p5mhh/07TOU6dM2Vjh/RqadlWvvxONxAKYl4tCW77J1a+rXqCiCz786h7PPOSztnO+/+y9PPTEh1ho3iqoKOnaqz7atPurUdXPXvd25+JLDD5okBCHELCllt/29jj1hV+W2JbMPDqSUvPn6NF55cQqKKgiHdPqc0oqTT2lFnTpu+p/ehowMR2z8tCkbePD+P/lv/lZycpxcctnhbN3qZdrUjTRrls0dd3dn1r+beO+dfxNkpttj4423T+PyKzrx3/wt/PH7SlwuG/0HtOHE4z6jpCSUtLZ0RgqX28aQT87gvrvHpJR3WdkOvvjq3LS1Y0tKgmxYX0zTZtk7baZQU9gTGdbtkAw54+XkJLxUqBfOqlJZWRUKayOgkZRythAiC5gFnCOlXJTunINN+Hm9XjIyMvb3MvYb48au5v47x7B2TSFOp43Lr+7Ecy/3iSUr7Q6PPDCWgR/OSmmBtNsVHn70OAZ9MItt21J3zKqMghnlhBObMWzkxbz4/ETeeHX6bq8ZoGHDDLZv96cttRKlZatazFt4M0IIlizOp89JX+D3hdMmCgDccntXHn/yRH4dtZzioiA7tgd46YVJKc/JzHLwyedn0a59Lk2bZqes2LBmTSHdjxySFBsLZnxvNGfC47Fz6x3dePLpk3by6msGNURh3SW5faDL7NLSUjIyMg6aTVUqNm8q4e23pjPx73W0alWLu+87hqO7JzZW+f7bhdx+y29JcaVCgNNpyuuvvj2XU087hHQsXZLPtm0+uhzRgKwsJ107D2b58u1J4xo1yuSc8w7js0/mEQqZzVR0XcbarlYWh0PhyadPYvq0jYwcsSzpeFaWgy+/PZdGjbIIBDQ6dqrPju1+vhw6n1UrCznu+Gace167Pfo+OtDYU4V1+ouVCzGxXTyzeimsSRMKMQJ4T0r5Z7oxB7rw2xUCgQDdu3fnuOOO44033sDjSe4nX5N569VpvPb8FOwOBS1s0KJVDj/8chGNmlQ+dnX5sgJ27AjQqXP9mFsnf5uP7l2HsL3ARwXJptWSyijLLpfKH39dSa3aLs7s/w1bt3hTKo7xNGiYEYvt1XQDJLQ9rA7z521NGut0qkgpsdlUnE6Vl187hUsuSy51885b03nu6YlomoGUEiHMMIXy77nLZWPpqjuS2tvWRGqCwlqencntA1lmBwIBevfuzeGHH87AgQOx2Q4exSTKurVFnHDsp5SWhiKNT0zL5OAhZ3D2ue1i47p0GMjqSsTav/7Wqdx4c9eE5/I2l3LheT+wbFkBdptKOKzzf0+cwNNP/J3SGyaEKTd21ulvZ3g8Nh76X0/+mbCW8ePWJMnWrCwH9ep5yMvzoqgCRYhY/ddAQCMjw07DRpmMn3g1tWrVfPkFVaCwvnB4pcbaLvm3SmVllUbOCiFaAkcCSWYoIcRNQoiZQoiZ27Ztq8rLVmsURaFfv34MGjSIrl27Mnfu3P29pH3G2D9W8caLUwkENEqKQ/j9GsuXbufyC4YBpvvpl5+XcvG5P3DhWd/z0/eLEiymG9YX07PHJxx/zKece9Z3tG72TqwUU249D5OmXkv9Bpn75bXtCZXZIwYCOm+9MZWLzv2B9euKdqqsAmzJ81JaGqK0NETArxEIaKxYsYMTTmoeS9xSFIGimDUOQyEDny/Mjh0B7rnzd/75e23SnHfd04PJ06/lkf/rycOP9uTQtnVSbhAcDpXFCw+e+7omkU5u1xSZ7XQ6OfXUU/n4448555xz8HorF0tek3jhuYkUFwdjimO05ui9d49B18tu6A0biis136MPj+O/+VsSnrv4gh9ZuGArfp9GcXEQv1/jhWcnUTc3dQxsVSirYG6g33lzBn9PSFRWbXazjJeiClavLsTnC1NaEqK4OEggoMVkqtcbZv26Yl55afIer+WgIBrDWplHJRBCqEKIOUKIUTsbW2UKqxAiE/gJuEdKmfSpl1IOllJ2k1J2q1fv4CkP4XA4eOWVV/jzzz8pKiqie/fuvP766xXWoKspDErRulTXJcuXFLByxXZuu/E3brn+V8aMXsXYMau569bfueayn5FSIqXk3LO+Y9HCbfj9psLr9Ya5+44/OP7YTzi228d8+vEcOnaqft1GVLVq3I6jRi5nxYrtu1w6Jh6fN8zkietjc0Sto+U/fj6fxuuvTE05x6Ft6/LQIz155NHj6dChfsqqBaGQvktWc4vqQUVyu6bIbCEEzzzzDAMHDmT06NH06dOHA1kB3xUCAY3XX53Cd98sTBka5C0NJSiplW0kEgzqvPT8pNjvK1ZsZ8ni/KQYU58vTL16Gbg9iVZtj8dOy1a1duGVmDgcCjabICvLQVaWg4wMOwPOOBSvN4RePjVASl585WQMXe5UhoZCOsN/XLLL6zk4iVQJqMyjctwNLK7MwCpRWIUQdkyh95WUclhVzFnTOOWUU5g/fz4DBgzggQceoF+/fmzatGl/L2uvUpAmftRmV5g5bRMjhi1J6Cbl84b568/VTJ+6kfnztrBhfXGSkA2FdObP3crChdt4643pzJubh8djL3+JGkE4bFQYs1pZKlt2Jtq3uyLuvKd7UqyXw6HS/ZjGtNqNLyCL/cfBJrdvvvlmhg0bxvz58+nZsyerVlXclelAxzAkZw34hpdfmJJWjui6JCenzA2+K81NRv2ynOaN3iLb/RKnnPRFWmvp+nXF5OZ6cDhMdaN+gwxeeLkPG9ZXzpoLkJlpx+lUOf/CDqxafxcfDj6dQUPOYMXaO9m0sTRli2qPx8Hy5QVpk3PL43BW/rUfzEhZ+cfOEEI0BU4HhlTm2nussAoziv1jYLGU8o09na8mk5uby/Dhwxk0aBCTJk2ic+fOjBhRcemhA5n+Z7bB6UoWAoYh2bixBC2FIPH7woz/azW/jlxOyK+jSCKFipPnDwZ1SoqDHNW1YdUvfg+oCiVzf9C+Qy5SShYv2sbcOXkJrsIoR3VtxJBPz6R+fQ8ejw2nU+XUvq356tvdq/hgsX84WOX22WefzdixY8nPz+e4445j1qxZ+3tJe40J49fw339bKwwn0nWD11+ZSiikEw7rO63NHI+UUFhoFt7fvj19q+vCwgDr1xUTChkoCgQDGvXqZ+ySnHz59VOZ89/NDBpyBrVquTnrnMM446y2ZGQ4aNW6Vkqvj88Xpm3bupUKp3K7bVxz3RGVXs9BjxSVe0BuNKwo8rip3ExvAQ9Ryb5YVWFh7QlcCfQRQsyNPAZUwbw1EiEEN910E7Nnz6Z58+acc8453Hrrrfh8lSuxdCBx0+1dadAwE5fbtMgJYdbk63Z0Y8b/uTql69zptDHr38289+YMDF0iUtQBjScQ0FFtCi1b5VQ4Lp4LLmx/wFhl92VSc//TD+WIjoPofcIXDOj7NW1avMu4v1YnjTvz7MNYtvpOps++gWWr7+SbH85PsNJYHBActHK7Z8+eTJkyBafTSa9evRgzZsz+XtJeYea/mxI8WKnQdcnggbO45YZRlQ4H2BMMA4qKgrz9xrRdautqtyk0bZad8tg113VJac0zDMmP3y9OK0MzM+243TZcLhuHd6zHRRdXfXOFGkvlY1jzo2FFkcfg6BRCiDOArVLKSu8a91hhlVJOklIKKWVnKeURkcdvezpvTaddu3ZMnTqVBx54gIEDB9KtW7cal5CVU8vFhOnX8NBjPTn2+Ka0P7weioQpf69n+qQNKeuRCiGZ9Pe6mHtJRk2raYSOENC0STb9+rfBZqucwB320xIyMtJ3kKpOVHERj7S0bFWLpx6bwOpVkeSE0hAFBX4uueCnlK47RRG0aFHroKgKUBM52OV2VP62bt2a008/naFDh+7vJVUZa1cX8sDdY/jq8/9Q1Z1/xfv9Gr+MXMbA92fuUteqPWHmv7seDjd3Th5j/1yV1E5108bStJ68f2dsSilD3R4bTz/Xi+YtcjAMybKl2zmi4yBuvP4Xxv+1ms2bShLGSyn5e8Ia3n5jOj8PWxKrMnBQIkFKUanHTugJnCWEWAN8i7l5/rKiE6q8rFVlOJBLpOwNxo4dy9VXX01+fj4vvvgi99xzD4qyf1qf7S02rCvmuM5DEorQG0h0JG6PHVUVCEVw9/09ePu16RQXBwFTYTUgrcJqtyuM++cqGjTI5LjuH1NUFKx0zJJFGU2aZJGXV5rSTXfFVZ34YNDp+2FV1ZOaWNZqZ9RUmV1UVMR5553HuHHjePnll3nwwQerba3WrXleFs/fRpPm2bRpl7ot8n/zttD/5K8IBnTCml7phkRZ2Q4Cfq1ayk5VFbRsWYtNm0oIhw00zSAj085d9/Tg1tuPoluXISkbBggBdXM95G9LPuZ0qvTv34bRo1ckxb86nSqGITniqIY88dSJdO3WiLMGfMfixdsIBXWcThtZWQ7GjL+CFi1q7a2XvVfZExnWtWWWnPbYkZUa67hxYqWuI4ToBTwgpTyjonE1Sys6QIlPyLr//vs57bTTalxC1rg/ViW5nBQEdgT9BxzC1z+ez/L1d3LagDZoWuWF5pVXd6Zlq1o0aJjBgDMOrZYC90Bg48aStDFlI39eyv7Y2FpY7G1ycnL47bffuPTSS3n44Ye566670JPSzfcvhiH5v7v+4ri2H3PbFb8y4JivuKDP9xQVJseNPnL/WLylYTTNQCBiOQDRmqe1ajlRU+QW+X0advv+VwfK22lsNoV69TysXLkDv1+LfTd4S8O8/spUDmn+XtpuflLCju3+pNflctk4+9zD+C2FsgpmbkQ4bPDv9E2cd/b3dGo/kP/mb8FbGiYcNigtDbFli5ebr99pFaYaSxVZWHeZ/f8JtQCgbt26DBs2jEGDBjF58uQDMiFLSpnWpeRy21BSWC5sNpU2h9bh+JOa43CotO+QS4MGGTGDqkCYH9Ly00YE8acfzaVV07e5+PwfGfr5/Cp8NRZRvN5wzOJtYVHTcDqdfPnll9x333289957XHzxxQQC6ZOI9jVffjSfH4cuIhTUKSkyayzP/XczD96U3OPh3+mJhg6BQI0orjPm3MB11x+BoZOQyOp22+h2dCOUSoQP7G2i+2KbTeB0qtx+Zzfy8lLXzTUTxSo2UOi6WSIxI8OOxxOtNNCeZ57rXal4q3DIoCDfn6TYRsMNioqqz+dknyExU6Qq86jslFJO2Jl1FSyFtVpxoCZkaZrBm09MoWvuINq73uWMI79ixj8bEsacdkYbjBQCwmZXuOBSs2tG/jYvXdsNYvOGkgT9NKq0inIVAxTMSAEjLPlj9Mp9Fu9Z3VEUgd1Rdbe2alMOqraFFgcfiqLw+uuv8/rrr/PTTz/Rt29fduzYsb+XBcDH78zG70vMdA+GdMaMWsmwbxYnZMFnZjpSzpHhcfD8k//w9mszENKUnVFDQIfDcyko8FJaEtp7L6KSRGW4pkmCQZ2335yxx3NqmuToHo2ZPvv6WEmsRo0zadx4z+tGHwTl1FNT+SoBVYqlsFZDogkBDz744AGRkPXMXRP47O25eCMCb/nCAm48aySL5mxlw9pixoxcyca1xQz5+mxcbhWXw4ZDVbErKg8/0ZM2beuwvcBPj05DWL+umEA4RTIWAiViLYg+ov8BaUtfHZxI7Da1Slx8TqfK+Re0j/URT0copPPtNwu44tJh3H3H78ydk7fH17aw2Nfcd999fPPNN0yfPp0TTjiB9evX7+8lUVyU6N0wIqmomm7wvzvGclTzQcz5dzMA199yJG534r3qctm44OL2/PidWZtdxP2nAHP+zWP50sJ98Er2H5P+WQdAVpYTMI1D73zQH4/HtluNXoSAzl0aHLRJp9IQlXpUNZbCWk1xOp2xDlmFhYX06NGjWnbIKtoRYPjQxQTKFY0O+MPcfNEvnNLpcx64/g8u6PU9rz82hWy3C0UKpAZOm8q7L8xgycJ8brn2F4oKd8/tLBBluqqltGIYZg1CRYFjj2u6R3MdcWRD3ni7LwDBoMbggbPoc+IX9DvlK777ZiGGIQmFdAb0/Yp77/yDkT8v49OP53LicZ/RtvV7/DmmZhdnt6h5XHLJJfz++++sX7+eY489lgULFuzX9fTq2zKmVMlyAs5bGqa4KMjV5/yMphk89GhPzjynLU6nSnaOE6dLpecJzfhjdPr78EAVmbuyIdc0yVWX/ZzwXK/eLRk/8WouvawjHQ7PxWZLP5/HYyMz0yyF6MmwU6uWi0FDdurBrplU1rq6FyysVpWAA4D8/HxuuOEGRowYwamnnsrnn39Oo0aN9veyAFg8dxtXnPxTkjtJw0AqiWFCupApwwI6d23AnP82x+KR9F0UoVGLw05Kth50CAEnn9KKvyesIRzevfu87WF1mDn3JnTdYEDfr5k7Ny/mnvR47Jx9zmGccFJz7rtnTNKmBczY5W9/OJ8+J7fao9dSnbCqBBwczJs3j/79++Pz+RgxYgQnnXTSflnHxnXFDDjmK7ylYQLB1EXwM7McfDrsbI49sRkAmzeVsGLZduo3zKB/76+SSkFFkRHZuRd0i2qHUGDlmrvIredJOlZSEuTffzcx9LN5jPx5GVKaoW5mcxQbo8dewYplBcyZnUfLVrU474J2MWvtgcgeVQloni0nP9C9UmPdd/9VpbLSCkw7AIh2yBo8eDD33nsvnTp14pNPPuGss86q8mtJKVk1vYCS/CCHHJNLVm7FN2XTVtmEU7jwDUHS1j2VsgqwYM5WFHuZxFSoOF5bUtZQoLzFwaIMKWH7dj+mJp/+fbLZRFIP8CjLlm7nxOM+5dbbj2b+vC0JsXQ+X5ifhy9h8uR1KZVVgIBf4+kn/q5RCqvFwUGXLl2YOnUq/fr1o2/fvnz55ZdceOGF+3wdTZpn89fcq/nsw7l89uEcCncke6KEIKFkYKPGWTRqnMVnQ+amzISP52CRoNKArVtKExTWcFjngXv/5Ouv/sMWSTp7+NGe1KnjZu6cPDocXo9LL+9E7douOnTI5axzDttfy69e7KcdjhUScIAghODmm2+OJWSdffbZVZ6QtW11KY+0HcWrfccz+Iop3N/8Z0Y8PQsj71OM5bdhrHsFGUxMpsrKcXLxDR1xeRL3PrvycTYMSU7tMsU4GlsVTaoqT3ll9WARuLvDwgXbOLRt7QrHpFNWo/w3fyuPPzouZdtGw5CsX1dxT/Dly7bvfKEWFtWQFi1aMHnyZLp168bFF1/MO++8s1/WkVvfwwNPHsfTb/TGk5HcpU/TDLr3bJL0/OpVO/D5Ku4olbIKSw3l7DO/o6SkTOF/7H/j+PbrBQQDZltarzfMG69OJSvLyXsfDuC2O44+aONU0yFl5R9VjaWwHmCU75DVtWvXKknIklLy1hl/s211KcFSDX+xRjhg8NvLC/nv+5FQ+Bds+Ry54ExkSaJr8H+vncjt/9edOvXc2GwKXbo3oPeAVqjlOk+VVz5F3EP6iSULKBBLrLJFarWmIj5xwHwRe/gm1ECCQZ1FCwv2aA5dlxQU+NPGeO2sm07LVrX26PoWFvuTOnXqMHbsWM4++2zuvvtuHnroof2WS3D2Re04qkcjPJF4SptdweW28erAvikVWbfLhlNVsMfLyTgSZOhBID/zt/li5Q9DIZ3PPpkX66oYxefTePrJCfthdQcQlW/NWqVYCusBiNPp5NVXX+XPP/+kqKiIHj168MYbb+yREN20qJj8dV5kuSlCATtjv4t2tdDA8CFX/y+hkLyiCG56sBtTN97IQt8dfD/pYp5/72Rq1XHjiiihNrtCptsea6EXX2cVwFsUprbLhdOmxp6PrwKgVmCztZTWvY8QpGzooKqiwuQHt9vG40+duDeXZmGx13G73fz444/ceuutvPrqq1x11VWEQnu/DJSUkuXzClg8cxuaZmCzKXw16jw++OJ0Lr++Ezff05W7H+7BS49MpLX7LXp1+JQ/Rqwgf6uPay74mXdfnAGaQEhRoQwtu2Dcvwdg5RWHQ8FRQUk/XZcxhbWkJJS2WcqG9SVsySvdK2s88Klc0wCrcYBFAuU7ZPXv35/Nmzfv1ly+olDa8h6+YnfiE6E80PIrnK9hk0zGzruKOx/tzol9W3DFTZ0ZPfNK2h5WN0lZjf67Y3sALWyUlaqKG7ezD6pIOsuiKkkVNiAEPPtCn7Sun4aNMhk45HT6D2izl1dnYbH3UVWV999/n+eff56vvvqKM844g5KSkp2fuItIKVkweQufPTObM5p/yU0njeD2vqM4o9mXzPhrA6qqcMrprXn5g1Np2jSbD1/5ly2bTGPD2pVF3HrJKHq0+Ygxv6xEGqZxQMFUWtMRazAAB5ySGiUr2wEIQqGKDTfr1xUBULu2K6kEWBQh4MsvKt+IRkrJypU72Lyp6j8P1RKrDqvF7pCbmxvrkDVx4kQ6derEyJEjd3meFkfWTlkE2e4M07X3onLPSgq3CUZ9sIQfX1nAqnmpYxRr1XFx20Pd+fyXc3nyjV60OCSHy27olFg/NWHWPZOUaW+PA1QAVxfsdiXtZqagwMd7H/bH5bLh8dhxu204nSovvNyHZavu4Nzz2ieMX758O6+8NJnnn53I/Hlb9sXyLSyqDCEEjz76KJ9++injxo3jpJNOIi+v6moOlxYGue3okTwy4A8GPTuTgjw/fq+GryRMYX6Ah84bw7ZNZucnKSWvPjE5qamANEALlgnzMre/QE0jC2Xkv+hZ6fIHqjMBv0YotPO2uuGwwcqVO3j1pck0bpK6eYCU8PRT/9DtiMH8PWFNhfP9PWEN7dt8QM/uH9O5w0BOPukLNqyvOK7/gEaC1JVKPaoaS2GtAZTvkLU7CVkOt40r3uuKw6MiFBF5TlK3UREnnRcfs2pjxswBXH/YH3zy8CyGPjGHB08Yzfu3T0vbb76kKMhjN/xFt+xBvHrXJFJ0aDVfR1SNLTdNtPRKZaysMQvBAerSqm5kZdm54aajUsavSgm6JrnoksNZsORWXnzlZJ57sQ9z/ruZO+5KLnsy8P2Z9Oz+CS89P5lXX5rCqb2H8sRj4/fFy7CwqFKuueYafvnlF5YuXcqxxx7LsmXLqmTe9+6exppFhZR6wyk9F4ZuMPpL81p//bKKkqLUYQnplE0VAZJYfkA0TCCq1EatrLHa1geQ1up0qpUaJwQc2XEQzz87icWLKvAUSrNKykXn/Zi2EcratYVcfP5PbNpUgs+nEQzqzJ61mdP7fZ22TXlNwAoJsNhjyidk7WqHrOOvas3//j6Fnle1pONpjbjw5aN4YsR63JkClExQPARoy2sPdyHk1wn5dXRNEvTpjP9yFXPHJocjSCm5sd8Ifvt2GaGgjjTAKZMFS9S6qkY/krLsfMz/Jbqt0hBr4xr3sNh9pBQU7kjupR1l2TIzoat+gwyuvf4Ibrz5KJo2y04at3FDMY8/Np5AQEPTDAxD4vdrDPpgVuzLQErJyJ+Xcmb/b+h1/Ge8/eY0vN793y7SwiIV/fv3Z8KECXi9Xo477jimTZu2R/NJKfn7hzVoISOttykUNMjfbBoiXn50Utq5UnmwREQiRhOwlMi/9rjR8SUDUxkPqjOVsa6CmVS1KwQCGq++NCXlsU8+mptU1lHXJdu2+pgyef93SdsryEomXFlJVxY7Iz4hK9oha1cSslp2rcP1nxzD/aN7ccod7XB3egXRcRSi1XOIwz5n/ua3U4YOBLwa479K7qYyf+p66rKAHu3W43KY5VUcUsGlqkmWVoHAjoILFTsKQpoKrIhYBZBE+mBXHK8adX9FH8ABJXirE6WlIX78YXHa45MmrqvUPL+PXomSwrQeDOqMGL4UgMf+N56bbhjF3xPWMntWHi88O4lTeg1N6JVuYVGdOProo5kyZQq1atWiT58+/PLLL7s9l+mxMIWrmuar2Z1p4+iTm2IYkjXLiyqUg7ZyR6NKsBAiTikts6xGf4+NKzsxfpJqK0vTJVBVllq1XShKilA1CUuWpLbErl1bGGt4U55NG2tuPKtV1sqiSimfkNWvX7/dTsgSruaIOv2Rnk5sWFBIyJd6J1s+JEBumU3b1Vfzwg3jeemWiYx79wdO7rYWgcCpqRx7dBPq5LhAgiLLXFEKCjYUXNhQ4352ouJARZFl5a7sCHbmCLIqCOwZ6QQyVL49oqqKlKEgimJWkNi4oZjBA2fhi6v16vdrrF5dyI/fl4+htrCoPrRp04YpU6Zw+OGHc8455zBkyJDdmkdRBJ1PbIgQZS77eJnl8tho2yWX4/o3QwjJVaev4q7z5mFXk+/PaKKVAwUHCradbvLjfy5TXhNkZ3nFtZqxpwrrGWceisOR+ttkyxYv55/zPRP/WZvw/IkntcDjSV0Xt1v3xnu0nuqKxAoJsNgLxCdkTZo0ic6dO1faAuDfHmTMPf/ybrNhvNdqOEP7j+OJ3GFMfXwxh0gHOUbyR6dL77J2sTLsxfj7fhzCT6Y7TKYnjMep89yNk8nN8aEIwaLp29AKddwxO2rZBzz6s0KiAC27QGIyQUUCuTJVBix2jzp1ktscpmLA6YemtMzb7CrnX9CeaVM3pvyy8HnD/PH7yj1dpoXFXqV+/fqMHz+e0047jRtvvJGnnnoqbUx/RdzzwXFk1nbgdKu4UMly2HHYFA7tUpc7XurBu3+cjqoqyJlP8dClM+hz1EYcNiPmdYr+G71yvIx07HRrn0zU+lr+AVRLpXV3UVXBRZccznU3HpEywbS4KMiff6zignN/YOjn82LPX3JZRxo2zMARFz/r8dg59/x2tG5dccOWAxYr6cpibxFNyJo1axZNmzblrLPO4rbbbktKyAoV+ln74wJWDp1L0bIChp44hnmfrsKXH8SbF2DjhC04vSGQEhuC+tjINpTYzttlV2nUIhNvYYhNy4rJn/UXg4Z25doHL+HOJ89l/NRDkBJ+n9KKwmI3UsqYIE2nakZjrlLFowqRqNwqkXAClah1oeyRMHsNErLVgZKSYKVc9vUbZPDuB2Y1Abfbhsul4nLZ+L/HT6Bd+1zq1fek/NPYbIJGjTKrfuEWFlVMZmYmI0aM4JprruHpp5/mpptuQtN2LZyladscPltyAdc8cxR9rzqU217pwU/LL+WyGw6nzqYJeIdcQWDIAOS637ErITq23k7HVgU47FpMjsYrq5T72VZFX/kxeVxD5KmuS666/Gfq1fOkDAuI4vdpPPLQX7F4WY/HzvhJ13D7HUfTqnUtDu9Yjxde7sOHg08HYPSvyznt5C/pcvhA7r3rdzZuqCHVA/ZTWSuxO7vAPaVbt25y5syZOx9oUWWE87ez8Yvvef7zIQyZP4u2TZvx7fDhHNmtK3kTVjPzvtEgQOqSohIHeflZ6KHEz4YhoSgsCEc+iBqS9YSxIXA5Vbqc3YQZIzcgbAKvXkJYV9A0c+fptIdx1SphVYELo5yCqmHgQ0vKkJJSYggZS7gCCKNjiPSfWT3muyqbLFplQAIp0xmszKzdxm5XOPa4poz6/bKkY1JKPv14Lm+8No38fB9HHdWQex84lnVriwiHdfqf3oYWLWoBoOsGHdt9yKaNJQmxT263jX+mXMNh7XL30SvaOUKIWVLKbvt7HfsSS2ZXHikljz/+OM8//zxnnHEG3333HR5P5TwR5dm8soQHjvuNAcfOYMDxC3DYdeyZQdRsPyKie/qDKq9+cxTDJ7YmELIR1kVaPTKMTlgkuzoMJEbkrJ1lO8RLUAMOOvmZmelg7IQr6XB4vQrHvfv2DJ5/ZmKsNa7NppCd7WTqv9fRqHHqclr7ij2RYUc2ri3H39irUmNrP/NzlcpKS2E9CNBLSln70DPopV6QksmbN/Dw1HEUhoI899zzHPa9BxkoE1NbCjLYUZwsYKWEUk3gN6I7eclmdJwZKnVbZbJxVQkhv04AnbDQkeWURgPYLgLJiikSL+FE4ScjQlTIBEuBhm4qpSmEZHwdwcocS3ouleCVaZ63iOHx2Pn1j8vo2q1RwvMvPDeRd96ckdDL3OOxM3bClXTsVD9pnpUrd3DReT+wcUNJJOZV8P7A/px9bru9/hp2BUthtagMH374Ibfffjvdu3dn1KhR5Oam3nRJKdk4ZRsFS4up2y4btbaTYQ/MY8U/29isBdAI0Pe4ZYydchihsI2mjXZw0w3j6NhxY8I8obDC4JEdeeenLpGNe7nrIAljoGEkyjQJjWr7adW4kMmL68cMEpUhdp2DTEZedEkHnnz6JJo1z0l53OsNcUjzd5IqEtjtCjfcfBQvv3rKvlhmWvZYYb2+d6XG1n5ueJXKytRtHixqFEXjJ2MEgrG0vZ6NmjJywIU8NuMfHvrfwxyZ3Z47619KHbt589ltOkIYSJnoPpJAfLqVtAladqpN39vbMui26SghyEAhILQEZRWisVAkxFfFH8vAbiqtcfJPltWziqGioFMu6Ws3lcrommJWV5kYelCNE2KrFVJK5s/bkqCw+nxh3n5zelJRc78/zEsvTOLLb85LmueQQ2ozc+6NLFmcT2lpmC5HNEibBGFhUd259dZbadSoEZdeeik9e/bk999/p1WrVgljAoUhvj/9L3YsL0EapgDy+Qw2+gUI0/OU4Tb4c3I7QmHz63r9pjo8+9LZvPjs97RuVZa9bqDy44TUXeWiVtGoFTVesAlg644MtuzwABJFGBgk6bQpUYhYWQ+yjf2wHxcz5vdVTJlxXcoyfkuXFKCmqF0dDhtMGLdmH6xwL7KXKgBUBiuG9SDAv3wlMhxOeK6Oy80Hfc/i+cvuYFHpCu5e/TIzSv4DICczGMnoLvtURsqhEu16Z3er3Dj8BF6cNYCmHWvhCJmZqNEEqHQSzi1tKY9JJDaUWOZ/tLyVUm4us/SVmtwYIFJpIPV1y5VpKUf0+bL6rWW9uA7kVoX7CkURrFhewGefzCVvs9l/e/26IlQldbOBuXPSd7gSQtC+Qz2O7t7YUlYtDnjOOeccxo4dy7Zt2zj22GOZM2dOwvFxD84if1EhYa+G5tfRfDo2KalrM7iw/0yEkPj8zpiyGiUcVhk2/Gh0XeAP2Cj2ZbG1xbNszs9ElQqqTK6tqmFENulKQtk/JaIGRBOsRNzGPfpI1/nqYE1o1TRJaWmIV19OXZ+1QYOMtHVhUym4BxJWlQCLvYqjcSOwJX/5C0Ny1//u4Y22D5Nrq8ULG4YwcPP3aCJIqyYFuJ1hQCKQuN0BNEcIu1ul8RG1uOannnQ4vTGLf93E4LMmxqVHQaYsVwEwTrH0YMMVVTjjiLrm49u2xn4q97mPZrzaI/Va7REBrEZ+Ti7BkqiAlv/Qp0pOiP/5IDIc7DJCmNbUIR/N4ZEH/6Jzh4EMGTybRo2zkgpqR2lzaA3NnrWwSEHPnj2ZNGkSDoeDE088kT///BMwPRNLf1qLUS5XQBGQZZe0bl5Am0Y7Us4ppcKa1fV5+tX+3PbohVx065VcdvIqcnCQiY0cHGRIO267jbbt6sSU06gsiyquNpRIM5ZEuSdk/MY9XTPtsvEH48Ze0wwm/ZO6DnWTptn0PL5Z0qbb7bFx97099sXy9iLCqhJgsfeodepJCLVc9IdNxdG8MZkdD+GMFy/jtcMe4JzcPvxeOJkH1rzGZtZwSIsC2h+yhXZtttCm+XZOOXY9D/3Rgftnn0a7fo2Y9dUahl48he1bAwlT21GoJW0ICVnSRg7mI1vaUKUgU9qxy3ICMs3a0+3gy5oDKDHhG/3XHrUZxIUXlC/JUt5ikO5GqLh6oUW0QLTfp+HzhQkENB59eBxbt3q5/MpOuN3JUUcL/9u20/7cFhY1iQ4dOjB16lRatWrFgAED+PLLLwEwtNRaXtTBdWXv1E07FGFQ7FeZMrcl6zfVpjSoEYrUS46ql5lOO7ffdzTrV5mZ6Q5U3NhwRRRVhURlVCG+aUC6yi0W8TRpmj556vMvz6FXnxY4nSqZmXays528/mZfTjypxT5c4V5AWhZWi72IPbcOTR+9G0ezxpFK7SoZR3ZmnXIyrzUcxtBL51PobM3/bnqMwTe9idfwc9+Kd/h5298IRUdVzACl0u0Oso9oDZjWgV8fmEfYpycGQktQJLilQo60x+qoRoViFmZIgBMVIQWKFNilQoa0ky0dZEo7aoIyizmugtdXXuSbSqsaKXMlYh2Wymq7mquJPy+dDSFdi0SL9GiazrAfF/Pam3258ZajksrEbNni5eLzf2L58u37aYUWFvueJk2aMHHiRI4//niuvPJKXnnlFeq0clFegklA2sNIoGFtHyd1XIdNLdf+UwrySsymK6rETFktJ77CQYMvXp9LOGQkhDqlSkyNSj9p/rJbmmlNs7Ke2rdVhcc9Hjv3PnBs2uM5OS5+HH4RC5fexl9/X8Wq9XdxxVWdq3qZ+4f9VNbKUlhrKIv+3sJjPf/gmlrf89ARv7JgoY0WLz7GIYNe45CP3mDeiiOY/OYKGmevISe7iA1zvPzx6mYyN3TknXYP0y2rHUM2j+TJ1R+xPWzu0A3FiS3TDUCgKIxvexCnYtDUBq1skuauEPVVSR1FklFOSYz/ORMbtaWTeoaTDGnDiYIDFScqzTI1Lj9yEy6bKaBjLQNlua4rsTlTE7XApnq+sjI1qqzWIBlcpaTqXAVmfNeWvFJUVXDpZR1xupLDUUIhjf89OJZvv1lQc2oTWljshJycHH7//XcuueQSHnnkEb5f/y4IM8kVQAiJI1MlUD8Xr8+FbsAlJy3hzO4ryPYEURWDOo0dhJ3meEfEa5RORmlhA1u52P6K5JktEg6QblC6c+MbvdQUgfnnmNUVHn/p1ZPp3aflTuep3yCD9h3q1aiYfMvCalFlLJywhRcHjGf51Hz8xWHWzi/k7csmMemb1ShuF8FSybzPV9O1w0LWbqjP9u1ZgEBKhZVjt7JlXTP+r8W13N7kAhZ5V3PHsteYVrQAR24moSLT/e/ItJGhGHgUsAlToSzxOwnrKrqhEkojtGKB/QjCkZJVUWHntuu8dN4crui6gY4NSnHZ9EhDgDJLqxpRXBWIzJOeWOa/NM8VEoxdSG+MWSMsP1hK0imsAEM/n0/n9gMZ/9ca7CnipzVN8tfY1dx31xiO6DiI5575Zy+u1MKi+uB0Ovnik885M/dkft8xnp94i8ysEjI9QWpn++l4vMrjy85AOeM+ho/oy8eDTqdgYRtu6FvAtyuO59P1F/PcyJOp49SxKzImU1PRJDtA39Y7Eu7VisSZgoITW0IugIz9VzEHU8x/Roada647Yn8vY78hjco9qhqrrFUNoLQoxJb1pTRskUlGloMvH5xNyJ/oQgr5dL58cA49L2lJ4ZpSatf1UlSSga4n+n8MQ8HrFwQMN/3rHsvhnta8tv5Lnlv7KbN9yyjstYMjHzgZ52HNcajE2m169UQxFW2VWl7IRUv4q5BUT/WEQ7fgtOs4bPDSgGX8s6oOP/7XgGVbMxN2VsE4p1bUuS9JbEuoY2BgxKysUcuqDUFYmu6zWEmWChBESngcDFJ4F1AUUFUFI1W/VSAQ0Fm7tojH/298QleyeHTdzLQFeO/tfznhxOac1Kvl3lqyhUW1IVwY5LrGF1KbbL7IH06x8QYPN74Jt/BQtKKQeUOWMuq+/9D8TgBKS91MHnsIzp81Wh8yFTlkNPceb1Z++XdDLX5ZlssOGakEI0AgcagGtx67hiXbPAmeJRsKoYhsVCiTl/Hbf1tEcoYwEFLEgmoro7RKJEpc9axYmEENossRDdA0A1uK0lVR8jaX8sxTfzP6txV4PHauv/FI7rqnR4XnHBBI9oq7vzJYCusBjK4bvHnvFEZ+uhSbXUEPG/S77FCW/LcNMMtDxQuhoq0BQn6dWi0yyPEUUVjkwTCSrV+K0NnhaUqmfR3NjIa81voehm79jeHbJvBf8QoeeiGfo847F6GUqaR6OankighDPU7Zk1JGhKlI2dW6Zd1SXHZTAbIp0KfNdr6e1TQSiZooKoMyUj1VEIuThTKBaf6bqIxHj6kIdKLW3WQhHK9ox36Of8ICwwCHQ1CuWloSui5RFImqCnQ9/dedzxfms4/nWQqrxUGBq64HoQrOqnMydWw5vJs3lEfXvsW9de6hjtfF6LtmoYUTpWTYpzP+yfnYT5qPCIeJepiPblKINGDU8nrgDOB0hmhRx8eFnTdTGLDxzbzGOKRCMFK/WiBwopilU4VASokqVDTKul2VJbBG4mMruWmXMalbVlRLYHZJrEmyc/bszdx71x+8+0H/lMeLigKceNyn5Of70TSDAvy8/MJk5szKY+g35+7j1VYtEoFh7B+l+wBX9Q9uPnluNqM+X0YooOMrCRMM6Iz4ZAlb9ADblABblUBCxxNXhg27S8Vdx0mD41pSp3YJipKq9JCgeEMBdOqIbgjsio3rGp7FMy1vxqv7uXfxa3w+/H2EvewMR7ngJYGgnrThjD4dOZTj0OhUv5Q2tX1kCwUlzkC3dnsmgXDZR1JK6NO8iCMblCQkDdhR8aCiIkxBGInRSiyHlTpzoHxsqxJxp0VDDMqHGUSrFFg3SiKqKnjgoeNwu20VhgaAqdwqiqBR40xq1XJis6U+oSRibbWwqOkoDpV2tx+N6rbRM6sb99W5hwJ9O8/nv8jG8CbCWmqJowV0gt7EjZ/DJunerBCnEJx+aAHvnfsfD/deSeu6PoYvrE9QN2WlGzVSKxtAxDwfQpjy0pZGXtojEjMaVlUR5ZNXE8IEks6V5f49cAj4db79egH523wpj3/5+XyKioJoWtkXnN+v8cfvK2tGsqmVdGWxq3z37gIC5ToJgenmlgLCSHaIIABOj8r5D7YgPH0E/t8+5LjbczikzTZsNoN4x7ii6NStU0Kb5htYMnxdQkeLIzMP4502D9A1sz2DFw7lY++LFGPWCcxQo72tyk5wKpKmbo1MFFxAr5bbuaRTHg08QTYVeagl7TSULurqDhQp+WdZfYKaih5ZTn5+JrVssHBLdtKHX0HBjZmwpcQJ30pTTkaWj4ZVEh5xYvjAk617BSnhpD4t8HjsO1VYwezw0r1HE1q1ro2WopRPRoadCy5qvxdWamGxZ5QUBdm8vgTD2LObP5y3mZIJ4/DOmIYRDNL2uiPo8tgJqDkZtHe255G6j2Bg8GLBS6wXC1POoQgDuy1Z5isCMh1harvMY4Yu8Ba76d/Ux/ltdlDLqcVkWEUu+qhCIMtZW6NVV2yxNgOVJ6EGdkJClyj374GF02Xjv/+2pmwQMGXKBvz+5L+Tza4wf276xikHBBKkISr1qGqskIADiMLtAb4aNJ/Jf60jHNQpLdqJRUpAQBg43CpXPliLHsobBMfroIXAPp7mXZ301WYxe/ah5G2pjaoYtGq1hS6dVlPqdzB71iG0zk0U0jm2TP7X9Fq+Wz+XYcEfeE/+j/OUmzhMHEmuU8eW4afI68Ll0DBsGusKMxAIWtX20zbXR4HPzqS1ddHiXApOFOoaLkqCYf6Y1obTu61mytpafDblUFRF4pYKbsArw2jltvgqAi2FFpkgnMthxrsmxq+WDx4wyp0RnSshPAAOVFm7xyiK4LYbf2P7dn+l2vQpCvz+24qUwt3jsXFUt0acf4GlsFpUH4oLgzx8/Z9MGrMWoQhyajl59sM+9BpQcbmj8kgp2fH1ULxTJwEgFJUdX39Bvbvvp9UFHSjeFGb2S7Nobm/G/9V9lDe2v8mnoZc4T7mDDsoxsXkUu6BRg8KkDaKuK/i9Tq7ttIVaWWG0sErJ9hykFLStFaJVVoiTm5Xw4r8NWVkaiSNI5aKXUWdV6uoo0RAqRZaFdGli53Gt0RCt2OtA7tcQgexsB8XFe+7NKSkOct7Z36EqCudd0I433j6NzEwHAG3b1sHhUJPknTQkzVvk7PG19zd7owJAZbAU1v2EphmM+XUFs//No3mLbM65qD3ZOc604/O3+Dir+9cU7wgSDJo3QQ4ObDvZ79qdCk9P6Yv9h/tA+ssOhAMoikbDFjonZYRIjNyUbC7IYrOmsHBLFu0blKAIcxevG4JgyE6XcD8Oa3c8X219nS+2vMax6skMsF0OvgzqZ3pp22IbW7ZnYXgzCOrQvl4pDlUyf3M2WtLOS2ADjqrnp2t9H4uXNeGzWY0JGwrhuPKCGdgpkqHYUhUEqlBQkehSJgnAeGU2XmhGwwsUKTFEmbKaJFRjq4skEsSUYJmovB6ESqumGaxes6PSPaUVRSAESeNtNoU77+7OI/93PKpa8WfZ7w/z4/eLmPj3OlodUpurr+lC4ybpC3fvDF03GDJ4DoMHzqK0JES/AW149LHjadAwc7fntKg53Hb+KOZN30wo0o96q1/j7stG893fF9KuS72EsUXbA/z04SJm/LmBhi2yuOSujrTrao4JzJuDb/oUogHfEvPfVS99yIqGV7NmYj4+v426Lo0sWZ87PI/zse9NftDfZoCyg6PphxDgUcIYQRXdEKiKRAgIBW2UFpmfVwVBcaENb4mHnDrFMa+TXQVVSq5sX8DTM+qbcrz8i5VlSqpAoKdIR41vKhBVmm0RJTeV0SBKqlou6YwJ+4KcWk78AY1wKPk1CmGGO6XyApVHStA1iY7O8J+WkJfnZeSvlwBw/Y1HMfCDWQkKq92u0Kp1bbod3ajqXsz+Yj/98apEYRVCfAKcAWyVUnasijlrMqUlIc7o/RXr1hThLQ3j8dh57vF/GPHXpbQ/vF7Kc95/cQY7CgJo4bKbzCs0sqUZSJqobJWR2yiDFy8dzovnFiZPami4sgN48+MLm0q8QRe/TTsMCawqyGSHz0HLOl4cqiTodbGp0E1IgmNJBtfaHmW07Xsmar+x0ljMZY47aOBtwYx5WebNj8AjwKWa1ygN2Uin4bWvV4JDlczakoGexp1gR0EqGpqhoKKgStCFmYBV3mOnoqALPSaEy96fsjhWXRoYyKSQAjN2NVlpjf4cVVyBpOuWm6jGYqTuvBpDCMjMdBAK63Tt2pgpk9cnjbHZFRo0zGT+vK18NXQ+fr/Guee34+RTWiX8TXbsCND7+M/YssWL1xvG6VR5583p/PzLxfQ4tulurf/2W0bz87DF+CJhNV9+MZ/ff1vBjDk3kJPj2q05DxQsmV0xa5YXMn9mXkxZjRIK6Hzy1hxe+bRv7LntW/1c3W0YRdsDhIMG8yZvYcw3Kzj10kN47KOTKJ30NzIYTJhnQ14tvh59JFIsRgtJFMXJ9mIHpn3OxUU8zkjlHX7VPifs2sY5GRegCAUt5GLRoha0bbMJmz1MaXEmCUJGKuiaIOBz4c4ou6Yi4JCcEHYUQtJACJm0edSRsaYtUpiJqUpE0iV4o0T8z+Yu1I658w9XQpNJlJ77nvXrStIek5LdCv0IBnWmT93AX2NX89GgWUz8ex0ejx2Px05RpBRk75NbMXDw6bsevlbNqAlJV58B/aporhrPGy9NZeXyHXhLzZ22zxemqDDIbdf8mvac8b+uJhzWEyriacKgiBBhDKSQ2O0KLqeZNmSzK7gybPQ6tyXb87WYSNMNQYnfFlOyCn12QlJFOFSEIlFcTpYVtASlLEN1h9/BnI21mbauNrML3DRrWMhJHTfStJYfb9DBcfqVXMT/8Ekf7wafYGJ4dETEld2YWwuy0HVB0xw/qkixs5XmMYCgrqQsN2VXJEfWL8UlRKTsivmfW9rJwIEn0qBVFQKbULAJBRc27NJUPk9pWcSHp63ik9NXckmHfFyqjM2SjvgjqVxkUHYTxeK00s5Ws6hIsDudKo8/dSLvfTiApSvv4MqrO5GRYU8apyqChQu30e+ULxkyeA5DP5/PlZcO5/qrR5pVJSK8+vJkNmwowes175lgUMfrDXPDdb8kjKssa9cW8tOPi2LKKphxtoWFAb74bN4uz3cA8hmWzE7L5g0lKQu9G4Zk3aqihOeGvjKXHfl+wsEyqSUljPl6JQ+c8wdSK+cWljB8fCdKwgqFIQ0vOtKQ2KRZ2cRAoOLkXO7jKE5hTOA3Pi0ZgiY1hICg30VeXh0KttZNEycoCAYcSc9qhoi1ZlUjbn0Fs9SfDbOqjBr9XQocUsEmTVnriJNqIlLbOioQhRCxxC1zjooVsqSartUsLyBNpb6dIiVcfvEwRv+6gpKSENu2+Sj1hjj3/Pas2XgPPwy7kLq5nqpd7P7gQI9hlVL+I4RoWRVzHQwM/24xoWCyeWrFsu1s2+qlXv2MhOcNQ+ILhQnEypKYdfJsKOhC4iOMy2Vj6OTz+fePjcz5ZzPN2+Zw/m2HM/jxmWzNV1mxOZuZq3P5YWprQrqKx6Fx5YnLKCr2MHHJIVx1liS4ZStzV7hYuro2YSOIIgQZUsUVJ6wkks1+O80FbNyegSZNZ3or0YVr5SuMZhC/6V+yVMznHOVWskRtALYUZFGvTikd6pWyeFsWQU1iRONgpPlBLPA5yXJpdMz1MWNzFqFyH3gpYbPPTlCLy2cVZWWyDJRIsRZiQlBB4BA27uqxjr5tduCJlM1qXTvAsY1LuHdsy2T3WOT8qCUguiFWIrGtRpzAjSqtasKpUUmeet7qanWtX9/D1q2ps153Bbtd4dC2dbn/wWNj1oTzLmjPC89NIhTSCUe8BDa7QouWOXzx6VzC4bJvLa83zG+/reDvCWvp1bslACOGLU0Z/7olz8uG9cU0a75rcWFzZ+fhcKgEA4lz+v0aE/9ex51399il+Q40LJldMYd1yk36bAA4nCrdT2yS8NzkX9ehh1NrXfMm5rH+wiNp4FiODJlxkwVFbjaVusykpogsKI7UQa2TsN1VOU1cTya1+Sf0AyVFxdySfTtO4WbbthyEPUxORoCcrAA2NfH65Y14YV0wfZNpjVVQsCGJqxcAmHIrjE4o6pMqN4cT1SxxFUe9TD93nbiOE1oVUhJU+fzfxnw7rz6SimtcC+LCg6qpPNxVgkENm01JsFz7fRo/D1vCcy/0jsW31ghqegyrEOIm4CaA5s2b76vLVktUJc0fW8qkvutSSh64+Q/ytnrLnsOsjSek6fa2qQqHHl6Xtp1yadspl8sf6BIb26R1FqpN8Ng3PQjogqBmqlbFfgeDx7bHZtgQUuX1z0HIxrikWfNUCjO+0yd1sqRKrUjogYFgfaGbdnX8EStt2Xo9Ipvz5P3M4y/+kkP5UH+Is5VbOEzpCggWLG+ImunnqHolrCp0s9XnAGnWBLQBk1Y0oF6ndbTKCtAh18eifA8hQ4k54/1S4it144yziRrSLG6NKGscoMS3cQXqZwbof+h2nLYySZJX6uDxv1uaYQUYZTGwsnw8q1mnMBo2oETcZFo5K0GlY7KqsXC+4urOdOxUn+uuGrlH8/Tu05IPBg3glxHLWLBgK60Pqc3Z5xzGhIlXc93VI/h7wjrAbBu5aGF+yjl83jAjRyyNKawud2pRJaXE5dp1MdaseQ5GirqwdrvCIW3q7PJ8NZGDWWbXyXVz+a2d+HbwAvwRK7yigCfTzlV3HJEwNqeui/UrUrcXNqRklbc+zdq0JbRyOTIYZN6qBkkd9CRQhCRHSpQ4TVEIwfGcTxZ1GK19xBtFL3FTxn1ovvoY0kVhUQZOm8Ru06lbu5TaOaUoisTuDJoxloYps1cVOvlpWW7kWhJFJPuBDGmgRZOu0lYRKKuIXdsdYti188h2athUqJcZ5uE+a2hbz8cTY1umfW8Tql+nSPyqzjKyIlwuW8rKAE6XjSVL8mtUbHxVJV0JIVzAP4ATUx/9UUr5ZLrx+0xhlVIOBgYDdOvWrZo5AfYtF1/Vkfdem0EgUPbhVhTB4Z3rU6eum9kzN7NmVSGHd6rHB6//y09DF6ecRxMGOR4nDRtn8uygPkz7ez2KIli3qoic2i5OPK0FZ9/Yns9emoNXS25kGtbNYtEuJEiwSyU2b3zlkUI0HFLgRI1kkQqCYdXcgZcvDyUUOspTqEt7xvIu3xiv0k2eSl/lchzCyfpCNxSabpFouozELNei+F1Mmn8IDT1hjsnw06Z5McuLXdiExK/BwiLTKlE+OcqJgl/qCAHuuI+0HinM0rmBF10mLva1qU3whRUkZoFsIkprVFlNVeYqPhZWJTHRYH8mEVQVw39awpLF+SmToyqLy6Xy7of96Xfq1+RtLqW0NERGpp3H/zeeH4ZfyIzpmyo1j6oKMjPKLBLX33AkTz/1d0x5iI454siGSR6JynDkUQ1p1bo2SxbnJ9RKtNtVbrzlqF2eryZSE2S2lMnx6ZXlmnuO5PsvF0HAzO42FEGtpm7cGYlfm5fc04knrxqHFjaS5IbNrlC3cQb1zrmX4tnzGXXPMP5d1jjtNUNAqujpLqI3OUo2P+pv81rx81yuPMwhnvqoimlNNQyVLfnZrMrLYb1fQdo1VI+fbJfGKh+s9dpjiqBAAWkkKYaxuyD6fGzzTsoW1Zd23YzHoRPfednjMDi/yxben9qEzd7kEKB4UsrM6JMHoNJ6+hmHMnzYkqQGKaGgTouWtfbPovYKoiqrBASBPlLKUiGEHZgkhBgtpZyWarBVJWA/cMf93Zk4bi0L5pk13JwuG26PjVfe7cupPYeybGkBiiLQgjqEyzo3lUeogtzWGWTWcnDmcV9FgvJNK63bY8PuULn0usMpIoQnzZ86emtFE5EiNfjLXQi2KWEaGmYIfqOsANmeYMoPrZSgI6hDU87nWWbzHdPlr6zRF3G2cjtuWhJfUMQgUvNPKmga+EoVtvltdEKQZbdzbosdFIcU3llYJ+X7EHXbK5iZr/HHVRQ0DIoD9iQFbN7WDKKVY82i2So2QEvhyIq5zOIqEcSvorytIlZQ6wATvHmbS1m7pnC3lVUwLUK33jiKNat3xDJtvaVhAn6NG64dWamarQAOh8qll5flAt10a1emTd3AH7+vNCsOKILcXA+fDT17t9c58tdLuOG6kUz6x9zo1W+QwYeDB9C6de3dmtOieiCl5JMhc3nphUlsyfPSvEUOzzzfi/PO37WyaQ/e8icFRf4y74sOSxbm8+T943lt0GmxcX3Ob8Xsf9rx04eLEteBxFca5v+u/4vL53Xmxv91ZeTMhciQRE0p02UkgVRJeeww9Qiu5DG+0V/jU+NJbjLup7XaJjZCEYIMG7gUgTfkIBiy43OG6JoVprUq8RqwJGCwQ49zyccpp/FhTjKaSBVBxexaqMVZX49uXojLliwsQppCh/o+tq7OSR1uRVziVXkZWQ23RUJAg4YZdO7cgFatavPJx3NiYU1RTj6lFQ8/2pNff12esKl2ulSOP7E5LWuSwip336CRNJWZgFAa+dUeeaSd3VJY9wNut50Rf13K1IkbmDtrM02b53DaGYdw67WjWLRgaywz1QyMV9LqPLouWbwg3qVqYEcBg0hCV5iPX50Tc+OkDs83s0M1DKQws/BTIYGQMMhQ4MjGhbidBvVqedlW6MGIWGZlROhFb1cVB8dwJU3ozB98yGfGE5zApbTlNECNlIZKXJlEENRhm99GQ0USCCv8tiYHj2FHF2H0NK1W0qVOKQhmb8wmqCm4bQZK5OU5VIOAlqpBbOWJJljF5R8kHDNIcaC8cK5GCm0qd9buzDHur7VJz+u6ZNnS7djtFaek2WwKqip47sXetO9QL+H5L74+lyWL85k1cxNNmmZz4kktkkJodoXceh5+/uUSduwI4PeFadQ484DP4LWAIYNn8/ijE/D5zAS9dWuLuPXGX3HYVc44q22l5ggGNSb+tQ69XHkjXZd8+9lCepzQlAuvOBwwNz/uXDshh44tpMQktoE0E2K98OXb89m4sphtjhC6ZtDIcJW79SU6sJYwzaSdDJSE0AApoSikUodDuU55mq/li3zge5FrPXdyuP2IhJncqsSvKziBzlk6GQ5oKASGlByRoTCmyGBlUEFHYkgZ6fRnJl4VE0ZIYsqqKKe0RttaA6zZ7ubIJsUJFlYAuyrZWORElQqKkOZ1UrzH0Uos8RUJo+92gkK0n25Ju13BZlNwOFRKikOM+WNVWu/TpEnraNwki59+voi77/id1asKUVXBBRd24LU3T933i9+LSEDqlU4tzhVCzIz7fXDEcxNDCKECs4A2wPtSyunpJhO7k2GbNIkQ3wC9gFxgC/CklPLjdOO7desmZ86cme5wjaS4OMh3Xy1g9K8rKC0J0fmIBlx745Ec3tH8Ug6FdFrkvplQG06VZZVD1bQqWTKO6A5dmkX5BQI14tJPmENASOqRrE5TaNWSaQLDJTR1Glx2xAYUqaCFHOi6wuqtGazJq42QAg1BfGt5gUTFFEx+ihnDIFYxi5Z0pr+4GUPWibjVk19XLYdGplNjmVdFk2bM6HYRJCyS46skEj96yjhwAwMdSdMcH0+dspI6Lo1VO1x8+V89Jq3PRsZ/KSATWtmWv4aRUlmWNK9XwoptqeOTEhO0EgVzuRDggx6nU+XSyzryxNMnkVuv+mbTCiFmSSm77e917Ak1VWZLKWnd/B0K8v1Jxw5rV5d/59xYqXn8/jDt6r6X5N6N4nSpzFhxI3Ujn9N+7b5g/apiM7QKhUg2aBJmjoAgw1DJlWbdbUUIDGngw0AAHhRyUcmMSGsN2CENsjDzCzJVCNt28GX4VTYaa7nIfS3HOXoBZszqRr9KiWYmoB5aO0BdT+JGNGxIhmwzUlo/A2j4hY5I49XTMAhGqry0ruvlu6vn4naUfWcFNcGCzZlc8VXnWDUAAzM2NhXxpQKjvyfEuLJ/ZKUnw84XX57Dt98sYMTwpUkW1fJkZNiZPOO6mHemuDiIy2VLWWmiOrAnMqxTbj054vTzKjX2kC8GV/o6QohawHDgTinlglRjqqpKwKVVMU9NZeOGYvr0/JyCfH9MAP47fRNffDKPp1/sxa13HE04rCeVCSpTniQG0VJOJhUprzoyqS+0LiRBqWOPRWxKQpE4JkWawlXHHOMk9U12QeeNZDjNPPmgTefHxQ1YX+xCVYI01l1JttmosgrgJpuzeID5jOUfhvK5fIRTuYlcUn2WJT5dsLHURjQqzAZ4UCki2QooEIlqpoxXDgUaBuuK3Fz9U3scQmBIM9vfDQSlgS5knOBMNorGtylMRatcL6sLPOgpatOlt4+bAQkJHV+qmcV1X+J0qjRtls3Lr5+C211x7JvFnlNTZXYgoFG4I5Dy2Jo1hZWex+22c1SPRvw7JXXMtaoqjP1tFRdfbYatqLbIvS8quo0lfjQzfEkx8EodJwoZhhpLgnJKU5blY5BPYkc+NxKnXUH1KIjS2tyR8Sif+d/lW//HFBk7OM1xDgaC0jjt0GNPVktVIWhkhw3hpEO4sIGEoEjtzI9/XasLMrh72OE83X8ZtT0hFAGTVtXm0V/bRl5t5JxUyQ4Jc4qknxO/32TCfPsCQzf438N/sXLl9p3WmwZQVEHTptmx37Oz0zcBOuCRVZd0lTCtlIVCiPGY5fZSKqwHS8nI/cpjD48jf5svabceDhv834Pj2LSxmIwMBx06lmsaUKadYghJSOgEhU4QHVnB7RsraB9xx0T/CwiNQhFkuwhSLELowjCtq3FWxlKhEY6bP8sZ4upua3j1zP9o0XgH7gwfQkh+XVGPdUUuNEMhiGC70CLWRBlbQ7I3XNCFU7mMF3BRhxG8zhQ+QSNQbhwU6TJibyg71y1t5W3E2IB2Tmhqjzwvy+oK2lCwo5CBPVJRQUWXSmQPb/7njDQfiEaNpbJjx5efSUYwa1UuNmXXxGl81y0i6z7YqJvrpnnzHFq0zOG2O45m/MSrLWXVYo9wuWzUqeNOeWxXY5NfH3waTlcaC1m5OL7zru0Qq2RRZmYwy0QF0QihmQYIYSqgQXSCQiMg9AT5KzC9SQF0NIyyxiUCEAa608/i4iAbdZ0iXwbXOe+lu/0ERgeH8bX/E1aVCqKNpDPsOm57smDRJWQYDjKi2nU5nKgpn5eRsIVovoNAMG1NbU77sDsDBh1Nz3d6cOewDpQEzfchOk6VpoxNRWW8hpX3LVYdgYDO8mWVU1Y9HjuPPXFCtbWmVj1m0lVlHjudSYh6EcsqQgg3cCqwJN14K4Z1HzBm9Kq0xYilhD49v+Dvadfw8luncv6A7xKCttPdqamsqPGnhCL2WQ0DD7YyFTRySrQclI1o16eybKJiNOwouIXk3hNXkO3SiBoQ7A4NHR9L8z3osmy/U6To+KVBhqHiwix+7UHgiHMtSSQhQKEx/XmG+XzPf/zKZhbRizuoRwsQUMcVpjSQvJeyoVDLcFCshBECshXJ0VmCojC0cAi86EwplQRk4k5dIs0OL2kqA6pxNQAUJFq5t92A5OQA8wUhAH/AQYcmhSzZ5kZRwBssC+BIFd9a9neKuL7iB1QU83oAIQTk5nooLAqkbIEIEPBrDPn0LE4+pdU+Xp1FTUUIwWNPnsgjD41NkKNut42nnu0FQElJkEn/rMPuUDnhxOY4nam/BlsfWpuvRp3HpQN+SvoM+/1hZs3cSKs2tRjx/VJWLt1O3cYeCjb70EMGUpME4rxBEtAxw69swlT3NGmmnIaxxwrzFyihmDdHAi6pUEvaUQTk1i1hSb4ZelQKlOoGbq+d47gVQV2maz+zhRJO5i48OHErZve/8mHemoTCsIIU0VTVcu8hZoMBv9SiT8TQI/Iq2hzAiDjxt5U6k8Kp4uW+HRUhDTSR3pO1U+Jjq6oJqiq46JIO3HKb6SmUUjJj+iZmTNtAg4aZnHFWWzyefbMJLyoK8OP3i1m1cjtHdW3MmWe33WtKdBVaWBsBn0fiWBXgeynlqHSDLYV1H+BwqLEEgFRsyfPSvtX7qKpCpy71mTcrb6d7ykiOZeTnsrFOp0p2joOtW32mQiTALzUznCChp55ZI7AEDScGbuxl8wizzuvRTYpw2/WYsgqmIrK51BEpE5VISEiCahgtUh6qvuEgFxUiSqsfiT8yv4qdI+XlNKQzU/iQkTzOcVxKV9mPEr/dtC2kiFW1IciRKp0yJCuDkulFAl2CKkAVKjZCSe9cbIeeQukUJAUUJI2LVhtIeD5yihL5olm+OYfGOQHq1ylmxqrcyFBBzPWf9G5Fzy8T6lGSysscIEQTEtxuM3br6+/P4+zTv0ursHq9YYb9uNhSWC2qlGuvPwKHQ+WFZyeyaVMJrVrX4tnne9N/QBu+/3Yhd942GltEqCmK4Nsfz6fn8anrzB5zQjNuubcbH70zm1BIx9BN35EmJV9/toChQ+ZjQ4kohoIMt53bH+7GP7+v5b9ZW2PzRG9lDYmBblofhSAsIV8J0MBwExJGzAobJYCBX4TpWifIsoKyyibRf/wSshGcKC4hS9ZhLJ8ymufox4MYwUwySu00yAzHEmINYE6BE5sEKUxlNtXdaeZB2E3Dh5SokZAFnXAkkjaysU9tpE34TopuzM0ZzeuZyVup42hTIWKzVC90XfL9t4to0iSL+x48lovP/5GpUzaY1X+cNh68709G/3k5HdK0XK8qli7J59Q+XxIK6vh8YTIz7Tz3zD+M++dqateu2jbTUoLUq+bLSUo5HziysuOtkIBKMmvWZi696CeO7/EJD903ho0bipFS8u+MTXz+6TwmT1qXtkXkGWcfiqqm/wNLzExJTTOYN2dLrJtSKqI6ZxiDEHqk1mjZf68N7svWbb7Ijt6IFbxPmf0sQApJEINiQoTQCaETRidAmAmb3DwyqQkzNpclwKza4eLVqc1TJieZQsxMTpICtikhiiK/O1SNIqETFHFpSAIa0YnzeImWdGESQxnOy3gpwm5OmIRTNTg+B4p1KNYEWqSdoSYFQQOCaSSaWsFHPWbllCCkSLJICAROIWiQESK+/kt8fKpuKGzc4WLGynrEO7GiP+3s9k4VuxVrf1jdpHQapIRevZrzzHO9mL/4Vnoc05Qvvj4HpzP1Ll9RxG4V/bew2BmXX9mJhctuY0fpw8yefzOnn9mWlSt3cMeto/H7NUpKQpSUhCgqCnLhuT9QWhpKO9fDzxzPz39fjFBNmapFWmFrmrmJ1SNy35CSEl+ID9+fxbJF29POF72lo8ppGIM8xUeJCFEqwvjQysrrCdBsGk5bXGfAcuiY996Roi9ncy9bWcNwnqSYfNYVu5i/JYM1RU4W73Dyd56bUk3FiYJdKklNVgSRElZE5Z4Nl7BjR8WGgidi2Igqzq2ywqjCNEjsTJlQMLXb6LiKKuCke9+qIz5fmDdfn87AD2YyefJ6vN4w4bBBaWmIHTsCXHHp8N1qH70r3HzDKIoKAzHDWGlpmPXrinj+mX/2yvWqKiRgV7EU1krwyINj6XPC5/z6y3Lmz9/KwA9n07nDh/Ts/glnDfiGhx8YywXn/sCxR3/C9u1l2anhsM71V4/km28XYKT5wJavLRqfeCXL/aeqgjq5bkQkCVUK0IQkGFE0HRkKt149Ck0aMWU1JmDT3O4SU2k1M+01/Gj40AhhoEnYWOrgrdn1mbLRLM7+1X8NCRkK3kivJyOiOPsJo2PgRyMQU6TNsAO7K8QCNIqFTonQyBfhMve8AJVszuJ++nAdm1jC9zxMIXNxEt2+SxxC0jFTp08dSaYN1gVJiHGNTuaQSoqUMXN3r1LOHBAxZthRcUS6bUV/t0dspwLzJslWFX44cw1ZNhl5LhrHRdwj3Q2aLMyj80Yf8eeLuOeixw8UxfWff9ZxzHFNY7v6vqcdwoIlt6ZUWp1Olcuu6LSvl2hxkPLtV/+h66kLLI3+dUWF5+q6gbRJNGGYG/LIxjW+yUhUDhQXBQn403vU4jG7TkXaPkcmMERZDCuYFklfWCGIQaEIs12EKBFazGgQjBMMh9Cds/g//BQzjCfYwmp8usJWn4PigB2DqJUXcqQdGwqqNGP97SiokULc8eFjZe+SmRcQpW9TL2c092KPxO8rFSig0fdJLVdj1kaywpzufapwwD5CtaUJw1Pgs0/mJobzRdi4oZjVqwv32pqKigL8N39rUrmtUMhg2E9pw0H3CEthrabMmZ3H4IGzkz4M4bBkwYJteL1hfL4w3tIwy5cVcO+df8TGvPryFEb9soxQyEAzoglQiYpoOiUn6r6J/qvaFU4+rTX163uS68BFlNcSX7hMdMVpUlpEaS0L4Y9LzIooc+XPiVoAJJKgIfhwfi6aDmsKTUVEE5JCguSLIIURAbpdhAgIHSkkoYjC7BcaS0KR1xJZJwKK4pK0gsLApxgcKU7hdJ7HRW1+5TVm8Bk5hKklFRwSfIZkUwCWlQqCRur3zYNKRvyn2tTIkUQUUVEmMm1CwYGKIyZGy758TPeVecyOil9TuO6XtgTCpiobfc9Euf8UwCZN60XUghF9c6PCOV7Bjb9q2UjKHdnvchowu/bsDMOA556emPBcg4aZfPvjBXg8djIzHXg8dpxOlQcfOY6u3RrtreVaWCRQVBxMWZ5I1w1KSoKAWY7o6y/+4/23Z7DgP9OlX1oa4vYbR+Pza7F9Y7ThiYwYBaJE79lUabGpQjAlEUtjeQ+YMBOzQBLQBdMLHBQJnbCQ6MIMFdghzPRYFVNpNa14koYcxtk8jYLKSJ5hA//Frq1T1rVKFQr1pYtMaTMTTyW4UMnCkV7eSDOXwIng8jbFHN/Aj1ORRMOqylTWCnbX5Q6JiDFBJSk4q9ohjdQr1DWJoqSWj0KItOXRqoKKalFX5Nndfaou6WpXsfxxO2HYT4sTWjdWRDhsMOqXZei6gaoqDBk0p6wYe+RvZ0hZThgkK60ulxoZJwgG9UjrPcmE8asJ+SuxlmTDI0GpY0OYVkviXFLRISkEpiHLQhO8muCjhXWwK5KwIcz6ptEA+vhzpakg2yN2znAFn9kgBq5IMtQ2zESCHNGEAfIZ5vAdi/iNPBZxOneQIVuS7y3LwnWKIMGkuqhmjcJthoIDs1uLiiCEHom3KqseQLk4VReCAJr55ZMqwQrBFq8DF6ChE06z2bCVs++qkQVLUWaZLd9e1pzdjHcti31NfWx/YqS0TiUzf96WpOdOPqUVy1bfwehfV+D3hznl1NY0bZad4mwLi71Dv/5t+OKz+fi8idZPKSV9TmnF1MkbuOjsH5BSEg7rqE9N5NwL29OwQQarV+5Ims8gIiaEGb4Tf9+GhURJmalZPr4zfek7SSReEEFYKUvEAjNeVkrwKRpCQghJfI8sJw05Sz7D77zMaF6iF7fShp5mGb04vUpBkIODbGmnSIQjyrgpN7X4iv5xaxJAlh1cqsSuwotHF/Dq/Nps8Jny+d6j8nhhZoOUrylRWU+WaNGlVU7S7HtSJU+7PTauuqYLDRpk8NzTE5PKU2bnOGnTpvZeW1NWlpNjjm3KlMnrExRjp1Pl8r3kwdobymhlsCysOyEU3LXuP7ouYx/YlHFRET+vYkssfxJP+8PrMWfRzTz65AnUixSm1nWJ36+TKra1fOhASoTpCgpjoFFWIL/Cj12ClRZGr82m2DBtBxoplNXIOYaIuLoos0akQsegGA2/MAhgUCI0itHwCkF7cQl9eIQQXr7lcRbwW8xGLIm4s+L6yEajqrbqBoY0XVtRN1fsK6LsDU/p7nKglqmb5Vzw8S57JdqMARFXQiu1dSBmPZACIUXCPLtC7Jz9qLUqiqjUjr1V61opn8/OdnLxpYdzzXVHWMqqxT6nd5+W9O7TkoyMsqxtj8fOrXccTdOm2Vxx0TBKS0N4vWFCIQO/X+Pnn5Yw9JP5BAMp0oOEuRFNhYEkJEwvUzQgq5yuGJMFFW1FjUjIl5cw3ki4li8SuiWFJIBkuwhjCBkpfWV6sQIYBEUW/XicBhzGON5jHr+gpLmWDcExuKiHio7EKSOSMF5mSrMJgEDg0xS0iNLSJEPnrWPzGXz8Vj46IQ+XXeJIIyaiYQFlTQWSj6eKh91X8s9uVyptlRQCMjLt3Hp7N158+eTI5yqFEr4H3fgqy6AhZ9CwUSaZWQ4cDpWMDDtdjmjAQ//rWfUXi3gtK/WoYiwLawUMHjiLjz+au0vn9DimCXa7ebMff2Jz/vpzVZILv1Pn+mzYUML2Aj9GbBNralGtDqnFyD8uJTPTwfkXteeFpydGdtllDqXyymb89IaQpntHpFaMNIyEmqO7ildoKBEXuy1tG0uJT+j4hU6GtJGNnVSqcUCUldqyRYP/I8N0CdmiAyfL55nPx0xlKOuZR09uxUMtVBRyDQcBdHxCJ6yYVkgXNkKR6NZ4d7otEougSgiI1Cq0Is3kNDX+C0aKhBIu0WQEFYFI8x6XR8T9pfZE3ia0e90PG1xNk6TxeiVQ2faXFhb7EiEEX317HqNGLuPH7xfhdNm48urOnHhSC6ZMWk84lKyU+rxhRCXcuVH7Z7w80JHokQL8NmluiM1R0UrQJhoyEj9fdq6UZg1XDVMRdZSzw+rSNBrYo7Ko3PlRD5lfuOguH2Aeg5nO1+wgn87yCoSIu5Gl2cB9szTIQaVE6BQrBhnYzMTeiKfKjkIRuukxkoJfVmdxVqsSXDbz1ddxGhiKxrp8F1KWlfWLp2VWiHy/jRJNjQSNlSWflaf8+UnyL1WMxR4SDhuRChI7/5s/+MhxPPrYCTGF9OsvF6S0wO7Y4WfFih0cemidqltoOZo2y2b+olsY8/tK1qwp4ogjG3Bcz2Z7pdW0BIwUTXL2BZbCmoZ/Z2zi8UfHE0ohxCrinff7xX5+6ZWT6XX8egIBHU0zsNsV7A6Vt97tx8oVO7jrjtH4fVpMaZNSsnrNDs7o9zV2h8qC+Vvxh8qal6bwUqfsa2zu+lN/UKM7/ig2BBrEgumjO/4kRSy6wZUQEBp2qaa9Ruz1AKVoOKUZKxq/SxZx45DE20ETrmcji6PkPTRlPNP5ip95iBO4hWYcRbReoEuqbDVC6Iop/BNFu8QRsYkqkdcW2InDSY2EDEiRuK7o38BU+uMWWQlE/BsYexsS2xDukRtsHymx6eoJx/PCsxM59/x2vPjcJH76cTGGLhlwxqG89MrJNGiYuoWthcXeRtMMfv9tBbNmbubEXi244KIO1KplxuSvXrmDUFiPhOskyqL6jTLZuL4YXUsWttFxOga2iOUw1SZWCokW69sH0cwmT+Qcg0Tra1DoMUskEgLoCIh4dUx5ZkOYsahSJF1TRI4BKMLOkcatZFGb/xiNTxbSnVtQhSMmi0NAMHKuR9oJyDAhYZidryIYGDGvlUTy86psDCk4s1UxLlVSFFIZtS6LLk2KcNkkfk2NWVIF4LEZXN0pn4AOr05vjECJGF9Sv6/lt/emoSBS+3UvKKtRUifmJWKzCXr1bplgPU13nhCi0uFUe4LdrnL6mfvAWCBB7qeYDUthTcOnQ+YQCCSHA9hsCoZhpPzittkEbQ+rC5gVAt58bVpMWQUzDvWW246ia7dGdDu6MQ0aZvDqS1OYOmVD3BiYPSsvae5UFtF0lTLMOndpSNTl0JBokYxUe8SWmG7vFBU+QcyyVUfkhFhUFK3xJmJjogkJ0afzCdLcoVALG3bNjgfBek0Sjp1ZgbYlzBu+sdGHC+jAX7zHWF6lM6fSjcuROBEImkoHhhFmnaLHavspSLJVKDHMslfR/9xSxY+e9F5IJBnSjiDi0iunQkZdVQoiZf1AW7ReawUoCLNihIhXhEWFrV/N5ZUT36nesv1keS2Pphmc0mso27Z6CUVqsI4YvpRpUzcw57+brVJWFvuc0tIQ/U7+kpWrduAtDePx2Hjqib/55rvzeOKxCcyfl0c4HB8naoY0ZWQ4uPjyw3nt+SlJClTCxlhAWJpKZvwmN3rfhjFQMXCglAUeCbNGdtTyqmMqtTrRqiaUrSdiANCkWcfVnaBIVpzACyCEwpHyclzU5l++JiiLOYl7cZFBiPIqoyDHsLNNCSZsrnUJ9kiCmCkLFUauyeaXNVnYIrkNAL1a7aChR2N9qUAzzHfJphjkujWOblzC+cPbxlarQopm22ULiuqD8VI1anhIK2n3UA5WpgJV60Nq0/P4ZgnPXXZFJ5596p+yvJUItWu7YnpBzWDvJFRVBiuGNQ0F2/0pP7huj43ORzRIco3abAqnn9k2ZoJ/6om/+f67hQkJW7oueefNf3nlxckA9OrdkhtuPqpSO7pddSVrlItrTR1gGXvomJ1ZwpEWgvHCOSE6VpgCMojB1BI4tr6XTJuBGin1Eo0xLX+ddWGdJVqQ9tlhWjkUHJHYLw2D0kqUj85CoRFNuZTnOIoBzOdPfub/KGItCuBEwS0dtNVduKUpCBvbBCe6bagysZSKCxWXVMviVCNWBjUanSoiJWbSkC5JQo37kilP9AtFRTG72pR7kyq6/cssK1GFOTZp7BErj5XaYLFPCQR0tuSVxpRVMJXYwsIAPw/fO2VWLCzi2b7dzysvTub0077m9lt+49FH/mLpsgK8peY22efTKC4KcuF5PzB3TpmyGpOJgFAFJ/VuQTBkusWjsi0+9jJBxgoIR0r6RT1ZpgJqNkHRI+WqjEjVlmjNaw2z3XYQPS4RVsbmTxAOEVkdLT1YIsJsF8HIutLf+FKARxF0YAAnchsFLOcf+RxeWZByvMC03MbWIInV9I7ixhYLDwgbpqLuVCWG38NrPTfTv1URmXadDLtO/9aFfNhvNbPzMrHFxf1Gw6sSlh752YYZ+2+GI4hyb4NIGFv+3L0pA10uG6PHXJ7kbr/x5qPocmTDWIy0220jM9PB51+es1dc8/sTq0pANeOss9vy94S1SRmlAb/G4oXbEiysiiJo3DiT19/sC0D+Ni/vvT0jpcKraQZvvTGdex84FodD5Z03p1dqRxcVSPGukrJQgRQuKCCMGXsUU3B2phXFLK5mZyx7JNpVj8wTGxq5+TQpGLdd5e0jCnCq8L9ZdSnVlDSC08w6XaAFaSVsZKpQpBumpVOBsGHErpfqxWRHjtmwcxJX0ZIu/M6H/MxjHMulHEV/ok1mWxouVqtBGut2SnyC8u0HBQIPNlzSrCdL5L014r+sKnir4tdVfqAt4sgLp7DOxv+tVIRZcFyUHVdSWA3KW2tURMR9mOwuK1sUqVvJ7kO0FO5Tb2mYBfO3wqX7YUEWBw1b8ko5/phPKSoKEghoTPxnXdqxvhR1MwEQoKiCN987jSEDZ6fUgzSi4QNxOZrCdJ2rkZHlN7emhyrOiAE4IkfKx7+mtZoKUwH1Si2mYG8y/ORKJ840MlQA2UKQoyioxgm4qc143uAPnuYE+RA5omnCeAXwSIUSqZkWUgThcq/fhkImdlPRlpIsGzxy9FaaZWoQcnNdyyAXtVlHs1xfzFKqpEhSs0Vspro04r7TSFDyJDKyBhl7PTbKmibsTRwOJbb59njsDPxoAPXqZySNczpt/P7n5Yz9cxVTJ2+gYaNMLrioA3XruvfuAvcxEiwLa3Xjgos60K5dbqwPsBCmdVUICAYT1QrDkNSu4yY/3wfA7beOrvAmMgxJ/jZzbKoSQEnEbSb1iFITX6O1wizTyNFoc4BKXUuAJqJNALQKGw/4dfh1s5PtQUHzzCAVbW0NBPO88HPAyyLDbyqrEYqF2XAgaq0wYoJJRMrDJNKCLlzJK7SkC5MZyghexkshRHbsHQ0Xhq5SoinUkWoa3U3gRiUbe6SaQBnpzgBTwLrTB12gImK9wcuulBgCYIt+lcVZRM3XatZDVCLzJJbAKcugLV//NX7MfjawpsVmE7Rrn7u/l2FRw3npxcls3+5PGdK1K7hcNkaNWk4wlKqqqkmZDDaJ1lmO3tYJ3ioZVWDj5hLgI1rfVcbJv4plu4j9n4muSLYoAYoIx+JwoyiY7VvdCJwRY2ZjOtKPJ5AYjOMZtskl5eYXZGMnW9pxomKLNFMpvxwVs/tVFg5U3c6SfA+GBN2AoC6YsqEWQa1sNT0ae5M6dkWtrNG611EvV/kx8a/dDF0zjSo2TI9a1MsUk91VJAjjPUU+X5jbbh7NmjWFKccqiqDvaYfw5DMncfOtXWucshrFsrBWM5xOG3/8dTnff7uQn4cvpXZtFz16NOGpJ/8mFEouVzVv7hZOOPZTnnuxN2P+WFnh3KqqkBsrV1XJ6GVB2kBzQ5oWt3SW1mgJKjtpEqrirlGeaE2+dAY7Q8KIjU5GbjRjWR3osZisZMrcaj5hQPxYAV6h45VmFmomNpyogCQgjNThGWRxJg8wn7FMZChf8xCncDPt6Uq8+bORtFOMQUAkWjBdqCjYYu9JMJZ2YApIByohqcfWF7VwKiiR9oWSkDRSvjHRzlrpZGZ0fj1SZizy9sQUUmMnFtJyX3sxqktHmFTouuTc89vt72VY1HD+GL0yZYOAXaW0NMSjD40lrMX7tFJ7tKDM+pOUa4AZqhNtTlIeKcAntdgG1ex+JQhIDU+5r+iyCgOp1xBUdAKGymHYyMcsF+gTOgaCUiFwy7KNdF1aMoCnGcNL/M3LHCNvpSndzUSuyJj4TbMT1VSH08gmXSr8vroWUtUI6QpT8zysK7WTk+mnd8tCnDaJUzW45+g8XpnWCINEbyFx04pI1RhzQyBjAs+szJL4HitxZbLiNw6mJbDcIqtALgaDGu+8NZ033jptp2OXLM5n5IilCCE459zDOLRtDYhllcKqElAdcTptXHl1F668ugtgdr0yKihxEg4b/O+hcUmFg+NxuWzc+0APHA5TJDRrlsPatUWVW9Ae3mzREIH4EHYlTkimrAwQeSnxu3Zz92/OIYSIlBoxWwwGkHFNCqIpCNFkLFOsRJ8LkyIMILJ99hlaTJGTCmwxwjSQ9iTLpwF04lSa0IHfeYdfeJUN9KW3vAK7MJ1tKgqHSQcrCePDLBNT/svDhtnWNUSZAqpKgQs1Vk4uPsvXrFBgRxAmJI1Y9m70vTKrFYBeodIZbVUoYw0lytz/6b8Ydla7sbrictnIyHDs72VY1HBycpxsWF/58YqSXP1CVUWs9jWQYDBIZxwwh6U2GuwUEZXKpusbzJhXTRoJ7VB3Nl8YSZESIg/YIrVYLH4JUCA1WkjTXhpdZzb1OYOnGcurTOFdjuYqDuXUmIwJxYUt1UGlASpbCFGSIuVJAKpu4/sl9WJPOIG3pzXlrzW16N2ykAKfja8XNMQesyKbcjIcacNguv4TK9YoCHRpfuOkK4ATLaIVbcpSXqWPKcYy7v3bze/TcNhgTorE6PI8cO8YPhkyF8Mwv3NefWkK//fECdx9b4/du3B1oqIkj72IFRKwCxxxZAPq1KnYxB/9cKbC7lB4/MkTKCkJ0brZOzSp/0as5/qesrPQgCg6Zi3R6COMxEghfIDYnR3vpg8RbfNqXjPm8hJxY4U5b2mkc1QYHT86QQxCmIWwJWacZ1SJLY8qFLNnd2TefKGxSYQIRs7xobM+kscvkdSmMefyLF0YwBzG8DmPslWuBcCHQX5EObShJPWzjlJbOqltOHAYilk3UUZd8JGHLFNWkabwdEk7bmzY40pnqQicEVeZKncuF6ObASXuWjHLc4o/aSWDO8qftNcSERTF7KoihNmlLV2h7FBIp6gosHcWYWER4fY7j46Fcu2MjAw7r73Zl7PObkturoeMDDvtO6QJW4l8rCsraytLNAkrjJ5kpfWjJ8jJipKrJJIQOn40NhBK0ikMAetEKGn9LrLox//RjKP4l8+ZK79DSolNCuwCHBK6CQcdhJ22ws5JIoMTRUaCtcsWkXthKcjAhhtbpFGKwImduXmZvDmtKUP/a0g9T4iHe2zimzNXMPDUtfRq6sWJLdJaNvH1R1VPNRq1L9JtClLHDJefL+Gd280/oc2mcHjHehWOufWmXxk8cDaaZmAYEl2XBAIazz8zkdWrC3fvwtWI/RUSYCmsu4AQgh9/vjDWfSodUpqxLG63eUt7Muw0apTJwiW3MfbP1Qx8fxb5+T5KSkLMn1+JGNZKLW73T9WIKJ7x/hNZZhXVMCJZrFpKYanHWV9jwiRqKUUzA+VF2XMSYpbMEsKRzNlokkGaD6WAHYrOUjXAYtXPWjWEVzHIF0Ekkq1Cp0goHCYu40Qexk8pX/B/TJe/UhwpGhfr3hKPNC2pjkh3LBc26kgndQ0Xmagc6hK0cwsyRFlLV1WaCqkDBXtUeY37A8Tv8D3YYzGpYJaFccQ9okS7Z8XbBpwplNbKfFGWj43d28ZYh0PFk2FHCIGmSRo2TE5IADNr9u/xa/fuYiwOei6/shPXXNcFuz3115uiCBTFTKA5+7x2XHl1Z7789jxWrb+Lzfn3M2natel7v0dkWDqltfxzCVUEUlC+d2GSbVBAQOj4IvkEoUgVgjAaILFjFv2Pxf1HTk2nuOlI8oSfULm8BBtOenMPbenDIn5hBoNwoNPIcNJROHEjsAmBKsz40myp0FE4sQNuqWKLdPKL2jcViMk3BdNTJYAGnhDv911Nr+bF1PNotK0T4P4eG7nwsHxTAsrUCmn8+xM1FsRXRClTcFN/f8TKYIlyD0hrYEqHw6FWaCWdPnUD33+7MOUxXTf4deSyXbtgNUNKS2E9YGjfoR5LVt7OBRe1TysQAVq1qsVjT57IVdd05sWX+zBr/k1s3epl2rQNCckAlSnEvlPiYsx3d+dvYLqgDGkqrvElqqJd1ioqPhXGiFkCEpYmUkRbimgVAx0pJEGh4yWEjzCFBNlBkCIRjCvxkvLlIoWZ2b/t/9s77/goqu4PP3dmewoJvffeO6gISFURLIiCIDYUC/aur/W1/ey+Vqxg7w0BpaiA0kFAeu8tEJKQtmXm/v6Y3c1udjcFElKYh89qsjtz585k9+yZc8/5HkX3t4M1XqsjOjCYZ6hNB/7kE2bzPNkyDSuCOKkEnTmhg1MqOKSCTSo4UIyoqhTUseuMqSMZlCQYkCi4ppZCc3teG9bAv4L0U43IAFS3gtV/jMCyf+Cf3W/I8+dlBX4OFoNJjL+L9Oe3FnBMhRBlCJlnxBUi/xQlQW6uxrHUXHRd4vPpHDqUFXU7j0cr/reDiUkxEULw3AuD2LDFsNOhb7m69eK594EzuPGW7rTvUIOvv1hH7WovMeL8L9i1K42jR7KZOmX1CR03YK9kvv/rGB0I86KkAfvqN/4hDlSsFScpwCfAI3RyhL+7Hzo2RdLIaiymh0QN8vZD+h1cQ15LCh1FUUhXfMEleQis8qicyTV0YSRbWcAsXkQjhzgRpQhKCFy6SrK048RCHFYSsQalsITfThpjG90VrSiMbnMUh0VHDfnqdFokV7Y/QhU1dm5uXm4r/kIrJU/JJsRpDXVcQ4n1NWu1KixYeDV16yWEPa+oRG3R2q59DX765fICc1G//XZDmJxlKFJKxClo1VraGE5r4Y+SxsxhPQGsVpV3PxhOQoKdKR+uivjDCAHbth3jycfmcfmY9lw2uh1xcTb+/fcwSil+Ycugb5j/Lj/SIYpGwLBClM5TfoyOLtGLqgwnVPe3EQzxoqOdszDarxrGDHShIGSezp9H6hwVubikaiytB+/WjXkZeodGblYcEkXmOWkCcIpE+sq72cZcVvAZP/MA58iJNKQrLqn7i74i56UCNiR9qvmwhPaKBYYmCXYcCk8niHlN/RddInB7jfKqWA6/A0uEDFZg7ECkwvhCycuRjVXsFbpv/r+SkQ9WeEFXcVCUSOMUKzrl8ei89PxCzh/W3N/+0MSk9Dh6NJvpv2wJe2+mHM5mzqwdHDqUxYH9x4Pv1fnzdnNWr4/wuH0npJkZcDMDov/5i4k0QAjd37I0kHGZV0QVwIee11igkOPZ7blkuO2k6QFBv7wPtjGO8EdSwxF4kELBregkSAsWaUgX2jAc01ZcjEUms5wP+Yn/cq28jwSRFDZGrpRsRUcPvTkG4rCQIb2R9sU/tQ41s4n20dd0Qb0ED8fTbGFyf6GzVv034/ltrgUFn9SD6WOBqxthhaL8WRVFcP99c0lPy0tVslgURlzUkpsndeeLz9fh9WiMHNWGs/o0wGYr3GVShOHQyygemxCCERdW9NbVZVd0ZX5rnCAWi8Jrb5zL9F+vwOWyYLPlXcrA+9Tt1vjqi7WMHvUdAM2aJpf6vKQIFEXlPUrsRqeApbBQAt2ejHzWAjIuRaAK1O9Qh35RBFIKhMYx4UZDx2gGa4hm56DjETq+fLp+gRFUBIoQtGQwl/I08SQzkxdYyEc48RmR1qhTEtiEIIqMKIoS6Z4GG84WGPWM/bLRYlGJKKwInY/huOalDKj+yELooAqF333Gij6EjlMlyV7IKP7jKYLmzZOpWy8x6p10rNWHzZuP8tvMrUU6honJyfDWG8vxuMNvE71enX/XHCL1aE7YjZWuSzLS3eTmahGdiiKQef+LtuAvQx6h+ACvkGj+RzS7qAsjxz9/NDYSQZpXoU+9DDxB3de8T3ag3iCQ9xl8ABbFOH6m8JGmeMhUNcPZE3lzbib6czZ3kcp+JmuPstt7gKNeOOqFLA1SZKz1L4KtW31IkKDIvEbWh7Ki5xZbVcnRHIvRHCV6fCNmmgOB1wr7oovyututsXTxPrJC9NZ9Pp0Zv2ylWvU4XnltKG+8fT7nDGhSJGcV4NLL2mF3RL/peOKp/jRoWKVI45RnzJSACoim6Xzx+Vp8PmOZJdqNeeADsXHDEXr2rkvNWtHz+0qUfAYqdhDwxFzZ4PJSjP1DUxOClf4FOK0x2gUEX5dAmnCTgYcs/OoB/h3c6BH75p22wAbUpD6X8RSdOZ9/mcVXPEwWe2IaOF0qLD7kwq0J0j2Cbcct7M5S8engiKIP6PJrHQRTDWR4/paCkpePGjLH0PQAayAfNuolyCvGCqBi5IVZpaH5avHXLqv+R+zLGXKtA/MN/Aw0bFiFP+aPZ8LELlwxtn3UZTGApGQHK/+dyNDzmkWNlkopw27iAmRlevlrQTFKuE1MTpDt245FjfYLIXC7T0KjNcS5CwYFiuAvBdACzpwgajRXx+g66PYXXMUcR0LvGm586DRzgjXk5t0mRJ6zmm/umQEnWECO345bw74wDOqKzgzgIXLJ5WMeYx/b0BBk6ZClRzlfCYoUxEsLVXU7yVINKq1YUVCk4Lv1Ncj1hR/H7RMsOxDPMbcFIQR2v6Shke+v+pshRMnvDSGQcqD6/xAinx2M6uj47V80CTQh4I+5O6IeqzB69KzLzZN64HBYsFoVLP7Hi68MZtJtPU9ozPJEWeawmikBJ8Hbby7n22/W+4WFYxsWi0XhoQfm8tf8PSctZl1s/O+Z0O5IRZVFiil1BWhSFnjHGxqJdaMFHbiI8aSkbxKsSBd4Yk1JGMvYOcK4xkYrVqPW3yckWX69wsB8Qg2bzf+zBSt9GU8jOjGbt/mW/9CR0bSQQxAi3Jw5MESvFx1ykBlw6gT8CySqOeSGhBQDhWJOv55rqHseGjU1FAp0fPm6W+WdYuiyXizyXgv8DVUUI+848Jp/SIWC3pF5Q4WeuVCgZ6+6dOthPLKyPHz7zQY0LTKZId4vT3XHXb34+ot1ZGZ6gpFWl8tCn74NWfjX3gjNYofTQr36CfmHMzE5KdLScvlk6mr+WrCHFi2qMmFiV87u25BlS/eRmxv+/tV1icNhCYuqnRBRnMGAJnYg3lnUdKxQAnZX96cXBKSfoo3RPNHodtUlQeeQRyFD09EQWIQOBTgMgcitcRw9wqELWJRaNGMUT/ATz/E9/+U8bqcJXXFIBRGyDI+M1KG1SytVUckQXuxYsKKy7XASby/1cV23A9hUHUXAon0JvLa0bth1Cr1Blxg36L4YtjH0ugT0VwP/Na6fsZWGTuj9S0HWVlUFCQlFW22KxmNP9GPMFe35dcZW7A4LF17Uitp14k94vPKGLCNZK9NhPQneeWsFObFa+4Vw/LiHObNO7G6txPAb04AJCCxZxdIUNBzcPKczmuOqy0hDGtTNy5c7lYPmjyLmxVMlkjOrwKCqkm3ZcMQjQ+KlMmxkSV5+qy6N8Zz+JIFjwkuu0EnWrVj9Bipg/PLniTWiE1fwPHN5h1V8ykFW00NOxCmSUCQ4EST4Te9RH+hKyPofYPM5UckO01cNRDJCc9P8TxmFUsI4vuFk63hlSLFFCErw6yk6kbloeTqFQSWGkNdimeNo+a0gURSFO+7uHXwmLs7G4CFNmT1rW1i3F6fTwg03dQWgUaMkfp8/nkce/pOFf+0huaqDSbf1ZPzVHWnb4i2yssLzW1VFcPnodjHO0MSk+Bw6mMnZZ0whPT2XnBwfc6wK77/7D1M+uRCnyxrmsFosCuOv7sgfc3exY8exki8MEXlFkYGhAw5UfltptLyOtJ9A0GZIaaxSWf2f59BggxA6u7JUvGikaYKLq+t8lCLJ1WG/1Gih2jmUr0LTsEl5xVYISbr04hICuzSW63PRg/n2uQKqyZpcxpP8zPP8wosM4Hra0p84qZCF4bSKoI0NPRdDTUWRAl1If32Aytpddbhrd22SXLmkeCQp3sD5GAW/0XOIjSIuX4ijHf3yG7YtYK1CAzN5DnDe/hIZM6d/2PAWUY9RVFq2qkbLVpWgUUAUzNasFZDjGe7CNypPBJaxRIE330FCc2BjyVmF5lrp/kesRoZheoL+ive+SeBQYUI9jVZxEgXjUc8usYs8p0wT4VW1YV2kBOQIjf0il8yQRrISiTvKTFwkcj730ItrSGEjs3iQI/IfklBxoODF6JsdF7hg4ZeQ+tipIq3ESUNvEAj7dhJSBPO3jH3yIr82f/vBWAY3kJuqSuGXisk7bvRMtoKWyYqOQHDrbT1o1Cgp7Pm33h1Gx061cLmsJCTacDgsDL+wJbfc2iO4TavW1fn6u0vZe+hO/t1wExNv6obTaWXm7LG0aFkVp9OCy2WlXv0Efpx2edQ+3CYmJ8pTTy7gyJHsYO6p16uTne3l7jtn4c63oiWlznffbCwdZzVAaCpWiA+X/3ABHeuAwooP3dDJFgEbalT4a+hBSSuffzsPGnWcGu9tjUMH5mZofJ1qOKuBw+/TPThEXlTKkm8OQaUTAfuEh3S8ec6qf966gCNCQyGRS3iEBnRgLpNZxvfUkSr1pIUEKbCLyJSCwDA2ASqSOGGkLhnyVQpZOQ5axcPt7dOo7zAcUjVfboUMUfpX/WlTgZWooqa0RQbCw1e2HHYVm00lIdFGXJwVi0XBk6PRvvlbPP6fPw11E5MQipYOYKYEnEK2bz/G8Qw3bdvVwGqNnkB9zsDG/Pj9pgI7W5Ur8t9JhixhGb9Gf4PFuAEF4W9EEGL51ZDqyGjjudGM3CJhGCe7P8M+zgLj6ur49LwirJd3CjxanmOs+yMNAfcvIkKhwFHpxaEr6BjOn1dIsiW4CI94HEensxhMbdmG+bzJ77zEYYZwBuOwYAP/nbo1EL0MnB9Q2yrI8Rp1/w4U7Cjk4EOTRpcaBxZSpQerYnzxeIQM659t8TvEkddZIiS48sU/fTKQ81a895mRZhDejhaiO79CQPXqLjRNRw3RnElOdvD7/KtYs/oQu3el075jTRo3TirS8Vu3qc6yf65nx440fF6dFi2rnlAFtolJQcycsTWqjND+fccjmlhompE+UOpESReI5iCHzjry5byuV0FbixbcMMsn2J1tfIUf9kmchKYgQA6SHOnBjlGo6QMsMiCjF7lilqZ4qSYjKwqMGgKN2tLJcO5lLu+ymG/IJpXB8loSsZKqeDkiAwlSeSjANa2OcjjXwuw9VRAhdtCnK+zLcDK85WGGN0nnyw01jFOT4JYaWfhQhMCCoUqAAAtq0GnJjZkkkP8qBk4xLz4t/H8PV5yVtu1r8PWPlzLjly3cc/tspE9Hk4L0NDeT31zBti3H+OSri4twpNMDKTFbs5YXVizbzw0TfmHP7nQsFhWLRfDmO+cz/MJWEds+8VR//pi7k+xsL263hqoaBrHcEn2lxe+0Fp2IJZmIcQt2SgIaAkLAuizomSiw+HcJ1O+keuGIlteDK+CY+tBQUfCEiEQ5sQRlr6SQZCo+bDLvre0RguPSkG3RpNE+FgGZaDQS9Rkv/8sffMkaZrCP9QzmVqrRCAnYpcAb4ijqUpDhtlAV8AljOUxBIZ68lqMSSSIq8Q4P49sc4pvd8Ww95sQb8iG3o+CW4V+ygVytSNmWvG5YsQx0QNol2qUP5HUFXoqW2yolPPbQn7zw7ELuvv8MJt7SHYcj7xp27FSLjp1qxTh6bIQQND0F6hgmpw+bNx3lrTeWsWVzKmf3bYjTEf1rTNNk7AYAMbBYjOYXpU3MVKsQgp/TKM4vElI8oVJ+wt/eOVqQQAKav8tf7OX0ggpjvdJYt1JRGcxNJJLMUn4imzSGyVtx6RaUfNJ9CpJkC8jjiWw6Tpj9Cx5TwoqDCfy5JzHsathRQ2S5JNGslgUlrCgtT/dWhv0eioLA4bJw/ogWpKe7ueiSVoy8vC02m8qm9UfRfTJs+TE3x8ec37aza0cajZokRb8+pyFllRJwWjus+/cdZ+6c7TgcVqOyb+J0/v5rT8idsPHxm3DNNOb9XY3WbcJb9jVqlMSyf65n8jsrWLxwL9k5XlatPFhsI1kRCE1kL2gjTUgEklgyTXmbGtHHX1NV2seBQwGbAj5pONBfHQ5pVyzzHK7QNIAAudKHKyT2mi68VEUlV+rBZ6UwumuFRvikhANoNMTKWYynIZ34nbf5jv/QmzG0Z6jhJoactlMGBKYktaWdQ7qb/ApZAkFtp8417Y+i6VaSfXGManWYubuTOJJtQZOGA+6Sqr90zDgdNzpuEXnHExDh1vxR5WgEWsIGo90i73kAi1VB03R0HX+OV6Q5lxKOZ3h49sm/mDFtCzN/HxezzaqJSVnwx+87GTPqO9xuH5omWbZ0H4qq4HBYwgpaAxG04pKQYAcBx1JLOArrn0vk5y52QVVg4bugla9Q4f9Ae9RY28YWojIoqIjWC2Sgk4SCBegnxpAgqzKXKXzF04y13EVPp5N12ZChGTfJdWzQLk6yLdXFoWxAaJF5+0Ky/qgzwpk1bJ6CR+o4XBayc3xRI79qiE0MKyb2f2Hkd3OdLgv3PXwWt94d2alq1cqDUZf/bXaVTZuOmg5rCGYO6ynm1ZcX06n9O9x71xxunzSTzu0ns/DvPVGNnMejcf2107j3rtnM+3NnmCBwzVpxPPJYX86/oAVr1xyulM4q4F+Sj115HtBcBfxasDLiH4QbZh1IlxofpHiYm6azIUuyMB1e2gNbcyI/EAEnN39ZkgQ8aOj+fC+vPy9MCvAIiVvo0Q2/MLQRj/tvTBrQict4nvp05G8+Zgb/hySN6gjiJcRLBRcCJ0ZxlhVwych7PlVI2lfLxq5KfLpgX5qLXftr8uqgHXwyfAsfnL+Np/rsJg4rTizYUFFQCpQLDxhwu18iy4KI+PCqKCTF2YlzWVAR2K0qcS4rH39zMXc/eGbYe7ugd6nbrbHu3xR+n7OjgK0KZtnS/Vx2yTd0bPsOV17xA+vXpZzwWCYmYOQz3jJxOtnZ3qCdzc3VcOd6qV03HofDQny89YSdVTBs/egx7bFYSvgLOX8ea/SUzyCK/2UpiBSgz5cbHyBXGG1b88cWA3UIgfzY/NqugZ81YWhcR5R9SrADUgiOCUmmkOQgacUQhnI7KexkqvdJfCKFPlUkQ5ONR6d4wxn3agKnDLjSeWMLJPE2jRxfbDfE4bTw0udDqVU3et57wC7mdb5SsfsTKUQ+G+lyWXj2pUFMuiu6tFSHTjWj6kd73BrNW1SNOcfTDzOH9ZTyz8qDPPvUX7hzNQpuOGqgaZI1qw+xetUhPv14DecNa8H/3hzK55+u5bdft1G1qpPvv90QVc+tQhA08LHv5kMJLumTPyU277dAm9fAXbuOxIYaNI4SyeVNMxndNBMw7rS/2JLIzweSCpyDYXj1CPcuULRgFQpCCtLw0BAHuUD+Stz8WEPm6CSR87iHdcxmIZ/wNQ8wTE6kheiKF/DIvLFUoCoqWVJDBeJ0w5HUFJ3WVXPwarD6UAK6VDiQ4eJIpp3q8W42p8Tz3rJGweNKpP+DKMmRkVGIwHkrIVdc+DOPjSK3gNyX4MyzGjB8dCvm/radOvUSGHdtR5q1qEpWlhdXnJWsTK9//4Kd1qwsL199to7NG4/SrEVVBg1pEpbbWhBzZm9n7OXfB4tgdu9KZ/Zv25n+2xV0614HgL17Mti6NZXmzatSv0FikcY1Ob05eCCTI0eyI57XNMjO8rJk5QQ++2QNr7+2tEjqLdHIyfGydMk+WrSsyob1R092yuEU8fs7dJk/L9/SXz0v8S/NRx/MLTSQgV5ZkTfoWdKHXSjYpRpMFQqUqTqwkC00NCQuaQnquVRVoYFdkOnTSPMo5CLIlYb9b0kv4qjCDPkCL6Q/xUTX3TR1NAxqkgvguM9wHJN1KxmKD13REUiaVs3mig4H+GZTNY4dsJA//9ViVfjo74tp2aE6X7W5jKGdPwlrBBHThoXcsARUA1xxVj744kIGDGkS87pPvKU7Uz9cHfY97nCo9O3fkKbNzLSmIBJT1upU8vln/+J2Fy/ZNPAByMryMuOXzSz8ew/HjuWQk+1DUUTFKbyKgcz3UzSplWj7hL6SXyJLipBlKGkYxoAD2yLRw7xDdv48aGdg3RxGNc5mdIsM/j3qZGuGDV0Yzq4qRdBw68bEIqMnIdELozWsilQk6WoOXVzQIU5hyVEb+3NVwoy43/inoZPsr16V/nPvyBAa0Jbf+B/f8gLd5BAGiLEkKHYEhuPqlka0tbFuwx2cikDoCrP/bUinJgdZsr8KAA2qZGFF4nWr7Etz4rJqZHosefuAXyRbxZ3PaRUYBWSeELGWvGsuwpz33Gwfo8a2Y9TYcOmoCy5qyaMP/EFOtg9dLzgfNuAW//DVBn74ZgMOp4WateL49Y9xRWp8cc+ds8O6Bem6JDvby4P3z+WXmWO44dppTP9lC3a7Bbfbx/nDWvDeR8Ox2QpvSWly+hIXb0OPEROoUsVOkyZJNGmafFLtr3UdViw/AJyifFb/BzH/ClD+lSjDV5UhXlrspX+PMHRVbSF2ITie8Kce+bVXg6lb0i+fJYQ/NcmDAAYlKDSwKaj+WodMn2D1URsShTggQZV0VFvSQH+MKb7n+V/WM4x130nnuNaoqiTXrZLulx2wIqim27AIL0M77KV17QwA+tU9zprD8Xh1EbTtDpeFiU92p2UHIw2vYdMq/LbqSp65fz7zftuFx63F1GZ94Ok+vPTsQmNFTjP0aMde05FzBjcu8E/RsFEVZswZyz23z2L50v04HBbGXd2RJ57pX+B+pxsSM4f1lBL40j5RsrN9uN2ZwWWpiu6sAoYjiOEM5l+6KSqB5gSxopmBPts6ku1ZFrz+u7SvdsSzOMXBq72O4rJ58QpLcHuvMLpGBYuqpEQpIJPFcKINt/OYBvMz4aim0zvZy9FDKh7d+GoIGH8fknQhqaLnLcMFqEp9LuNplvMFy5jBLrmei7iVOkqjYImVR0L+7CqJwO1TmL+lNpqq0b72MUZ22otVNa7QOY2P0bt+Os//3YQMd16rQiH9XyAioGMr/ckCxuhejPYyaqDYIMplPufc6BEEh8PCr3+O47aJM1n8914AWrerzubNR8jJCY9aBJbTAKQuycnysntHOleN+YHpc8YWmNfqdvvYuSMt6murVh7kqSfnM3PGVtxuLXjT+OvMrfz38fn895lzYo5rYpKYaGfgoCbMnbM9TBfY5bJy0y3dAejRo26JpWWdiuIryNO8LggZ/H/sOYU6vTqQi4bFv2QeQf4bfwEemde/UMUoDPVICKySKwISrJLa8V72ZFloHmcUzua4VWopDbjR+jhTfM8z1fc8mZkT6WI5E5sFqiiSoyHxIZ9mZfqqJsxWfaDqeLwqI7p4ONKkIWsWHaJ6HRdXP9CFASObGuclJSv/PMBvn22lYWIib3x4HreMn4GmGRKIAQkwgJo14rj57h6Mv6ETv07bQkaGm34DG9OsiEv6HTvXYta8K42beoGpahIDrYwirKdlDuvFI1sTFxe9p3EAIYx+6VZrDOerCEbRZlc486z6JzTHMkMEkvnz/m88XbQ3aGQGVd64xuvGiN6QN7xHF+w8rvL+xkTWHovsLhKoFA2YJR8ytuEOVGf58UlYm20snQ+pnUubRK9fHsbo2e3z571m4COak27HymBlPKPFg+SQyUfyPyzRZwASW/Ccop2uXzRbSC5svx+banyZbDxQhc8WteDjv1rRxA6KP0xtkUZjgarYqS7tuKQVm1/9QCAQUhCv24jHhhML8djCctsCPy+es5s1yw6GzSU3x8dT98/jgt6fs2FpCucPac6C5dfw55KriXPa8qV1hP+tg46rlCxbuJ8uLd9hy+bYS6VWq4rTGf0+uGo1Jx++909Er/acHB8fvv9PzDFNTAK88/4FdO5SG6fLQmKiHbtd5fIx7bh2QhfAEGu/YEQLXK5yGouJqHSMfDrvpjv0uZDM05AageiHyJMB9MWqOpD57ndloA4Bv4SW0ba1Wr7LqAqo59Sp49BRVfBpeS5EFVGNGyyP0lC04mvtLf7WZyClpIld0N4BoV+lqpDUj/Ph9loM6arajXnumyHM2HslHy8bSZ9ukD79R47Pmcnk22fzwMWzmDl1MzM+3sz9Y+egaMJ/DiKYu5rgtHPdHUZDk/gEG5de0Y5rb+xaZGc1FEURprMaC0mZ5bCelg7rgIGNOX9Yi6DTqqoCp9PCuPEd6N6jDk2bJXPTpO6sXn8jNnuk4VMKuGoul5WEBBt2u8pZfRrwz8oDpXUapYffaJVG1D+Wm++VCtP3xOGOoe9m6L3qSCHRhCG0XZBjHIoq4IjPiBTUc2r+wiw9aPwlcFTxkeO/V9f9Xxkq4PSP11R0YoJ4niZ0YJb8mC/058iUaQW68Q5Vp2FcLhZ/t6xFW2sybVUj9qfFkZFj5/CxeKrptqCSQUDIW0WhirSiBP4A0vigOjB6c4NAVUSwyUDAcFulYPEf+7hy8Pe8/9KK4DyuG/kTU99ZTdqxXHKyffw+cyeXDviatGO5WK2qP60gtjEIbU6wf99xRg37JuaqgqIIrr+xa4TT6nJZuOOuXmTGaId50m0yTU4LkpMdzPlzPH8uuIoPpo5gzfobee2Nc4NRf69XY9gFLenZqx61apfDBhWBJf0odVR5+f15v4c6qqGupy+0CYv/X8A5zV9wFcu5jVAFECGOsv87YG+Muox6VoEaJT3BKeK41nofndRe/OL9nJ89nyGlxKFAK7uxAqcKSfMkN+2r59IowYtPU9jyeyqb5x1GSknqpx+S8spzHJ/+E2k/fMeQnC/pVNP4HvWio+sSuzRkr0KvZb+hjZlwZ9eo8zUpOSRm0dUpRQjB+1OGM3/eLn7+cTNxcVbGjG1Pm7Y1Irb9/KuRXHHZdwhh5Knqus6FF7di2s9byA75klUUQctWVXny6XPYtzcDj0fjwfvmxsy5qjDEyLEqSG6loLF0QImhF6jnv+sPfQ0dRF7Vpwfdv9SV164wIP+Uf2xdQoICmpSszTS0/YzmBXnzksA+xUu81LGiYAPqCTVvAyRxIpHLuZeVzGa2/IR35X2cy0TqiS5k5zPcipA0rpJLrQQNRUg8PoUFm+vgC3HIJUYfbyFF1BN3SpUsfMH8VoHhpEppGG0rCorfvQ4959xsH689vpgLx7bmSEo2yxft9xcY+q+HLsnJ9vHVlLWMvrI9k99YHvZ6QUgJx47lsGLpfnr0rhd1m0cf70dGupvPP12L1arg8+nceEt3Jt7Uje++2cCSxfsi9uneo26Rjm9i8sfvO3nhub/ZtSudnr3q8eDDfWjZqhppabkM6v8J+/ZlkJXpxeWyYLUqSClP2fJ+kci/pAEgiXA3BYGbyFC7m+cjetD9N7AUWEWZi4YjmFSFP60o0LzZ7yT7h9H8jV1Uf4LUbq9Ol5CxfBL25Arq+QezWCReLXxZy65aGatOItGbzHzfr2TINMbYb8SlWBhcNxuH3YfVYhy3edVszmxyGF0qrHl5Hg2qtSJn2RKkx2OclvRhs8BN/bawZm8yuV7DVgoELqzBSHKc08p5I5oXuTDU5OSo0DmsQohzgdcwUl/el1I+VxLjliZCCPr1b0y//o0L3O6cAY3ZvGMSM6dvJTvby6DBTanfIJH/PjGf/72yBJtdRdclNWvG8c0Po/ji07W89MIifD694jurfgI5VtGWqaIVZ+mIyAKs4GvGnX20ntEaxBS21vyjBJesBWRLX1B7MGB+bfne0gpQwwrJVticDSuzJQkFfNY0f5TBA+wVHjrHS3K9Ctm5RjReKILuDKGRbMP3+ht8zwv0FoPpLMfiwxb83qgd56ZJsqHlePhoPLrFh6LIiPyB/JWxAYw2rgpCWv2jGgUGXnQsQuARMvjNFW0Ei1Xh7zm7kSpRc05zc3ysWnaQl94fypKFe/l39WF8Pg2vRy9UEkhRBBkFtCW2WBReff1cHv9vf/bvO07DRlWIjzeyfl98ZTDnDv4Mj1vD69WxWhVsNpUXXxlc8EFNSpyKaLe/+mIdt02aGVQB2Lf3OL/O3MpPv4zm2af/Yvu2Y8GuV9n+beLirfgyy3kEP2A4QhxYiWH3BDIyGuonUJxKcLdI+ykwOgwGfja6DIIjeMufT+HFiKGhSoEuBTPTfMSp0MquYhOCrVkqNqekjtVo9GJRJL5ARazfoimKwoW2cVQRyfzi/YLjuelc47gDl9uK5rOgKjpxcbk0SM7C5TT+NvrRzRx4fzU2j5tcj8rPi1qycksdNF3QtM4x2tTMZPG+BEL9Y2N1SKAqCrUbxJ/w5TcpHhXWYRVCqMCbwGBgL7BMCPGzlHL9yY5dXkhIsHPZ6PCq60ce68uNN3Vj2dL9VK/hokfPuiyYv5tXX15SbAWCco8wopT+H4Nua2hxVmDZOLA8FXBaA4SugnmFDEo6hRIrKysvggphynr+oimfDLRsFeTgwxHU4YPGDsmgqvD5IaN9oRACTeqoRK9ID51VthQkxbtRBaRn6qTnWPH6FFRF0sJei1u8jzPb9zV/aTPZITZwqeUWqomGOISkWc2soB5kRkp1lITjURPVY8YDpKEMYMcQxpYCrICKSraU1Kkfz6GjmWRlR/8iFsKQcqnVIB49igdqd6i06VAdl8vKL3OvYOmiffy7+jBx8VY++XANq1YcxKfp6FFytb0enR69okdXQ0lKcpCU5Ah7rlPn2ixadh2vv7qUNasP0bFTLSbd3pMmpij3KaUi2m1N03ng3jlhklW6LsnK9DJkwKcx01Ryc3xYrUr5lx2MEXkNREKj3ZoWJW6cPz8Wvz0JtZv50aSOHZUDXv8ttRfW5/pI1K04gTR81PWvQDntRkttnyawW30cd1uxSYEiBOfYLiBRSeYr92TezHmKO513UJVkNF0hO8dOzZrHgsdU0HEfzsCaCJOnd2X/kQR8unGMbfuTQZFYcePN11xFVQXV67jo0qdOEa6GyUkjQddKJpIthGgAfAzUMkbmXSnla7G2L4kIa09gq5Ryu38CXwIXAuXW8JUUNWrGcf4FLYK/f/T+KrJjOBAVnrx0yiD5VQX0sE0NpzUWAcOZv8WrZmR0ByWmDI1SJejw6ugFqgRoSLLwIgALEpdF5b39Ct7ARKXkqPBQVdqwhzqtMqASkDcfVRjP6RJsFkmNhPC/rdStjFDG0VrryFeeyUz2Psr51tH0UQfj1VTsioYQxjhaRiJ2JNlhYRT8y28B+a/wi+wkr693rtTxCSMHzILgykkdyZUarz+5OBhJCkUg6HtuY+wOlVZtqrFuTQrekMpqm01lzLUdjG2FoNeZ9el1plEgOObKDhw9kk1GhptrxvzI9i3HyM72IYQh5P3Y0/1IrBJZHFdUGjdO4qVXh5zw/iYlQoWz24cPZcXMdS5IqUXXi96itbw6tsVOvyoEw5bmq1ANwRJUfQ5PR8gQXhQpSHb4qFYll9xcKz6fiqpqOB1GOaz7qAvda6yoKAK6Wc4kQSQyNfdVnjv2NLcn30k9Sz18PpWsbCspqYnk5NhIiM/F4tJI9Vg4mBofdFaNYyvouiQeC7oSaBRjTL/TmbV55rVExL+3I33ZUPt8qDMcoRRcWG1yYgRyWEsIH3C3lHKlECIBWCGEmB3rxrkk3OR6wJ6Q3/f6nwtDCHGDEGK5EGJ5Skrl7HpzPDP2MmmlRYSrCeQ9XfAb2pd3rx/8FzqmLkI6Z4Xs50Xi8RdHhe6fH0NNAJZnGtJTMtDhxT9uqnAHyxYUIVGEUdQUQBU6cVYfPx1wMOOgHS3KB1QIw41uqXbkTseztFA68LP3Ez5wv0imfizfxpKO8RKXP16iCInNIZA2o8pVkXkFBBbAFRS0Mv45UPBfDuwWhbPOb8S1d3VlyeGJ3PZYb2x2lbgEK/EJNuITbUz+aTgOpwUhBJ/OGMmg85pisSooiqBb7zp8/+fl1ChAU7VadRdNmibz67wreerFgQwY0oRLR7fl+xmXM+GmbjH3M6kwFGq3y5vNrpLkoGgxxXCqVXMWedvy6KwWRtAGirwmALHsYv79YmENuVkO38cofMrUjRt5l9NLYkIucS4vimKM6NUVjuQqZHoFbg1URadLfCvuq/oAGhr/l/osmz2bkVKwaWs9Dh5KIi0jjr0Hklm4sDl7fW2jzknBUFKJx0JVaaeqtFMDB++8sY0ah++DlN/h2GLY9CysvA6pn1jzCJPCKamiKynlASnlSv/Px4ENRPEfA5yyDGUp5btSyu5Syu41akQWN1UGRo5qi6sQuaxKR2h+QOBBwcYwgBeJz580H4imBiSnZCCJK8YhA52tdAzn1o0e3C8whgjkd0X53EggFTcJDjejm6XTr1Y2NsWoYLUInZbJOWTrxnK8Vwo2ZglCgzRSgs2qBz+U8SKRq213cZH1KrbrG7hv/zOsyFob3F4ANkXSxiHokpjL2B57efSLXjz08znUbpKAy27BKvJar0bLWQukUfQcWp9GrZIAI1J6y8M9mb/zWp56ZyAvTB3Cwr0T6N7H+MxnZni4b+wsVvy6jxp2B9VtDi68sBWt2lUv9O8Dhn7r+Gs78fXPo3j7owtiFlqZVD7Km812uaz07d+oWPsIAeOv7ojDUXnri0PTrXRh3NDHlLMKwSP0oERgZH1CbLxCZ/1xEXEEKQ1H5mi2FQ2jycA+t8Bm01AVSUNrQx6o+hAJSgIvH3uRlbkrCCQ7GPsr+HwKmblNEDZb/sMGZ5mnpiJo0dqDeuBT0HPyNtRz2LbmEDPfmsY/8w9EtrY9SbKzvdx/x2waVnuFWnEvcPG5X7JlUwl3RSvn6P785sIexUEI0RjoAiyJtU1JfIr3AQ1Cfq/vf+60YOb0Lbzwf4vYv+84PXrVpU2b6qxflxKhNVkpKcChLOpbNRCdDST/BzBW+IxK+Fj7achg21UjJ0vPq4UV4AEkGvYYb3NNSFSLRoJNo1qNHNpWzyHHZ0QHZu5OxKcHMnYlOz0Ct67Q2imxqpIcr8KhbAuqLkiwGCL+NgX62gfSp2Z9Xk+ZwnMH3mFolb5cWe0irNjI9Rg3M01rZ5CaUpVfbvgH6dO59ZWu1B9Ul31bMnhp3ALS9+dEnS8IEpLtPPnVwIhXkqs5OX9Uy4jnH7xqNkvn7cXr1sGoAeOtJ5fSoFkVBl7YNMZxTE4DKqTdtlqKF2OJi7PRsVNtGjRMZMvm1FKaVSkg8/+YVysQ6lwGugBCSFdBAbqURfpyzxY+qqoSt89KaIpWDhrx+fJbA3m0bnTcEqbuVxldS8OuGvY+RxMsPxiPR0oUwO13RY/lqtSM8yEl1FBrcH/yQ7yR9j/eyXiLi2zH6SnODZmRIH1lKg07VmX7iqNoXhm8CAqCWqjskTpSgN2lMvERQFgwrD34fAoPPTWMFasaoCiHEepvVK/r4s3fL6BqLVcRrkjhXHnp9yxeuDeorPLXvN0M7fspi9dMKFInwAqPLFbRVXUhxPKQ39+VUr6bfyMhRDzwHXCHlDIj1mAlEWFdBrQQQjQRQtiA0cDPJTBuueeD9/7hmvE/s3zZfvbvP860nzazccMRHnuyH3b7adBmMtZ7VhTQQCAGEdtGidRGGy+0y4kiRFiUN/BLrHlowMZshbmHLXh1MFI8dcNZlUpY1NiH5LAX1qdb2ZTqYPdxG15NwSsFqV6FYx6FQ26VtFyF6jTk6Xr3MKzKOfyWPp8H9rzA6qNHkVKgKJK9+6tzYH8Cngwv3myNOXesxHc4l3p14zire62Y6RSNOiXx1pIR2IoYLTp2JIeFc/YYzmoIOdk+PnpxZZHGMKm0VEi7feRIrJu56Hi9Gj171SO5qqPwjcsTISZABu0pfhkngo+C9o+pVZ2PdE2gKT68wr/aJcCn6OQGrWtIBDbEJqZp8MF+C98etPLVfivf73Vw3KviBnL889SAFI/A7VHwaQJdhzgSuC3hPtpZOvGD52P+4HNslrzGLfYEKw/80p+4QBqUBDtQE5V4YaG6aqV+y0Tu+fBs2vdvCeS1dP38266sWNUAt9tKTo5CdqaXfdszePKaP4t65Qtkw7oUli7aFyYDKKXR4e+j9/4pkWOUd4zalSKnBBwJrNL4H9GcVSuGs/qZlPL7go590hFWKaVPCDEJ+A1DHuVDKeW6kx23vOP1ajz2nz/Diqx0XZKT42Pp4n38781zueXGmUGJlUpLATdagWX+WJIsRUH3/1cQvZpV+g2jwHAqA4vpgShvzG4wgYRQBPOPWtmQZqeRS5LuVvHK6C6jT+goMk9mJnC3FyrPlampbDwQT70qVi5xjqG56MiH6VN4Pv0poyDLMhSvFv6x09w6c/+zhm3zjpCS7SNOV8gS4e8bl6Ly7C9DSKpbtHy8Y4dzuG/kb3hiKFYcOZRdpHFMKicV1W6fN6w5q1cdIjc3fAXLalOw2y3kZHvQ/G95l8vKlVd1pH6DRA4eyCqD2Z4k+VQDgpYsz7gFo57R0PwubugqVay8VJ9UsBHe3SlX0XDrGlYUXH6F6/z4kKR6VZCQIBXcaMHMexUjKpoYD7qwIDUt+LexCpVJVW/h8+OfMCvnFzJFKmPjryZbS6TLDc3ZP3Mtg1rvx5sLOw4kc+y4KzjZTu2rUbdbVZa8tJUFz/nYse4mcj1WmrY8wPojNtzu8LQ8zSf5Z94Bso57iEuITDUoDls2pWKJEuV352qs+efQSY1dcRDoWskUXQnjDfcBsEFK+XJh25dIYo+UcgYwoyTGqijs2ZOBpkU6o7ouWbx4H/0HNEKvLEKsRUVG/npSIXz/0lZ0VzUPn7+JgI7wqwgQ7J/tQxKrnj2QUgCQpkncx61YpCA0FhOILUi/RNU+1UO8rlBHWgFBZPc+gUcqHMux4fNZqK/14C5HK752v8fP3k/YzBpGKjeSIKoE99A1yZa5h9G8hp5hHCo2qZArjK8ch1RIcNk4tje7SA6rlJI7zpvBjnWpUfMzVIug94AGUfc1OX2oiHZ7wg1dmfrhag4cyAw6rS6Xlcf/248xY9vz9hvL+eH7jSQk2Jh4U3cuvawNYFT/VxqK6iuIPKdVkYS2DojYMNaQUgGP1NGllwQKdvbcQguGC4x4gMAlYGDPbcT3OoP1s32k7zhOlSbxcCwNb7rCVVXGU1Wtyo+ZP5KhZ3BX94epxQaOfbGOJjW9SKBZ/VTWbq/Fhp010YFdq9PZvSYDXTNssw0HHnS2b6qLHR2LkotPRAYqvG4dEop47WLQolXVqEEou0OlY5daJzd4RUFS7PzUAjgLuBL4Vwixyv/cQ37bFEHlzUQvAm63j82bjlKtmou69Yr3Tq5e3RUzenokJZvH/jOv0jQOKDLRipv8uaj5TWIg8hnIwYr59heGA5YXD42+ZegYgXQEBWNZP0fmabMG8KEHpbMQ4JY6Kpo/LpA3x9A/YaDN4XGhkyx1nH4hrGiTznSr1E3OBgT2HBc32W5nofyTbzM+41XuZ5R1Iq3VLgAoDgXhE+CV2IAcjAIrq8z7eOpeSe3WibGuUhgblx9h37YMNM1QPsiVWmBaKIrRZ/vGh7sXaSwTk/JEYqKdBYuv4f3JK5n+yxZq1orj5kndObuvUYz1wMN9eODhPhH7XT66HS+9sBC3+/Qyygr4jXBAbDB6Y4ECEwf8KVFuqYVLAeJv7ypBSKMRgbF5XqFrDpKMLEnSxoWM/esxhKKQuiGV6SN+MbYVghEJw0lSk/g4/WOe+fdB6sWPJFnaCWR4KaqkQ9ND7DyQRLrHhtRDvz+MuTsQZCMRQiFJWDniz2kNUL9ZIknVTz4tpE27GvToXY8li/JyWIUAu93C1RM6n/T4FYFASkCJjCXlXxT9FuzUqQSUNz6ZupqmDf7H0IGf0an9O1xw7uccPVr0/KjERDuXXNomavWpx6Nx7FhuSU63wiKD/w/PRc1vIGXw+fBXhf8/oUoCof/0YElC/irXvM+ALiAbH9l48aDh9TurYfi38Qkdt/9oEV9t/l18SPaJ2Hq7DquXgb02067NLtq12UnnDttJTMxhQNIQPrrjG6qoyUzxvsBP3ilIp0ZSo3iEvxtVnBL56RVI4nwaMyYsJutQ4e/Rg3syUVRjFBsqLixYECgS6jeowncrR1O7wUmGGkxMyojERDt33XsGc+eN54uvRwad1YKYdHtPWrasTlxlUXGRgVTSInzXC2OpX89nO42i1jyrqhPNMgfGgFzhi3g9kChgi6Lb6p8mS3ZWQ8t1o+caTqRiUSIScPu6zubW5EnszdrPtQs+YE92ar5xBPVrR0/rCLTkDhzQpVpxxBnfyzaHijPeyn8+7F/wNSoGn353CeOu7ogrzoqiCvr0a8iv88ZRq/bp02mrpGStistp6bD+tWA399w1m+PHPWRmenDnaixauJexlxeY7xvBa2+cy0WXtCqlWVYSQgoGAkYxr4DAWLLSQ54PLy4IeV6Aho4WYnb1gNUm2i1aZBFXQP5FBJ+K3MuDhkdoZOIt0Hh7hMRnEVFCE5KBPbaQ6HKjKBJFkdjtPlo030eLEfUY98qFzJ32Jxe0GMUibRbvO5+k27u1qdLQBQJUIaiqGEUGiv8LpZoKVYRk6/R9fNxvNnohedGtu1YPaxJgQcGFlWpOJ+Nu7kiteqePYTUxAYiPt/Hn31fx1uRhJCSeXB5jeaGw2oA8jRNp2E4h8QmjvbMXHU3oeIWOV2h40MgNUXGNRl6xlyEb6EEnBw0VUcBcBFkeFWucinf/HnS3myrNq+CsHV6xL4SkX/2WvNV/Ipk+NxOWfsy69P3B11W7wuDHO6LaYqvGgJHuNGB8MyY934uhY5sz/oHOfLn+Mtp0LzlZNpfLyv+9OpjdR+/kcNa9/PDraFq2rlZi41cETIf1FPL6q0vD2vuBIRi9csUBdu5MK/I4DoeFp58dcHooApwMIrzSNVoxQaiqgAx5hCL91amBStbQtaxoy1xh+0rD8fUFY7LR5+lDJwcf2cIXezv/XGoMq45izZusAOpVT8du1VHyfbIURdKst/Fz6/MaM23z1/z6669kahmc1e8sMi/6F9VuRFOtAqpZJA1skno2iFeNZSfdJ8k+4mbbzP0URJ3GCQy6vBkOV170X7UqJCTbGX5t6wL3NTGprFitKhePbM0lI9tgsZT8l+mpJnpzABn2U55tDVmF8ttODUMMKtAxynBCY8gxSqMyL+D0akIG7aUgRFIryhw1i4/EhBRSXnuVfXfdSeb8eQz8YCD2ZDvWOCtWBzRusp9aNVJoLavwfo/xOC02blr+GX+nbAWM9qtNRnWl2+iGWBxKxDHc/uNbHSoXPtCOi29oy6NTzuGah7tSvU7JyFkB+Hw6c6dvZ8qbq1j2974S13itGBRNg7UE81yDVEiH9bNP/qVdy7eomvB/dOv4LjOnbynW/vv2HY/6vM2mcuhg8apJk6sWvYvKaU+o5FT+n4mUacnfKjVsGymDtlmN4qwGHODgP2EYWy8SL9Er542djcQpIURYR61oVGscz3PZlzLggdY4qxhlXgkOL4qQ7DuSwPRFLfjy9/ZMW9iSvYcT+GfyOg6vTw/uP3ToUP79918GDx7Mw08/wE/xLyFtR4mzazgtevSq3ByNo5tiytQFefDdvtz8bE8atqpC9bouRlzbio+WXkx8lcoRXTIxOVHuvf9M4uJsqGre58vptNC5Sy3sdpX4BGvYa+WVvBWp/Av9kTbLWFmKfk5Bm+rPVY1whP0/2lDDnpdIdCGN1Sh/5DZ038Aq2OaUeDKyFGRuLtLjJu3rr3Aqh7lsxeWc9XIf2pzvwOHSEdLQe6nvqM773cfTOK4a96z+hl8Or6X5o1egxjm4/M1utOxfE6tDwVHFWJKXNoFmgVZ9avDI/MHUbFo6K0gH9h6nf5uPuP2qmTz34AKuHv4jowZ8Te7poLkegpSga6JIj5KmwhVdffTBKh64b04wQrplSypXX/kTUz+7iHPPa16kMQYMaszGDUfweMIdF69Xo1374i0dZGV5oqoFmBSTGJn/eUY4z8fVBAgpCPQq1aQMNhK0ogYLuaLGSAV4kFj8wtpBI+4/QKiTnC18WKQSUVolEDjiLLTvU4uVsw/QYmQ9BjzQhvf6/0HmgVz2Hcli0bqGaLpxP3g828GidQ2pFpfD6j5zuXvrMFxVDe2CGjVq8PPPP/P2229z1x13sUV7iCvs19NK7YKmR5ajWZwq1dslFXo5FUUw8uZ2jLy5XaHbmpicTjRsVIW/llzD888uZP6fu6hTN5677j2Dc89rTsrhLPbtO07TZskIAW1bvkV6WjlsuR2wlyLQZCVUIjXSUSjMdQgUY0mMaKlAhqkKWDG6/inkRbk0/2rVccWLW9dIVux4dT2o0KL5BQedqs7BdCeJTiPvX3o8HJ8zhxo3t6LJ8CYc++IHpC9U11QhyZLImz2u4smjf/Dk6p+x/taDh7s/jD3Owi0z+3FkeyZHd2ZRu00iVeqcmqDRPdfN4sC+TPRAy0O3xtqVh3n9mSXc+9+zTskcygulsdxfFCpUhFVKyVNPzI9Yzs/J8fHEo/OKPM6tt/WkSpI9TOrE5bLy8KNnEx9fvAjUxg1HcTorSSJ/OSM0pxXCl7fypxXoIpBXpRERIYiCFx2Pv72r5s/nCugHBpAC0oWHTOElGx+5+PChY3epVK3j5IWrF/Ds2HncO/BXJp3xC6O+P5PzPhrMP9vrB53VAJqukHLchS/Xyz8f7wx7TQjBzTffzLLly6ieUJ13c17iR89UfPkqXRWrQkI9F02H1C76RTQxMYmgUaMk3nznfP78+2oGD23G5LdWcN/ds8k47qFzl9okJNiwWBQyMz2FD1ZWRFmtKkhFpajk4MMTbHVt2NZcf46rBw0PPrxBd9V/XCHIwkem4iNN8ZCmeDgujCJWn65gUXU2H6xCZq4RI9PS0vL2tUaLmwlcFie//DqDK6+8kkceeYSbbroJzS/kWr1pPK0G1DplzmpWpoelf+/Lc1b9uHM1vv1k/SmZQ3lBUjqtWYtChYqw5uT4SE2NXiW9beuxIo9To2YcC5dcyysvL2bOrO3UqhXPrXf0LDBCu3nTUR56YC5/LdhDYqKdm27pxm139KJOnfiISK3JCSCLfvcUKMqKiCb481sNGSs1bHvIV6Tgz9kK5F5JKbGiRuqWCvBIDaGAqiq40VFdKof3ZeHL1fH4pU32bs7gnj4zqJ7ghKz8SQqBeQi8uZCyMfqSfoeOHVh/YC23jrmZD36awm7Xv1xXbSL2Y61RrAqtL2nIwBe6oqjhV8qXq7Fj5h5yUnKpc0ZNanSoWqTreDKkpuQwb8ZOAPqd35iqNczUGJPywcEDmUx+eznLlu2nbdsa3DSpB02aJEVst29vBmefOYXM4x5yc33Mn7eLj6esYdLtPfhkyhoOH85C04rj6pUtoXYRwm2jEZCVEc+H7htQZAGjzTVILKhBVSxNGulUhs61wBawsRKswTWucLxCo5pV8srMTlgUHa+ucHbrw0y4oE1wm1oXdGfPh3PR3SHKK4ogrllt4mtXY+rUqdSrV4/nnnuOgwcP8vnnn+NylVxealEo6H3g855mK6wyIFd56qlQDqvTaaFKkoPUKPJTjRpXibJHbGrVjue55wfB84Vvu3dPBuf0nUrmcQ9SQnaWl+ee+ZutW47x5jvn43RacMfoKGRSBGTe/wozrKG7SIxK+ojt/E5mUGc1BCtKcMkqP1l4iRdW1JBjy0A1FSJotI6l5qJKQVJISwIhwXPQy4GDPmqixvxgWe069XvErig9PmcVN7nr0KnbZTz+73Se2vcUN7ccxFV9L6LTWyMRaniB39F1x/hh+Cx0r47u1RGKoNGQegz54OwIx7ak+PnTjTx5859B6aynb53HI2/248Ir2xSyp4lJ6bJ1ayrn9JlKbq4Pt1tj0d97+XjqGn6ZOYbuPeqGbfvkY/M5lpoT/Fx7vTper87zzy4si6mXCOHFqjKY3x9wqRRCl//zbGc0mT+flAh0FKEEnwuM5ZMSDQ0HakQNQegYQhV4PBZ8uopXM2zX35tq0WpHY0b4N6tzSS8y1uwkffk2YzdVoMY5aPHoZcbvQvDss89Sr149brvtNgYNGsS0adOoVu3UVeYnVrHTpkN11v5zOMxZs9oUzh/Z4pTNo7xgpgQUASEEDz58Fi5X+BK802nh8Sf7ldpx3/jfUnJzfGFv1JxsH19/uY5tW4+SkVEOc5wqEiHv/cIX88MJyKxEDpnXFCB0yczrTwMIJeCUakiy8QWltgpynI3obN44tpCPUka+5bIAViFxJtnoOLph1HPJ3ZvC3neng5ScWb0pX511Hb2qNebVjb9x0/cvs/GX8C9SKSUzxv2J+5gHb6YPza3jy9HYNXs/G7/YHvUYJ8vBvZk8efOfuHM1crJ85GT5cOdq/PeWeRzcE72Y0cTkVPHQ/b+TkeEOBhC8Xp3sLC+3T/o1YtvZs7ZXqAhqoQgi7F1+OxSqGBDIV9UKsLqxqv/1kPFjbaMooGjg08Nvsj0+hR/f2po3bVWl9X+voP0b19P4lvNo8chldP38Thy1k8P2mzRpEl9//TUrV66kT58+7Nq1K+pxS4uXPhxKYpIdp199xRVvpW6DBO5+/MxTOo+yRiLQ9KI9SpoK5bAC3HBjN5569hxq1ooDoGHDRN5+bxjDhrcstWMuX7Yfb5Swv8+nc/YZU06/jlalRCBHNVbhQEH7FdXNNRQEjKWt8IpW40UvOul4yMSLuwCVgIASQYBAsZZEkqlIjqEFj2A4wDqtugtuXnketrjo8dfUP1cjQzRWk20uXu5yKfe3Gcw/qTvoM24406dPD75+bHMG2YcjG1T4sn2sm7K5SNejuMz+flvU56WEWTFeMzE5Vcz/c1fU5cr161KCrVwDxMdX8tqDaDLR5KsFoICCLFGwXVUDVlpALpFNBWxWjTgRYzUrLTI3OK5ZbWpd0J3kXi0jVpICXHrppcyaNYuDBw9yxhlnsGrVqpjzK2latKnG/I3X8MCzfbhmUmeeeXMgv/1zJUlVT76DVkXD1GEtIkIIJtzQla07byUt637WbrqZS0aW7lJk6zbVo8qcaJokMzN2xyOT4lFQlWthhMq7BN/UocNIQoqqBD6MQoKIWKh/El6/Hmus6IOEYJasRAblYALuZpYi2Sd87BE+9gkfB2061y25jMQCNAGlV4tIDhJCMKphVz7rewW1qlbnggsu4NZbbyUnJ8dIAYhxqfRSyqvyerSoUSlN0/GYaTEmZUysolmLRQkrsgWYeFO3YLSsMlNgB6uCiLFLYCzFP7YHnUzFxxGRSxYedHQ6nZHMm3f/Qc2kyPQ9ISQd++cVjkqpF0vPtG/fvvz111+oqkrfvn2ZO3dusU7rZEhMcjD+xs48+lJ/LhzdGru98r9/IpBlV3RV4RzWUBSlZC7Irl1pbNxwJKY81W139Do935inEhGZw1pcAqoCgfFChwnkaOV3hmO2BxDGWG40cvCRiYcsvHilZjQJkOBBJ014SBMe0vEE28eGjhF4aJokZVfBGr9JZ7aNGe5omZzMX7/N4vbbb+eNN96gZ48e7Nf2YI2PfF+qTpVWo5sVeKwTpf+wxqhRRNctVoVzLmhSKsc0MSkKR49kRw0s2GwKoy5vi5ovp/vGW7pz8SWtsdsVXC5LhdBeLTYxoqyhBFVY8jmNobqu+bVefei40QxF6xC1gmyhkWnz8uBLzWnRMJObxizEbvWiCMMyW1QNl9PHNc90Qx5ejfbLleif9UH/aiDayjeRetE0Tdu1a8eiRYto1KgR5513Hl988UXRrodJiSD1oj1KmgrtsJ4sO3emcVavD+nR+X3OOXsqLZu+wdw5OyK2a9mqGt//dBmt21SP6GAUDUUBq7USGr/SRuTPsYoiYl0EghGFgNMa1CmMXh1b2FiB7li6Pyrrxocu4Ljw4RU6PiHJUjT2iuyYc5USbIVEc+JaNcCWFNI6y/+w2LxGS8I0jSsONOeFDlexf/MuenTrzq4B61BdKqrDiPda4yzUaF+F1qMalkoXlqZtqjL+js44nBaEAkIBh8vClbd2olnb0lcnMDGJxYRrp3HkSHbE802aJvPCy4MjnldVhc5dagMCKY1OWJUSEb4CFQ2v315GNFtBBleaAvmqgTUpRSiRN9gCvLqkfqe2IARd2uzn/+6ZTt8e22nRKIXz+23m9S+t1Kmegj73DkjbCkjw5cCmb9AXP1vk06pfvz4LFizgjDPO4IorruCll146octjUjwkZZcSIMqitVj37t3l8uXLT/lxQ9E0nY5t32Hf3uPoet41cLksLF4xgcaNk6Lut3jRXi4a/hXZWbFTARwOlXr1E9m1Mx1fIX3fTWIgI4KkQNHTBqzhlVyoMWRXIK9gKv/rqhRRu20hwYUVEWU9vq7iwurNp+eKJLGanamHLytwzgDH/lzCvre/xZdtnLnF6kN1qLg6tmHPnCPoucb77pgnk//b9hOLUjcxeMBg7hv4MPYMB87jO8letx2kxF6rCi3vO5/k7iUf+Vy34jC/frMFKeG8y1rQrlvNEj9GeUUIsUJK2b2s53EqKQ82uyCOHs2hVbM3oqaltG1Xg8XLr4t4fsmivYy44MsIXe9Ki9+mRigChBDN3ilS+PNVw5935GWxhqGqgpWHbsR5eCHaH8+C5jXCbRY7xNXAOnIy+tLnYdecyDCcYkO55EeEIzli3Fjk5uYyfvx4vvnmG+68805efPFFlKJElk5jTsaGNbQ0lffGP1mkbW9Lv7JEbeVpu869YP5ujh3LDXNWwagq/ej9VTzxVP+o+/XqXY8GDRLZuiU1IpfP6bSgqgrvvDeMs85uyN23/8ZPP25C0ySKKlAVaNu2JghITc1h756MMtMzK/dEW8qS4XJWxmbRjW+gXCqvsEBGGGND1sXYVs0n8wLRjXfe3KLIaQE1uyeQvjQrTGBaCME93/Qt5IQNkvr1xHvoKEd//h2hqkifJL5LW3JkTXTvweB2ybZ4nm19BT8dWcHbf89i1b+jeLzfdXQ4UsXIhQVy9x1j7f1f02Xy1cQ3r1Wk4xeVdt1qnlZOqkn5JjPTjRojRSw9LbIwEeC9yStjttVUFCpfMa0oWf3M/JLVARKq2HHFWVGa9UckNURf+wMy6zCiYW/UVucirE4jshptzVi1QuZ+KIbD6nA4+PLLL6lTpw6vvPIKBw4cYMqUKdjt9sJ3Nik2UlIqCgBF4bRxWFMOZzFnzg7sNpXBQ5ty6GBW1E+v16uzZ096zHGEEPw47XJGj/qOzZuOYrEo6FKnY8daLF92AI9H48orfqD3GfX5/OtL+PDjC3G7fezZnUG1ak6qVTeKbjwejYZ1XyE76zS5uy8JAp+RApb4A4SZQgEeqWNHJb8b7MXocKWHKK8GRo5lkC1WBVVXkPmCOapF0P6sWvR/pDGfPfIPh3Zk0aRTMtf/rwcN2yYV7RSFoObl51NtxAA8+w9jrZqEJTmRNbd9AvlukIQQjGx8FiMfmciNz9zHLd8+x8W1e3Fjo6HYVaMCWvf42PP5Ito8elGRjm9iUhFp0KAKVZIc5ORkhj1vsSicOyx6Q5ijR3NiOnAJCXYaN0nCalVYuzYFq0Xh+PFy3PWqKMjo9qwwdGRU7WrFJrAJNSyq7XRZuOOx3sH6EqVaU5R+d0cOWrUVpO+MdFo1LyTUL/YcFUXh1VdfpX79+tx3330cOnSIH374gSpViqfPblI0yirQdlrEzd+bvIK2rd7irtt/49abZ9KyyRt4vRo+X+RVd8VZGTCw4CXUevUTWbDoGhYuu46fZ4zhzrvPCEpfaZpESli0cC+d2k0mLS0Xp9NKy1bVgs4qgM2mkpxsdgc6ZQijgMrrl5vyoeNFRwqwOI2PQUDuJeDURlMIkMA5FzQhuVrk304osGj2Hu665jcyq2rcP70f/50zuMjOaiiq04GzWUMsyYkAJHZqiLBF5tjpXh/dh/RhzpTvGdWwDz8cXMKN/77D9ix/NFaXZO88Uuzjm5hUJBRFGE1cQoqnHA4LVas5uf+B6H3eh18YWwoxM9PDr3PG8vv8q9i9/w4++vjCCJWBikj++oDo2+R7XmB0uFIFcfFWnC4LdrvKHY/05o0vzqdeowQURVC7XjyPvdqfK2/sVOg8lHbjQc0XAVUd0GwYwp7nZMqMtch/b0cuHoFc/zAye2fMMYUQ3HvvvXz66acsWLCAvn37sn///kLnYlJ8TFmrUmLD+hT+8+AfuHM1sjK9HD/uISvLyz13zuaiS1qFNSGwO1Tq109k5KiiyWQ1a5ZMt+51eOO1pXi9kR/+jHQ3b72+DCllRC7rv2sOcSw1+lKVSSkhDMFrXRgFBcFwgwUatEwgMdlG3QbxXHFDB4Bg0UHoP4td8MJHQ3jn1+HUbhiPK95KXKIVq91IUtiyNpW0I7ks+X0f1w38ia/eWVsiecx1L+mO6rRByLKn4rBSc3AH7DUTqdqyPrc2Hcb/tRlPmjebiWsm892BRUhVkNiu3kkf38SkPCOlJCPdTZs2NUiu6qR5i2TuvKcXS1dOoHad+Kj79OxdL6YsHMDUj1Yz9aPVZGV6GDi4Ca64Cq7b6rd5eoziVgjXZw1FCuh/XmOee30QT754Dn+vv46LLmvNf+75nT2HMsAFB45mciy9aN9pokpjlMFvQo0OICxgT0K0H4/SIy8aK4/+DSuvgpS5kL0NDv4My0YhMzcVOPbYsWOZMWMG27dv54wzzmDDhg1FmpNJ0dFl0R4lTaUvunrskT/53ytLIvJN4+NtvPzaEDRd8t47K8jK8nLJpW245dYeJCYWPffl8KFMmjd+I+brtWrFkZPr43iGm6bNkvm/FwcxZGgzZk7fwjXjfyL7dEn4L0lkAfmlhRBrv9DnO3WrhS9DZ8fmtODxAGwOhRvu6h7sbCKlZMM/R8jO9PDAuDkcORhZoawogipJdl769ly69qlT7PmGknswnZ3v/UHqoq1Y4uzUHdWTepf2RPid2C0v/8rB6as5mnGM57f9yKJjm+hdrRVf/vEzjTqUXmON0w2z6Kr8MemmmXz7zfpgMazTZaFDh5r8OmccFkteXGbjhiOsWL6fnTvTeO3lJeTmRhZpCWFE6+x2FSEEupS8/NoQpn6wiiVLKmHEzm/fQlus5m+3arEIOnerzfCLWjHumo7EJ9hoV+dNMtLD0yScTguf/HQJZ/ZrcHJTkhIWnQu5eyNfTD4T0eW9Qsf4559/OO+88/B4PEybNo2zzooeaT8dORkbVl9pKm93PFWkbe/LGWsWXRWHnGxvRGEVgK5LPB6N8Vd3Yuy4Dic09tEj2fTq/kGB26SkZAePv23rMa4c8wM/TrucTp1rkxMj4d+kEILFA/lyOk8oQyuS1SsOGUoAqhU0cMVb0DTJoAuactvDvfKOJwRtu9YgM8ND2pHokQVdl6SnurltxAx+3Xkl8YnRhc2LgqN2FVo/clHM15vfMRRn/apYv1rC/yVdy69s4uUFn9NjYB8++ugjhg0bdkLH3b/8KCve3kzWwVyaD6tHx6uaxuzWZWJyqtmyJZWvv1wX1skqJ9vHurUpzPhlCyMuaoXPp3PNlT8x67dtCEXEVHmxWhV8Ph1dl2H2+eYbZhRrTjVquEhJibyBLZf4E/ZDC0kDEVdVEei6UYC2fMkB1q1J4Y1Xl3Lhha0inFWAnBwfH7y58qQdVrRscB+I/lrGqiIN0aVLFxYtWsTQoUMZNGgQX3zxBRdddNHJzcsEKLuiq0qfEjD8wpY4XZFLOZquM2hw05Ma+3+vLeXokchOHqHkd5Zzcnw88/Rf1K2XgC1KTmKAwFJVSTVHqGzIsGWtcJ3BvHar4de+WFFZAdm6F7fio2ajOOasHs8bnw2LqtXojLNgcxSu4fjHj5EavyWJUAT1L+tJ7+9u5ezZ9/H07A9YvmI5tWvXDuuQVRxWT93GZ4Pmsu7zneyYc5A/Hl7FlDN+w2N2eDMpJ/y9YHdUO5mV5WWOX1d78lvLmfXbNnJyfAVKEiZWsZeIza0wzmo+pF9n1ef/fyA6Hfgey8nxcexoDl9MXRtzjJRDJXDuih1EjBQMa1KRh2nSpAkLFy6kU6dOjBw5krfffvvk52aClEV7lDSV3mHtc3ZDRoxoSZw//0hRBE6nhYf+czZ16yWc1Ng/fr8x5muKQkyHdPOmowB06lI76uvx8VbuuKsX5w1rTv9zGmGzVfo/04khDMc1kNuthzzyZK0koemqsQoNAvvkL7TSpGTzllSSa8QukFNVhStu7YCjgMYAXq9G+rFTn7Pcrl07li5dyh133GF0yOrZk3///bdI+3qzfcy+cwW+HC1YzOvL1kjfncU/728txVmbmBSd6tVdMTpcqdSuHQfAB+//U6QVrfh4W9QVudOBgO0MBAE0AdneyGvm80my3bGd/v6DG530XIRigToXG45rKIoDGl5drLGqV6/O77//zvnnn8/NN9/Mww8/XCpNVU4XJGZr1lJDCMHkDy7g869HcvV1nZl4U1d+mzuOO+/ufdJju6JEbgNcPLJ1WO5UKO3aG/qVjz3RF6cz3Mlxuazc9+BZPPHUOXz17aV8+PGFOJ0VPNn/VBHSChVCI6/5fy+4SjawTQApJSuW7mfn9mMsX7yfX77fzN7dGWH73PRoD0bf3B6bPfwmJSDSrSqC3gNPcpnsBHE4HLzyyivMnDmTlJQUevTowf/+979CjfbBlamIKI6AL0dj0w97Smu6JibFYsCgxng8kbmoqioYd2VHgJh6q6G44qxMuKHLKbG3+e1+WZOYaAu3n4EggIx+k++McXNusSjccHu3kplUi/ugxiBQbKDGG85r/TFQ74piD+Vyufjhhx+4/vrreeaZZ7jmmmvwes1VohNFFvFR0lR6hxUMp/WcAY353xvn8n8vDva34zt5JtzQNerziiK4+JI2TLqtB5Z8fddtNoVbbu2B2+3j7L6N+PzrkbRtWx2LRVCnbjxPPXsOt9+ZlydZtaqT114fWikkVUqVKC0Cg0Y3XxOB0EhsYU6r8OfLXjPqJ3q3e58RAz7njut/5ewOH3L/pNlBp09VFe545gwWpFzLmYPq43JZgs6qM87C0NEtaN6+bFuXnnvuuaxZs4bBgwdz++23M2zYMA4dOhRze3uSDalFvz7OqqYot0n54N13VkYVGO3Zqx4NGxkSSRde3DrqipcQRndDu13l2us6c9sdvRh/dUfDISvFbKzyFMUVAjp1qRVTMUHLZyOdTgsTbuxKclUHlpDvJYtV8MHXI4hPKBnbIBQbot3zcOZc6PIh9JmHaH5P1A6DRcFisTB58mQef/xxpk6dyogRI8jMzCx8R5NwiqgQYKoElDKapjPlw9V8+P4/5Ob6GDmqDbfe3pOEGB9AKSW9u73Phg1Hg88JAa1aV+PvJddyz52z+fTjNXi94bJGVquCqipceVVHnn1+YIG5rGA0GWjW8H+kp7tP/iRPN/K9vRUMZaiARlxx8loVQPj3U1FwuSw8879BXHZlu7DtNE1n9jfb+OWzzaiqwoVXt+acCxufsKEtaaSUvPXWW9xzzz0kJiby0Ucfcf7550fd7r1OM0jdkhGm7211qVzy9dk0HXxyqgcVFVMloHzRpvmb7Nt3POJ5u11l3+G7sNlUUlNzOKfPVA4dziI7y4vdrmKxKDzwcB9q1HTRp0/DoHMLsHLFASZO+IVNG49GjFsZOatPA5Ys3hcpwedXZAm0ZrVaFc4b3px3Px7BsaM5fPj2Pyz5ay/NWlblhlu70aJNtTKZf3F5//33mThxIl27dmX69OnUrHl6de07GRtWVzSTEy3PFGnbx32jS9RWmg5rCNdc+RMzZ2wJSk3ZHSpNmyYzf+HV2O3Rl0A8Ho3/vbqEj97/B4lgzNj23Hl3bzRNp0XjN8IqV/PjdFq4bHQ7Xn/rvALn9evMrVx/zbSgwyqEkZ/Vuk11Nm08ElWaxSSEvP6sII0ot+bVQYpiy2MFnFbFb8A7dK7Jb0uuLIVJlz7r1q1jzJgx/Pvvv0yaNInnn38epzM8VzdtRyZfnP8H2YdzQRFobo0+D7fnzPvbxRi18mM6rOWLejVfjtqFymJR2Lnv9qBMYXa2l2+/Xs9fC3bTtGky46/uVGAdwxv/W8p/H58fkfvqcFiY9fs4XntlCd99Uzk0Pj+YOoJbbpwRmToRIiHodFh48tlzmHBT9JXFisa0adO4/PLLqVu3Lr/++ivNm0fviFYZOVmHdYJaNIf1v1rJOqzmOrOfDetTmDF9S5guqjtXY/eudH76IbZQsc2mcs99Z7Ju8y2s33wzjzzWl/h4Gwf2Z4Ytl0QjJ8fHl5+vJSMjduR086ajXDXux7DoqpRQtZqTufOuZMSFrWLmypr4Ca26EuDx6MEKxkBhVlHJv2VmBW7XGCjIuv3222MWZCU1iefG9RcweuY5XPjxmUzacdFp7ayalD/6ndMoamW/rkveeWs5mmZEDV0uK+Ov7sS7HwzngYf7FFp0O258R+LibWFjOxwqPXvXo3OX2lSpUnnSYoaPaMkjj/XF4VDz8vBlwHQa52+1qYy7pmPZTbKEGT58OL///jtpaWmceeaZLFu2rKynVCEwiq7KJiXA9HT8LFu6P+qSbVaWl/nzdhV7vAYNE4OGsiAsFqVACZT3Jq/E446M0h7Yn8njj8zj3Q+HF6nRgdNVuOzS6UTgL5OXIB67CCuU0C1sNpXzL25RCrM7dTgcDl599dUCC7KEENTrWZ3m59XFVa3yfEmbVA7++8wAEhJsEXn+ui556YVF3H3HrBMaNynJwZ8LruKC4S1wuiwkJzuYMLEr33x/KWAENCoDXbrWxuGwcOvtPVm+6gaefnYAl1/eFqfdQpUEOwkJNqok2fn6p0txOMpXsdjJ0rt3bxYuXEh8fDznnHMOM2fOLOspVQjMoqsyplbt+KjSKHa7SoOGVaLsUTBxcTZuu6NXgUoCYDgD9evHvtPfvTsdLYZdnPz2ClYs309aWuFySYmJDho2Six0u9ORgHpAnhMr/YVakU5sIFjrclmpVTeOm+/ucWonW0oECrIGDRpUpIIsE5PyQrNmySxdOYFmzZMjXsvJ9vHZJ/+Smlo8/eEADRtV4dMvL+HQ0XvYtf8OnnluYFBF4MJLWgXlEkuaklYRUFXB8Asjb67j4qy88XZeSlrDRlWYeFM33psygjWbbuS8Ec1JrGanbsMEVq8+hNdbOZz0UFq2bMnChQtp2bIlw4cP56OPPirrKZV7yirCWrlul06CgYOaEJ9gIysrvDOWalG4cvyJLYM89J8+1KkTz8svLiblcBZutxY2tstl5f6HzorIj92/7zjffbOe9Aw3TZokoapEdVqtVpVFi/YWqeL0SEp2uSn6KQ/kvxKKIqhTJx4pJfv3G5Wjgauq+DvAWK0K557XHHToO6gRl41rR1z8iXeuKm/UrFmTadOmBQuyOnbsGLMgy8SkPFGnbgKxyvrtdpUd29OoWjW2lvKJMGRoMwYNacqcWdvJCmlGEKgxAInbXfgqWzSqVnOyb29kIVnw9aoOjh/3RBT0CgG9z6hHg4ZV+PnHzYDxPfPUs+cwbnxHjqRk8+zTC1i/7ghn92vIdRO6UKt2fMT4mqZzxeXfs/bfw8Ec3kce/oM5s7bz1XeXVrrvktq1azNv3jxGjhzJtddey759+3j44Ycr3XmWFGWlcWE6rH4sFoWZs8dy5Zgf2LI5FUURJCU7+GDK8BNuMCCE4NoJXbh2QhcAli3dx+2TfmXr1mPUqhXHI4/15bLR4fmA06dt5tqrfg62jjXutP3VQvnIzfXxwrN/F2kumlZaQfqKSSANQFUUateJ4+8V1/LqC0uY/GZkYYkE+g9oxMtvDqVRk6RTPdVTihCCW265hf79+zNmzBiGDRvGrbfeyvPPP4/D4Sjr6ZmYxKR9hxps3nQ04gbe7dZo3Lj4q2SFoSiCjz+7iN/n7mTaT5uIT7Axekx72rargRAwe9Z2rhr3I1kn0BWuIGcVICvLE5FyJgTccGNXXnh5CAA5b3lJT3NTo6YLVVXIyfHy8AO/8/13G1BVwfp1KTRqVIWxV0YGZGb9tp3161PCCs5ysn3Mn7eb5csO0KNn3WKfU3knISGBX375heuuu45HHnmE/fv38/rrr6OqZjpdKIEVybLAdFhDaNo0mb+XXMue3enkujWaN08+6Tus1NQcMo97OHYsl5EXfYPPq6NrkpTD2fzw/UYuubRNsGhq1840rhr3U5gIdna2D6tV8Tuc4ei6JCOj4hb9lBmB3tkC2rStxqdfjyQpycmnU9dEVVywWBW+/OnSqG1ZKyuBgqwHHniA1157jT/++IMvvviC9u3bl/XUTEyicve9ZzDjl61kZ+c5iE6nhUsva0u16q5SOaYQgoGDmjBwUJOI1/r1b1RqEbpokVspYepHa7jxlh40a5aM02kNa4Jw600z+fmnzbjdho3LzvZx1x2zqF0nHpfTyoEDmXTtXofGjZNYtHBPVEfb59NYsnhvpXRYAWw2G1OnTqVu3bo8//zzHDhwgM8//zxCPeV0p6wSQ8wc1ig0aFiFFi2qnpSxSU3NYeRFX9OyyRt07/wu/ftMIe1YLpmZHjwejexsL3/M3cHHU1YD8OXna+na8d2oHVvyL/uEcjp0mCsVmy/AZlcZfklrGjdNQkqJxx39Y6jrMlKf8DQgUJA1Y8YMUlJS6N69O6+//rrZ1tCkXNKufU1+nHY5HTrWRAije9Mtt/XgtTfOLZP52O0WnnluQFgdg6IYTnRprTT7fBqT31rOMX8b6AP7j7N3TwapqTn89OOmCJnFnGwfY0Z9zyUXfsUN102jc7t3OPuMD4lzWaPm0RrtbiNTCCoTiqLwf//3f7z22mv89NNPDBo0iNTU1LKeVrlBYvgdRXmUNCelwyqEGAU8DrQBekopiyTUV541/UqKAf0+Zs2qQ1Ed0FC6dqvNE//tzyUXfl2gY6qqIiLKGujCFI2LLmnFjF+2Fnr805369ROwOyxs23oMh82Cz6NFXNNGjavw9gcX0PvMeqdtTtPhw4e59tprmT59Oueddx4fffQRtWrVKutplQkVXYf1ROx2RbPZui6jSl2daqSUPHjfXN5/9x88Ho3kZAcvvjqEunXjuXb8zxw4UPKdlmx2FSkl8fE2Mo970DSJqgqklPh8Rfu+t1oFVqsaJvMIRufFDVtvPm3ahX/zzTeMGzeOpk2b8uuvv9KoUaOynlKJcDI2rJZoJsdQNB3W1yhfOqxrgUuA+SUwlwqD16uRlRV7KX7D+hTWrT1cJGdx86ajhTqrTqcFR767ciGI2cxAVQVDhjajZs3SWQYrLuXZx9u37zjbth4DINftQ0qC7XQD8z50KItRI76mXy8jSn46EijIeuONN/jjjz/o2LEjM2bMKOtpmZwYld5ulwdnFeB/ry5lyoerg98Fx47lMunGGfh8Opu2T6JJKeTEe9waXo/OsdRcvF4dXZd4vXqRnVUAr1eSlOykfoNEXP5oa/MWVZkx64rTxlkFGDVqFLNmzeLAgQOcccYZrF69uqynVC4oKVkrIcSHQojDQoi1RTnuSTmsUsoNUsrYqvqVjOPH3Vx/7TTqVH+Z+rVeoXf3D1i2dH/Ednv3HsdaBDF/VRXk5moFOqtgFEx98vlFtG1XA7tdxWZT6dCxFg/+56yoUlyaJpk7ewc1a53Y0o3dXrKZImW5guxyWXn7vWG0bFU1wnFWFBE+N2H0zfb4dCw2BeG/DLk5PrKyvGzceIS7b/vtlM29vBEoyFq+fDm1atVi2LBh3HbbbeTmnp5OfEXldLPbZYXPp/PCcwvDcmrBaBjz5GPz2bjhCAcOFlxcVZoohZj5/fuOs+yfCSxYdDULl13HitXX07ZdjVMzuXJEv379+Ouvv1BVlb59+/L777+X9ZTKlFAZyMIeRWAKUOScnVOWwyqEuEEIsVwIsTwlJeVUHbZEuWzkt/zw/UY8Hg1Nk6xfl8Lw879g5860sO06dqwZTGzPT6DAKj7eipQUKTfS69W4etzPSCm5+97erF43kb8WX0PXbnVwRtF5tVoV6jdI5K57eheqA5t/bvEJNmrXia2KULWak6ZNk4ocNRUCHI6ySZW22VWmfnYhY8d14JsfLqNuvQTiE2zExVlxOCyolignIYxiLLdXw6uF67B6PTrTftxcpIYQlZnQDllvvfUWq1atKuspmZQClcFmlyUph7PIdUdXCNiy+Sh792Zgs576umen00LtOvGMuKgV1asXXEwkJbRoWY1mzYpXgHwsNYdvvlzPt1+tJ70IOuHlnfbt27Nw4UIaNGjAE088cdrn8WtFfBSGlHI+UOQE4UI/LUKIOUDtKC89LKX8qagHklK+C7wLRj5UUfcrL2zccISVyw9EFOZ4PRqT31rBs88PDD5Xq3Y8V1/XmU+mrAneXVssClWS7Nxyaw+OH/dwVp8GjL70uyJpqEoJGRlu1q9zs2N7GkuW7Of7ny6jz9kNSU52kJ1PO9ZiVbhmQmeaNk1m1650nn5yAR6PFtM5Tkiw0bpNdXqfWZ+bbu7OrTfPZNfO9KjbnnFmfc7qU5+H7v+j0HmDoYEYi4JycEuCOrXjGTykGQBNmiSxduNNzPtzFwcPZtKzVz1uun46SxbvK9aYmib9OWGlMeOKQ6Ag69Zbb6VZs2ZlPR2TfJSE3a7oNrusmDtnB/fdPZutW1Jj2rfmLarSsWMt3FG6GIYiFFAVpUSLPvv0bch3P14GwO9zd3DRBV9F3c5iESfUGOHrL9Zx+02/+lOrBJqm8/YHw7jwktYnM+0yp0GDBixYsABN007bWgYotqxVdSFEaPL7u367ckIU6rBKKQed6OCVie3bjmG1qmG6dGBU8G9YHxl9eP7FQbRvX4O33lhOWlouQ89rzoMPneUXuDaIi7OSnu4u1jxycnwsWriXpYv30euM+kz/7QquuOw7tm09hqoKHA4L77x/AY0aVWHm9C0cOpTJQ4+cRfXqcdx0w/SoBjQry0OzZsk8/ewApJSsXBmZ5hDgz993Mn3aliLPVwjBvfefyYvPLwo676oqiIuzceVVHfnj9x3ouiGEnXI4i23bjqGXkG3+8++rwnLZVFVhwMA8+ZlHn+jLpRd9E/E3zZs8YYk4QkDP3nX9ouAmgOmsllNMu102LF2yjysu+y62TcGIcD7yeD9q1orj2gmdef/dlXi90T3bS0e1pXp1Fx+890+JFNA6nRYm3dYz+PuAgU2494EzeOG5RWHbCQG3TOpZbMdsz+4Mbr/p1wg1ghuvnc4ZZzWgZq24E598OSA5ObKb2ulH0dqY+zlSkkVXpg5rEWnbvkZUg2F3qPTsVS/ieSEEV13Tmauu6Rx1vNWrDsZMG1AUCnTavB6NJX6HtXHjJBYuvY4dO9LIzvLQuk11PB6NQf0/YePGI2RleoPyJA6HJaoh1XX4/ruNPP3cABKr2ElPi11QFtrRpSgkJzu45/4zad6iKq++vITDhzLpP6AxDzzUh4aNIsW8Lx7+JXPn7CzWMaIxYFBjqlUruOjs7L6N+OyrS5h43TRSUqK3bnQ5LeTkGA0cbHaV10LaGJqYmJiE8uzTfxXorDZrnswz/zeQcwY05rNP/+XbrzfEtPX16ifw+lvn4fVqfPTBPyc1L5tNQVUFTzzVn3MGNA577T+P9sVqUXn5xcVIKZESJtzQmcef6lfs4/z43Yaoq4ZCwM8/bmLCxK4negom5YgK2ThACHEx8DpQA5guhFglpRxaIjMrZzRunMSwC1owY/qWoEFSFIHLaeX6E/gQbt6UGsxnzU+16k6OpOTEXE6y29WwSC0QVm369ptLWL8ur0tJ4P8F3Sx7PBo9urzHZ19eQo2aLg4dzCr6ycTA5bJwy609ALjoktZcVIQloVZtqjN/3u5CC9FiYbcruFw2Xn6taG/DQYObsmrdjfQ9cwr79x0nJ8dn5N06Lfzn0bOxWy38s+IAbdvXYOz4jlStZgpIm1RsTie7farZtPFo1Ofj4238+dd4WraqDhgdDe+67bcw59ZqVUhMtJOU5GDMuPZMuq2nvwbByoQbuvLBe/8U6AzHwm5X+fzrkZx5Vn3i4iJbSQsheODhPtx+Vy/278+kdu24qNsVBXeuFjXH36fp5J7A3E3KJxWyNauU8gfghxKaS7nn3Q8v4NWXlvDe5BVkZnk5Z0Bjnnp2ADVqFn+Zo3WbamhRbq0dTgudu9RhwbzdEcsqEOhTbeGCES1ijv3VF+uiGjaLxeiYFStvNjU1l0sv/oZHn+jL44/OIyf7xA2Mza4w9sqO3BKy/ASQk+Pl0f/8wSdT1+DO1TirTwNeenUIrVobhnziTd2Z8sHqYjusQhjRi8tHt+e667tQvUbRJb0SEuzMX3g1H32wihm/bKFmzThuvLk7Z/ZpUKw5mJhUBE43u30q6dChJvv2ZkQEGzRdp36DvBWlZ56KjMR6vTo5uT42rZwQkXb01LMDqN8gkddfXcrRo9mAwO2X4SsMq1Xl6NHsQp1Qp9NKs2Ynt+Q95LxmvPz8oohzUxWFIeeZ6UOVgZJszSqE+ALoj5Hruhd4TEr5QaztzU5XxcBqVbn3gTPZvONW9h++i8++vOSEdfQ6dKxFjx51sTvyDJMQ4HRYOHwoK6qzClC/QSIzZxeshWdRo/9ZLRaFF14eHDOyC/ibEwhef/M8GjWu4lccSMDhKDxv025XadgwkddeH8qmbbfy0qtDwnJIPR6N7p3fY/JbK8nO8qFpkvnzdjOw38cc9AtoN2mSxPc/XUazZsnYbCpWqxJVuis/DoeFz78ayf0PnVUsZzVAQoKd2+7oxa9zxvHx5xebzqqJiUmxefA/fSI6RLlcVm6e1COo2LJlSyrr10VXXdB8OsuX7mfihF/od9YU7r5jFrt2paEogpsn9WDD1lsYeWlbNE0vRsGqRHBqioQ6dq7F+Gs64XJZEcL/neaycOOkbrRsVe2UzMGk9NGELNKjMKSUY6SUdaSUVill/YKcVTAd1jLl6+9HcfU1nYmPt2G1Kgwa3JTfF1xF1arRl50dTgvffD+KNm0L1sIbf00nnK5woykENGiYyIQbuvDX4mto2apq1H1zc30cPpTFZaPb8e+GmziacR/rNt1M8xbVsFoj3y5Wq8JZfRqwaNm1rF53I/9uvIlrJnShWpSl8zGXfcee3RkRz+fk+Hj/vZXB38/s04CV/97Apm23sPvAHcz7+2r6nN0Aq1WhajUn/c9pjMNhwWIRqKrA6bRw+529aN2meoHXxcTExKQ06dylNj9OG03XbnWwWhVq147j0Sf68ujjfQHIzPQw5JxPIroWBrDZFC658Cu+/nId/6w8yJQPV3FGjw/ZuOFIcJufftxUrBUon08y5NxTF9189qWBfDvtMq65vjPXTezCjzNG89hT/U/Z8U1KlxLWYS0WZtFVGeJyWXnh5cG88PLgsOevn9iVJYv3hQlOCwH16yXQpm3hTtl113dh7uztzJ+3G5+mY7Op2Kwqn35xCUII2rarweT3hzNs6OcRotZxcVb69G0Y9pwQgl9+HcOdt/7GL9M2o+uS7j3q8tAjfejQoVbMiOaWzUeZ+tFqUlKy6dW7Hn/+vjPqdl6vzqp/DkYcs1p1Y9yOnWoxY9bYsNc3bjjCD99vROqSERe1on2HmoVeFxMTE5PSpveZ9fnzr6uivvb9txvIibF6ZrOpOF02Dh/Kqx8wOlR5eOiB3/n+J0OKqqiF+8aqmOCtyeeTnOwo1jmcDEIIzjirPmecVf+UHdPk1FIMlYASxXRYyyHDhrdgwsQuTH5rBVZ/LlNiop2vvh9VoMxIdraXdWtTqFHTxdffj2LF8gMsXrSXOnXiOf+CFjgceX/ubt3rMGhIE+bM2hF0Wl0uC9161I2oIgWjh/TUzy7C5zNa/dlsRr/qWPP57tv13HzDDL/B1fnhuw0xowpCQKfO0SQjY9O6TXUefLhPsfYxMTE5vTl8KIsP3v+HlcsP0KFTTSZc35W69WI3SilpdmxPIzuG0sqIC1vyw/cbI56XEhb9vSf4+8UjW/Pl52vxeGLHsCwWhVtu7cENN3aLKNA1MTlZKqRKgEnpIITgqWcGcNPN3VmyeB/Vqjvpc3ZD1Bi5qQDvvLmcxx+dh2oReD06XbrW5vOvLqFb9zox95n66UV88dlapn60Gk3TuWJcB8Zf3Sks7/TokWzef/cf/vprNy1aVuWmm7tz9EgO99w1m3/XHCIx0c6NN3fj/of6BHNjc3K8TLpxZljifW5ubA1Bi0VhwvVdinOJTExMTIrFli2pDDh7Km63Rm6ujz/+2Mnkt1bw29xxp2yFplPnWsTHW8nMzLeyFW/liis7MG3aZjQt0lZWqZIXIf3vMwNYsngf+/ZmRIwTwOWyctbZDU1n1aTEkZgRVpMo1KufyCWXJha63dw5O3j80Xlhy/vLl+1n3JgfmDl7bMz9VFVh3PiOjBvfEYD583Zx3dU/cyw1hxEXtuKcgY0ZOvBTMjM95OZq/L1gN59O/RcpZVBDNj3dzf9eW8rhw9m89obREnjZ0v1hTm8oiiLCVAoURfDlNyNLzbAeO5aL1aoQH39iMi0mJiaVg/vvmUNGhjtYrORxa3jcGnfe9huz/7jylMxh2PAW1H0ykZ070oK63na7SosW1RgwsAlXjO3AF5+tDSu6dbos3DwpT3s9OdnB4uXXMeu37bz5+jIW/b0nIqfV7fbRoRykSWVne/lk6hqm/bSJ6tVd3HBjN7OgtRJgRlhNTpj/vbIkIhfV69VZsfwAu3elRxXoz89rLy/m2af/Do6zbNl+7E9ayMjIDQpb+3wSny8y/yon28cnU9ewetVBsrK8dO9eN6Z0VtNmSaSm5nI8w033nnV5/sVBxU4HKAprVh/ixut/Ceoint2vIZPfu4BateNL/FgmJibln/l/7opaWb90yT40TS9wBauksFpV5vx5JU89MZ/vvt2AIgSjx7bnwYf7oCiC514YSEpKFrN/247dbsHt9jHmivZMuj1cHlBVFc47vzlt2lSjS8fwTpdCwMhRbcrc1mVneznn7Kns3JEW1Lf+deY2HnuyLzfd0qNM52Zy4kgokgJAcOMSpFI5rLouWTB/F5s3pdKmbXXO6tPgtOj5e/BgZtTndV3n+muncXa/hky4viu160Q3YKmpOTz15IKwzls52b5i6bD6fDorVxiFUzt3pEXtfe1yWXn2+UEMLeWK1SMp2Zw35DOOZ+R17Jr/5y7OHfwZK1bfEDP6a2JiUnlxOi1RuxXabGqp24T09FwWLdxLQoKN3mfU58VXhvDiK0OizNHK51+NZN/eDHbtTKdlq2oFyvRNfnslqiLQ8nkGu3amlfQpFJuPp6wOOqtg5OJmZ3t57D/zuGJch7A0B5OKhRlhPUlSU3M4f8hn7N6Vjs8nUS2CZk2TmT7rikr/wRgytBnbtqZGJOF7PDqLFu5l5YoDvPPmCmb/MS6qJNaSxfuw29WYrWKLi9ut4XCo2O0CVVXQpcTn07n+xq4MGdq0RI5REB9PXY0337Xw+SSHDmaxYP4u+vVvXOpzMDExKV+Mv7oT701eGbbcbrerjBnbvlQDG++/u5KH7v8dq01BSoiPs/LDtMtp1z72kn29+onUq194OthXX66LsPtSwpIl+8nM9JRpKtQv0zZHbWBjsyksX3aAgYOalMGsTE4eWWY5rJVGh/W+u2ezdUsqmZlecnN9ZGV62bjxKA8/8HtZT63Uue2OniQnOyO6owRwuzWOH3dz952zor6enOSI2s9aCCJE+4saicjN1TizTwM+mDqCl18bysrVN/Dfp885JRHvrVtSozZe0HXJrp3ppX58ExOT8scjj/elb/+GOJ0WEhJtOJ0Wevaux7PPDyy1Y65ccYCHH/id3FwfxzM8ZB73cPBgFhde8FXUFqbFRcbqHiDB59P47ddtPPfMX3z2yRqysjzRty0lqld3RZXg0jRJUlLlDiJVZspSh7VSOKxSSn78flOUCKPGt19vKKNZnTpq1Ixj0bLruOXWHnToWDOqkZASFv61N6qB69m7HsnJjoj9HA4LQ89tht2ukphox+my0KJl1SJ1vVJVQaPGSZx7XnPGXNGeBg0Lz6MtKXr2qocrLlonMEnHTrVO2TxMTEzKDw6HhW9/uIwFi67h7cnD+POvq5j+6xWFtiw9GT58/5+oK1c52V7+WrD7pMcfeWmbiECFogi6dK3NiPO/4uorf+SZ//7FPXfOpl3Lt9m86ehJH7OoTLyxW0RHRiGgZq04unYr+boFk1OHjizSo6SpFA4rEPNuNVouZWWkeg0XTzzVn7+XXBvTADscatQIp6IIfpoxmoaNqhAXbzWcU6eFZ58fyJffXsqqtRN5f8pw/lxwFUtWTKB586oxo7kBbDaVGyZ2LZFzKy6XjW5HtWrOsM5cDoeF3mfUp3MX01CamJzOtGxVjREXtSq0Y2BJcOxYbvQCVAEZ6e6THv8/j/WlSdOk4NJ/XJyVqlUddOxck40bjpDll73KyvJy7FgO1139c7HGTzmcxZuvL+WRh/9g9qztMYtpo3HGWQ144ql+OJ0WEhNtxMVZadIkmR+mXX5a1JZUVgJFVyXRmrW4VIocViEE5wxswu9zdoR9oFRVMPgU5EyWN8Zd1YEpH6wK0z61O1TGjO0Qc5/mzauyZv2NrFxxkIwMNz161g0awfz5VDPnjOXRh//k22/Wo+uSgQObsHNnGps3pWKxKFhtCm+9c/4p+UKIhstlZd5fV/PkE/OZ9tMm7DaV8dd04u57zyiT+ZiYmJyeXDCiJb/P2UFWvmYBXo/GmWedvLxTUpKDRcuuY+b0LaxZfYjGTZK46JLW9OjyfkRalJSwYcMRUg5nUaNmXKFj//3Xbi696Bs0TZKb6+P9ySvp3LU2P/0yutCARYCJN3VnzNgOLFu6n+RkB1261jad1UpAWeWwipg5MKVI9+7d5fLly0t0zJ070xjQ92Oys7xkZ3txxVmJj7fx54KrqN+g8OT1ykROjpcrLvuev//abRQ8eSV16sbz7Q+jCkz0P1l270rn+HE3rVpXDzYRMDGpbAghVkgpuxe+ZeWhNGz26YDHo3He4M9YtzaF7GwvQhhKAA/+pw+339mr1I7bruVb7NmTEfG8zaayYcvNhTqsmqbTsukbpBzODnve6bTw32fO4YYbu5XofE1OLSdjw6qIJrK3+liRtp2lXVOitrLSeBWNGyexet1Enn5uABNu6MKz/zeAVWsnnnbOKhgG8a3J5+OKsyF1o9jo0MEsBvb7hAXzd5XacRs2qkK79jVNZ9XExMQEw0GcOXssL702hCHnNmXU5W354efLStVZBRgztn1ErYEQ0LZd9SJFV9etTSEnO7KLVk6Oj88/XVti8zSpeMgi5q+WRg5rpUgJCJCQYOc6s8UnAE//dwEZ6e5gDq/Pp+Pz6dw8cQZr1t9oLsuYmJiYnAJsNpWx4zowdlzslKyS5q57z2DunB1s3HiEnGwfTpcFh93CB1NGFGl/VRVRmywAYbUBJqcnZZMQUMkcVpM8fpu5t4os6QAABxpJREFULWrB2cEDmRw6mBWziYCJiYmJScXG5bIyd954/vh9J/+sPEC9+olceFErXK5o6imRtG1Xg6rVnBG5ty6Xlauv7VQaUzapQOilUFBVFEyHtZISn2Dj0KGsiOelNHpTm5iYmJhUXhRFMHBQkxMS6BdC8PlXl3DBeV+g+SQer4ZFVRhyblNGX9G+FGZrUlGQENFZ7VRhei6VlBtu7MYTj/5Jdkh7VatVof+ARpW+85eJiYmJycnRqXNtNm2bxC8/b+bw4Sz6nN3QlAU0ASiV/NSiYDqslZSJN3VjzepDfPv1eux2FZ8madmyGpPfH17WUzMxMTExqQC4XFYuG92urKdhUo4wOl2ZDqtJCaIogrffHcZD/+nD6lWHaNAwkU6dzbtjExMTExMTkxOnrNoxmQ5rJadBwyqntC2qiYmJiYmJSWVFllnjANNhNTExMTExMTExKRQzJcDExMTExMTExKRcIwX4TFkrExMTExMTExOT8owZYTUxMTExMTExMSnXmDmsJiYmJiYmJiYm5RaJNCOsJiYmJiYmJiYm5RvTYTUxMTExMTExMSm3SMBXRkqspsNqYmJiYmJiYmJSJHRRNsc1HVYTExMTExMTE5NCMXVYTUxMTExMTExMyjlm0ZWJiYmJiYmJiUk5RgKa6bCamJiYmJiYmJiUZ8wIq4mJiYmJiYmJSblFIvEKrUyOrZzMzkKIF4QQG4UQa4QQPwghkkpoXiYmJiYmpYBpt01MTE6UQEpAUR6FIYQ4VwixSQixVQjxQGHbn5TDCswG2kspOwKbgQdPcjwTExMTk9LFtNsmJiYnTEk4rEIIFXgTOA9oC4wRQrQtaJ+TclillLOklD7/r4uB+icznomJiYlJ6WLabRMTkxNFApqQRXoUQk9gq5Ryu5TSA3wJXFjQDiWZw3ot8FWsF4UQNwA3+H91CyHWluCxKwLVgSNlPYlTjHnOlZ/T7XwBWpX1BEqQmHa7EtrsyvBerejnUNHnD5XjHE7Yhuly32/Hcx+sXsTNHUKI5SG/vyulfNf/cz1gT8hre4FeBQ1WqMMqhJgD1I7y0sNSyp/82zwM+IDPYo3jn+S7/u2XSym7F3bsyoR5zqcHp9s5n27nC8Y5l/UcCqMk7HZls9nmOZQ9FX3+UHnO4UT3lVKeW5JzKQ6FOqxSykEFvS6EuBq4ABgopSwbrQMTExMTkyCm3TYxMSnn7AMahPxe3/9cTE5WJeBc4D5ghJQy+2TGMjExMTEpfUy7bWJiUg5YBrQQQjQRQtiA0cDPBe1wsjmsbwB2YLYQAmCxlPLGIuz3buGbVDrMcz49ON3O+XQ7X6j453widruinzOY51AeqOjzB/McSgQppU8IMQn4DVCBD6WU6wraR5irQSYmJiYmJiYmJuWZk9VhNTExMTExMTExMSlVTIfVxMTExMTExMSkXFNmDuvp2B5QCDFKCLFOCKELISq0LEZBFLfdWmVACPGhEOJwJdCqLBJCiAZCiD+EEOv97+nby3pOpY0QwiGEWCqEWO0/5yfKek6nkspgsyuqDa7oNrUy2MeKbvMqg/0qywjr6dgecC1wCTC/rCdSWpxIu7VKwhSgzPTpygAfcLeUsi3QG7jlNPg7u4EBUspOQGfgXCFE77Kd0imlMtjsCmeDK4lNnULFt48V3eZVePtVZg7r6dgeUEq5QUq5qaznUcoUu91aZUBKOR9ILet5nCqklAeklCv9Px8HNmB0Lqm0SINM/69W/+O0qVqtDDa7gtrgCm9TK4N9rOg2rzLYr/KSw3otMLOsJ2FSIkRrt1ZhPtQmxUcI0RjoAiwp46mUOkIIVQixCjgMzJZSVvpzjoFps08dpk0tZ1RUm1fR7dfJ6rAWSEm1da1IFOWcTUwqC0KIeOA74A4pZUZZz6e0kVJqQGd//uYPQoj2UsoKm5eXn8pgs00bbFKaVGSbV9HtV6k6rKdje8DCzvk0oNjt1kwqJkIIK4bh/kxK+X1Zz+dUIqVME0L8gZGXV2EMfmFUBptdCW2waVPLCZXF5lVU+1WWKgFme8DKSbHbrZlUPITRIukDYIOU8uWyns+pQAhRI1AZL4RwAoOBjWU6qVOIabPLDNOmlgMqus2rDParLHNY3wASMNoDrhJCvFOGczklCCEuFkLsBc4ApgshfivrOZU0/qKMQLu1DcDXhbVbqwwIIb4AFgGthBB7hRDXlfWcSpmzgCuBAf7P7yohxPllPalSpg7whxBiDYYTMVtK+UsZz+lUUuFtdkW0wZXBplYS+1jRbV6Ft19ma1YTExMTExMTE5NyTXlRCTAxMTExMTExMTGJiumwmpiYmJiYmJiYlGtMh9XExMTExMTExKRcYzqsJiYmJiYmJiYm5RrTYTUxMTExMTExMSnXmA6riYmJiYmJiYlJucZ0WE1MTExMTExMTMo1/w9DlAsvBTWEWwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_influences = {\n",
    "    'train': mean_train_influences,\n",
    "    'test': mean_test_influences\n",
    "}\n",
    "plot_datasets(datasets, x_min, x_max, decision_boundary, mean_influences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8682a4c0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Flipping 5% of the training dataset and restore almost 80% of the flipped samples\n",
    "\n",
    "It is assumed that our reference test set is not flipped and was checked. Usually this is a viable solution as the test set is much smaller than the train set. First 5% of the training set get flipped at random positions. Second it is shown how to identify these examples. So a flipped dataset is created by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16c65af8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from valuation.utils import flip_dataset\n",
    "\n",
    "flipped_dataset, flipped_idx = flip_dataset(dataset, flip_percentage=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf4842d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "sampling random indices and inverting those. It is noteworthy to say that a new model has to fitted, because otherwise the old model contains information about the correct label of the specific data samples. Hence,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e89e22c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6944276094436646\n",
      "Training loss: 0.6926726698875427\n",
      "Training loss: 0.6900033950805664\n",
      "Training loss: 0.687126636505127\n",
      "Training loss: 0.6867794394493103\n",
      "Training loss: 0.6837233901023865\n",
      "Training loss: 0.6819912791252136\n",
      "Training loss: 0.6784887909889221\n",
      "Training loss: 0.6789347529411316\n",
      "Training loss: 0.6749834418296814\n",
      "Training loss: 0.6727129220962524\n",
      "Training loss: 0.670674741268158\n",
      "Training loss: 0.6618537306785583\n",
      "Training loss: 0.670244574546814\n",
      "Training loss: 0.6629287004470825\n",
      "Training loss: 0.6597762107849121\n",
      "Training loss: 0.6617328524589539\n",
      "Training loss: 0.6587206125259399\n",
      "Training loss: 0.6557075381278992\n",
      "Training loss: 0.6527829766273499\n",
      "Training loss: 0.652809202671051\n",
      "Training loss: 0.6587448716163635\n",
      "Training loss: 0.6399072408676147\n",
      "Training loss: 0.6484068036079407\n",
      "Training loss: 0.6478569507598877\n",
      "Training loss: 0.644780695438385\n",
      "Training loss: 0.6513148546218872\n",
      "Training loss: 0.6361852884292603\n",
      "Training loss: 0.6371304988861084\n",
      "Training loss: 0.6405543088912964\n",
      "Training loss: 0.63782799243927\n",
      "Training loss: 0.6286965608596802\n",
      "Training loss: 0.6393072605133057\n",
      "Training loss: 0.6230264902114868\n",
      "Training loss: 0.628242552280426\n",
      "Training loss: 0.633209228515625\n",
      "Training loss: 0.6300578117370605\n",
      "Training loss: 0.6127434968948364\n",
      "Training loss: 0.6145721077919006\n",
      "Training loss: 0.6135460138320923\n",
      "Training loss: 0.6052002906799316\n",
      "Training loss: 0.6052033305168152\n",
      "Training loss: 0.6008585095405579\n",
      "Training loss: 0.6089444756507874\n",
      "Training loss: 0.6078940629959106\n",
      "Training loss: 0.6216229200363159\n",
      "Training loss: 0.6219053268432617\n",
      "Training loss: 0.6089483499526978\n",
      "Training loss: 0.5926957726478577\n",
      "Training loss: 0.5952657461166382\n",
      "Training loss: 0.5989934802055359\n",
      "Training loss: 0.5914720296859741\n",
      "Training loss: 0.5944094061851501\n",
      "Training loss: 0.5947197675704956\n",
      "Training loss: 0.5793215036392212\n",
      "Training loss: 0.5883783102035522\n",
      "Training loss: 0.5906776189804077\n",
      "Training loss: 0.5920799970626831\n",
      "Training loss: 0.5738973617553711\n",
      "Training loss: 0.598574697971344\n",
      "Training loss: 0.581764817237854\n",
      "Training loss: 0.5828951001167297\n",
      "Training loss: 0.5784544348716736\n",
      "Training loss: 0.5857760310173035\n",
      "Training loss: 0.5766162872314453\n",
      "Training loss: 0.5784292817115784\n",
      "Training loss: 0.5737157464027405\n",
      "Training loss: 0.5430351495742798\n",
      "Training loss: 0.5899533629417419\n",
      "Training loss: 0.5647386908531189\n",
      "Training loss: 0.5565852522850037\n",
      "Training loss: 0.5771521925926208\n",
      "Training loss: 0.5652213096618652\n",
      "Training loss: 0.5596399903297424\n",
      "Training loss: 0.5595971345901489\n",
      "Training loss: 0.5638265609741211\n",
      "Training loss: 0.5803515911102295\n",
      "Training loss: 0.5430468916893005\n",
      "Training loss: 0.5635132789611816\n",
      "Training loss: 0.562609076499939\n",
      "Training loss: 0.5608446002006531\n",
      "Training loss: 0.5740470886230469\n",
      "Training loss: 0.5463473796844482\n",
      "Training loss: 0.5524121522903442\n",
      "Training loss: 0.5579995512962341\n",
      "Training loss: 0.5586101412773132\n",
      "Training loss: 0.5459944605827332\n",
      "Training loss: 0.5709015130996704\n",
      "Training loss: 0.5326967239379883\n",
      "Training loss: 0.5381847023963928\n",
      "Training loss: 0.5542557835578918\n",
      "Training loss: 0.5491428971290588\n",
      "Training loss: 0.5232641696929932\n",
      "Training loss: 0.5257490873336792\n",
      "Training loss: 0.5307413339614868\n",
      "Training loss: 0.5197174549102783\n",
      "Training loss: 0.5169692039489746\n",
      "Training loss: 0.5093677043914795\n",
      "Training loss: 0.5286984443664551\n",
      "Training loss: 0.5294865965843201\n",
      "Training loss: 0.5495971441268921\n",
      "Training loss: 0.5571648478507996\n",
      "Training loss: 0.5339666604995728\n",
      "Training loss: 0.5146080851554871\n",
      "Training loss: 0.5230088233947754\n",
      "Training loss: 0.5166429877281189\n",
      "Training loss: 0.5083685517311096\n",
      "Training loss: 0.5178295373916626\n",
      "Training loss: 0.5223883986473083\n",
      "Training loss: 0.4937087893486023\n",
      "Training loss: 0.514549195766449\n",
      "Training loss: 0.513750433921814\n",
      "Training loss: 0.5241877436637878\n",
      "Training loss: 0.5005884170532227\n",
      "Training loss: 0.5335514545440674\n",
      "Training loss: 0.5094857215881348\n",
      "Training loss: 0.5087229013442993\n",
      "Training loss: 0.5101073980331421\n",
      "Training loss: 0.5134131908416748\n",
      "Training loss: 0.5049307346343994\n",
      "Training loss: 0.5137233138084412\n",
      "Training loss: 0.5063640475273132\n",
      "Training loss: 0.46202754974365234\n",
      "Training loss: 0.534813642501831\n",
      "Training loss: 0.4987636208534241\n",
      "Training loss: 0.48437923192977905\n",
      "Training loss: 0.5209819078445435\n",
      "Training loss: 0.4976368844509125\n",
      "Training loss: 0.4892982840538025\n",
      "Training loss: 0.4958585500717163\n",
      "Training loss: 0.5004351735115051\n",
      "Training loss: 0.5236230492591858\n",
      "Training loss: 0.47961708903312683\n",
      "Training loss: 0.5019318461418152\n",
      "Training loss: 0.5018380284309387\n",
      "Training loss: 0.5021164417266846\n",
      "Training loss: 0.5156521201133728\n",
      "Training loss: 0.4836650490760803\n",
      "Training loss: 0.49258601665496826\n",
      "Training loss: 0.4977944791316986\n",
      "Training loss: 0.5037775635719299\n",
      "Training loss: 0.490084707736969\n",
      "Training loss: 0.5243640542030334\n",
      "Training loss: 0.4696597158908844\n",
      "Training loss: 0.4715435206890106\n",
      "Training loss: 0.495242178440094\n",
      "Training loss: 0.48974886536598206\n",
      "Training loss: 0.4603446125984192\n",
      "Training loss: 0.4625537097454071\n",
      "Training loss: 0.471678227186203\n",
      "Training loss: 0.4598745107650757\n",
      "Training loss: 0.45412030816078186\n",
      "Training loss: 0.44314005970954895\n",
      "Training loss: 0.47200265526771545\n",
      "Training loss: 0.4740939140319824\n",
      "Training loss: 0.4959198534488678\n",
      "Training loss: 0.5111622214317322\n",
      "Training loss: 0.47930267453193665\n",
      "Training loss: 0.4601549804210663\n",
      "Training loss: 0.47419092059135437\n",
      "Training loss: 0.4561627507209778\n",
      "Training loss: 0.44700390100479126\n",
      "Training loss: 0.4612874984741211\n",
      "Training loss: 0.47066640853881836\n",
      "Training loss: 0.43087270855903625\n",
      "Training loss: 0.46317148208618164\n",
      "Training loss: 0.45798829197883606\n",
      "Training loss: 0.47514593601226807\n",
      "Training loss: 0.4501328468322754\n",
      "Training loss: 0.48702841997146606\n",
      "Training loss: 0.45859020948410034\n",
      "Training loss: 0.45466703176498413\n",
      "Training loss: 0.4622717797756195\n",
      "Training loss: 0.46054011583328247\n",
      "Training loss: 0.45310476422309875\n",
      "Training loss: 0.46802571415901184\n",
      "Training loss: 0.4588210880756378\n",
      "Training loss: 0.40437716245651245\n",
      "Training loss: 0.49664056301116943\n",
      "Training loss: 0.45313379168510437\n",
      "Training loss: 0.43331339955329895\n",
      "Training loss: 0.48263484239578247\n",
      "Training loss: 0.44871658086776733\n",
      "Training loss: 0.4379345178604126\n",
      "Training loss: 0.4517483711242676\n",
      "Training loss: 0.4542620778083801\n",
      "Training loss: 0.48503178358078003\n",
      "Training loss: 0.435771644115448\n",
      "Training loss: 0.45616114139556885\n",
      "Training loss: 0.4591434895992279\n",
      "Training loss: 0.46078991889953613\n",
      "Training loss: 0.4729616940021515\n",
      "Training loss: 0.4392186999320984\n",
      "Training loss: 0.4496624171733856\n",
      "Training loss: 0.45453524589538574\n",
      "Training loss: 0.46627408266067505\n",
      "Training loss: 0.4513885974884033\n",
      "Training loss: 0.4923781454563141\n",
      "Training loss: 0.4255523383617401\n",
      "Training loss: 0.4232600927352905\n",
      "Training loss: 0.45182985067367554\n",
      "Training loss: 0.44693663716316223\n",
      "Training loss: 0.4154294431209564\n",
      "Training loss: 0.41750431060791016\n",
      "Training loss: 0.42864328622817993\n",
      "Training loss: 0.41645869612693787\n",
      "Training loss: 0.40856364369392395\n",
      "Training loss: 0.3945278823375702\n",
      "Training loss: 0.43175870180130005\n",
      "Training loss: 0.43455034494400024\n",
      "Training loss: 0.4564618170261383\n",
      "Training loss: 0.4786621928215027\n",
      "Training loss: 0.43922901153564453\n",
      "Training loss: 0.42111098766326904\n",
      "Training loss: 0.44034504890441895\n",
      "Training loss: 0.41195812821388245\n",
      "Training loss: 0.40130701661109924\n",
      "Training loss: 0.4189314842224121\n",
      "Training loss: 0.43305739760398865\n",
      "Training loss: 0.38404589891433716\n",
      "Training loss: 0.4272337257862091\n",
      "Training loss: 0.4176102876663208\n",
      "Training loss: 0.4391024112701416\n",
      "Training loss: 0.41464611887931824\n",
      "Training loss: 0.453761488199234\n",
      "Training loss: 0.4224761724472046\n",
      "Training loss: 0.41512662172317505\n",
      "Training loss: 0.42829689383506775\n",
      "Training loss: 0.42201948165893555\n",
      "Training loss: 0.41535910964012146\n",
      "Training loss: 0.4353058636188507\n",
      "Training loss: 0.4248315691947937\n",
      "Training loss: 0.36238980293273926\n",
      "Training loss: 0.47011393308639526\n",
      "Training loss: 0.4211874008178711\n",
      "Training loss: 0.3966967463493347\n",
      "Training loss: 0.4562135338783264\n",
      "Training loss: 0.4128444790840149\n",
      "Training loss: 0.4000401496887207\n",
      "Training loss: 0.4209129214286804\n",
      "Training loss: 0.42011886835098267\n",
      "Training loss: 0.45909756422042847\n",
      "Training loss: 0.40480756759643555\n",
      "Training loss: 0.4216344654560089\n",
      "Training loss: 0.4290388226509094\n",
      "Training loss: 0.431365042924881\n",
      "Training loss: 0.4416564404964447\n",
      "Training loss: 0.40718740224838257\n",
      "Training loss: 0.41848134994506836\n",
      "Training loss: 0.4231977164745331\n",
      "Training loss: 0.44065213203430176\n",
      "Training loss: 0.42421868443489075\n",
      "Training loss: 0.4702226519584656\n",
      "Training loss: 0.3943040668964386\n",
      "Training loss: 0.3879479765892029\n",
      "Training loss: 0.41963693499565125\n",
      "Training loss: 0.41580930352211\n",
      "Training loss: 0.38287782669067383\n",
      "Training loss: 0.384979248046875\n",
      "Training loss: 0.396705687046051\n",
      "Training loss: 0.384315550327301\n",
      "Training loss: 0.3748970627784729\n",
      "Training loss: 0.3581300377845764\n",
      "Training loss: 0.40280166268348694\n",
      "Training loss: 0.40595874190330505\n",
      "Training loss: 0.427193284034729\n",
      "Training loss: 0.4554879069328308\n",
      "Training loss: 0.4094166159629822\n",
      "Training loss: 0.39255645871162415\n",
      "Training loss: 0.4164922833442688\n",
      "Training loss: 0.37924280762672424\n",
      "Training loss: 0.36672359704971313\n",
      "Training loss: 0.3866751194000244\n",
      "Training loss: 0.40521690249443054\n",
      "Training loss: 0.3484683334827423\n",
      "Training loss: 0.40180784463882446\n",
      "Training loss: 0.387984037399292\n",
      "Training loss: 0.41215378046035767\n",
      "Training loss: 0.38923054933547974\n",
      "Training loss: 0.42970916628837585\n",
      "Training loss: 0.39645642042160034\n",
      "Training loss: 0.38579168915748596\n",
      "Training loss: 0.4037386476993561\n",
      "Training loss: 0.3936150074005127\n",
      "Training loss: 0.3874565362930298\n",
      "Training loss: 0.4115025997161865\n",
      "Training loss: 0.40014156699180603\n",
      "Training loss: 0.3311001658439636\n",
      "Training loss: 0.4515498876571655\n",
      "Training loss: 0.39852532744407654\n",
      "Training loss: 0.3699690103530884\n",
      "Training loss: 0.43783649802207947\n",
      "Training loss: 0.38603949546813965\n",
      "Training loss: 0.37163180112838745\n",
      "Training loss: 0.3990734815597534\n",
      "Training loss: 0.39443546533584595\n",
      "Training loss: 0.4417206048965454\n",
      "Training loss: 0.3825342655181885\n",
      "Training loss: 0.3951655328273773\n",
      "Training loss: 0.4076104760169983\n",
      "Training loss: 0.4100942611694336\n",
      "Training loss: 0.41842448711395264\n",
      "Training loss: 0.383670836687088\n",
      "Training loss: 0.3954992890357971\n",
      "Training loss: 0.40016165375709534\n",
      "Training loss: 0.4231250584125519\n",
      "Training loss: 0.4048597812652588\n",
      "Training loss: 0.4547474682331085\n",
      "Training loss: 0.3718084692955017\n",
      "Training loss: 0.36169207096099854\n",
      "Training loss: 0.395416259765625\n",
      "Training loss: 0.39283841848373413\n",
      "Training loss: 0.3588825464248657\n",
      "Training loss: 0.3611074984073639\n",
      "Training loss: 0.37255287170410156\n",
      "Training loss: 0.36006593704223633\n",
      "Training loss: 0.34950488805770874\n",
      "Training loss: 0.3302794098854065\n",
      "Training loss: 0.3816254138946533\n",
      "Training loss: 0.3849918842315674\n",
      "Training loss: 0.4051661193370819\n",
      "Training loss: 0.43876588344573975\n",
      "Training loss: 0.3868500292301178\n",
      "Training loss: 0.3712743818759918\n",
      "Training loss: 0.39943185448646545\n",
      "Training loss: 0.35462185740470886\n",
      "Training loss: 0.34006744623184204\n",
      "Training loss: 0.36167752742767334\n",
      "Training loss: 0.38423410058021545\n",
      "Training loss: 0.32090112566947937\n",
      "Training loss: 0.3835838735103607\n",
      "Training loss: 0.3658829927444458\n",
      "Training loss: 0.3916506767272949\n",
      "Training loss: 0.3707135319709778\n",
      "Training loss: 0.41208523511886597\n",
      "Training loss: 0.3773977756500244\n",
      "Training loss: 0.36365851759910583\n",
      "Training loss: 0.3856673538684845\n",
      "Training loss: 0.372341513633728\n",
      "Training loss: 0.36648422479629517\n",
      "Training loss: 0.3939021825790405\n",
      "Training loss: 0.38191285729408264\n",
      "Training loss: 0.30726146697998047\n",
      "Training loss: 0.4384724199771881\n",
      "Training loss: 0.38224291801452637\n",
      "Training loss: 0.3500998318195343\n",
      "Training loss: 0.42495113611221313\n",
      "Training loss: 0.3656076192855835\n",
      "Training loss: 0.3499649167060852\n",
      "Training loss: 0.3834041655063629\n",
      "Training loss: 0.37478139996528625\n",
      "Training loss: 0.4301324784755707\n",
      "Training loss: 0.3662452697753906\n",
      "Training loss: 0.37454351782798767\n",
      "Training loss: 0.3922047019004822\n",
      "Training loss: 0.3944869637489319\n",
      "Training loss: 0.4009435772895813\n",
      "Training loss: 0.36608946323394775\n",
      "Training loss: 0.37831252813339233\n",
      "Training loss: 0.38295865058898926\n",
      "Training loss: 0.41115257143974304\n",
      "Training loss: 0.39087989926338196\n",
      "Training loss: 0.4438716769218445\n",
      "Training loss: 0.35535603761672974\n",
      "Training loss: 0.3418234586715698\n",
      "Training loss: 0.3769051432609558\n",
      "Training loss: 0.37562093138694763\n",
      "Training loss: 0.3408983051776886\n",
      "Training loss: 0.34329575300216675\n",
      "Training loss: 0.3539595603942871\n",
      "Training loss: 0.34144964814186096\n",
      "Training loss: 0.32998281717300415\n",
      "Training loss: 0.3085205852985382\n",
      "Training loss: 0.3658950626850128\n",
      "Training loss: 0.3694100081920624\n",
      "Training loss: 0.38833409547805786\n",
      "Training loss: 0.4265698790550232\n",
      "Training loss: 0.36947497725486755\n",
      "Training loss: 0.35513320565223694\n",
      "Training loss: 0.38707640767097473\n",
      "Training loss: 0.3357747793197632\n",
      "Training loss: 0.31915172934532166\n",
      "Training loss: 0.34197843074798584\n",
      "Training loss: 0.3681567311286926\n",
      "Training loss: 0.29914551973342896\n",
      "Training loss: 0.3703663647174835\n",
      "Training loss: 0.34912148118019104\n",
      "Training loss: 0.3757948577404022\n",
      "Training loss: 0.35701605677604675\n",
      "Training loss: 0.39900726079940796\n",
      "Training loss: 0.3632233142852783\n",
      "Training loss: 0.34667959809303284\n",
      "Training loss: 0.37215009331703186\n",
      "Training loss: 0.3561590313911438\n",
      "Training loss: 0.35046622157096863\n",
      "Training loss: 0.3806906044483185\n",
      "Training loss: 0.36825448274612427\n",
      "Training loss: 0.28873127698898315\n",
      "Training loss: 0.42922553420066833\n",
      "Training loss: 0.3704178035259247\n",
      "Training loss: 0.3350774645805359\n",
      "Training loss: 0.41587358713150024\n",
      "Training loss: 0.34973832964897156\n",
      "Training loss: 0.33316609263420105\n",
      "Training loss: 0.37203842401504517\n",
      "Training loss: 0.3594999313354492\n",
      "Training loss: 0.42249855399131775\n",
      "Training loss: 0.35416367650032043\n",
      "Training loss: 0.3582335412502289\n",
      "Training loss: 0.38103675842285156\n",
      "Training loss: 0.382884681224823\n",
      "Training loss: 0.38761335611343384\n",
      "Training loss: 0.3527281880378723\n",
      "Training loss: 0.36528804898262024\n",
      "Training loss: 0.3699186444282532\n",
      "Training loss: 0.40303680300712585\n",
      "Training loss: 0.3806718587875366\n",
      "Training loss: 0.4362129867076874\n",
      "Training loss: 0.34315377473831177\n",
      "Training loss: 0.3265375792980194\n",
      "Training loss: 0.36254385113716125\n",
      "Training loss: 0.36252978444099426\n",
      "Training loss: 0.3272142708301544\n",
      "Training loss: 0.3298051357269287\n",
      "Training loss: 0.3394162058830261\n",
      "Training loss: 0.32693299651145935\n",
      "Training loss: 0.31471601128578186\n",
      "Training loss: 0.2911979854106903\n",
      "Training loss: 0.35404738783836365\n",
      "Training loss: 0.3576958477497101\n",
      "Training loss: 0.375286728143692\n",
      "Training loss: 0.41760215163230896\n",
      "Training loss: 0.35588768124580383\n",
      "Training loss: 0.3427000343799591\n",
      "Training loss: 0.37804222106933594\n",
      "Training loss: 0.3211181163787842\n",
      "Training loss: 0.30247047543525696\n",
      "Training loss: 0.32621657848358154\n",
      "Training loss: 0.35565805435180664\n",
      "Training loss: 0.2816920280456543\n",
      "Training loss: 0.36068862676620483\n",
      "Training loss: 0.3362155556678772\n",
      "Training loss: 0.3633529543876648\n",
      "Training loss: 0.3467530608177185\n",
      "Training loss: 0.3891971707344055\n",
      "Training loss: 0.35254165530204773\n",
      "Training loss: 0.33345508575439453\n",
      "Training loss: 0.36189380288124084\n",
      "Training loss: 0.34367215633392334\n",
      "Training loss: 0.3380534052848816\n",
      "Training loss: 0.3706410825252533\n",
      "Training loss: 0.3578910827636719\n",
      "Training loss: 0.2740705907344818\n",
      "Training loss: 0.42269188165664673\n",
      "Training loss: 0.3617611527442932\n",
      "Training loss: 0.3235490918159485\n",
      "Training loss: 0.4094783067703247\n",
      "Training loss: 0.3372022807598114\n",
      "Training loss: 0.3199462890625\n",
      "Training loss: 0.3637272119522095\n",
      "Training loss: 0.3474481701850891\n",
      "Training loss: 0.4175972640514374\n",
      "Training loss: 0.34509921073913574\n",
      "Training loss: 0.3451582193374634\n",
      "Training loss: 0.37289562821388245\n",
      "Training loss: 0.37416645884513855\n",
      "Training loss: 0.37732571363449097\n",
      "Training loss: 0.34242793917655945\n",
      "Training loss: 0.35530292987823486\n",
      "Training loss: 0.3599013090133667\n",
      "Training loss: 0.39763104915618896\n",
      "Training loss: 0.3731556534767151\n",
      "Training loss: 0.43083882331848145\n",
      "Training loss: 0.33399710059165955\n",
      "Training loss: 0.3146017789840698\n",
      "Training loss: 0.35124900937080383\n",
      "Training loss: 0.3524508774280548\n",
      "Training loss: 0.31666260957717896\n",
      "Training loss: 0.3194529712200165\n",
      "Training loss: 0.32788002490997314\n",
      "Training loss: 0.3154558837413788\n",
      "Training loss: 0.3025985658168793\n",
      "Training loss: 0.27717599272727966\n",
      "Training loss: 0.34501880407333374\n",
      "Training loss: 0.34880512952804565\n",
      "Training loss: 0.36504220962524414\n",
      "Training loss: 0.4109742045402527\n",
      "Training loss: 0.3451150059700012\n",
      "Training loss: 0.3329923152923584\n",
      "Training loss: 0.37139391899108887\n",
      "Training loss: 0.3095579147338867\n",
      "Training loss: 0.28897133469581604\n",
      "Training loss: 0.31343239545822144\n",
      "Training loss: 0.34581896662712097\n",
      "Training loss: 0.2674853503704071\n",
      "Training loss: 0.35355499386787415\n",
      "Training loss: 0.32614389061927795\n",
      "Training loss: 0.35346442461013794\n",
      "Training loss: 0.33898264169692993\n",
      "Training loss: 0.3817744255065918\n",
      "Training loss: 0.344403475522995\n",
      "Training loss: 0.32301440834999084\n",
      "Training loss: 0.3540167212486267\n",
      "Training loss: 0.33391404151916504\n",
      "Training loss: 0.3283105194568634\n",
      "Training loss: 0.36290934681892395\n",
      "Training loss: 0.34994617104530334\n",
      "Training loss: 0.2622913718223572\n",
      "Training loss: 0.4181060194969177\n",
      "Training loss: 0.3553933799266815\n",
      "Training loss: 0.3145875632762909\n",
      "Training loss: 0.40499961376190186\n",
      "Training loss: 0.32715004682540894\n",
      "Training loss: 0.3094049096107483\n",
      "Training loss: 0.3576198220252991\n",
      "Training loss: 0.33782416582107544\n",
      "Training loss: 0.4146038889884949\n",
      "Training loss: 0.33823758363723755\n",
      "Training loss: 0.3345501124858856\n",
      "Training loss: 0.3669477105140686\n",
      "Training loss: 0.3675609230995178\n",
      "Training loss: 0.3693031072616577\n",
      "Training loss: 0.33438998460769653\n",
      "Training loss: 0.3475725054740906\n",
      "Training loss: 0.3521154820919037\n",
      "Training loss: 0.39414945244789124\n",
      "Training loss: 0.3675917088985443\n",
      "Training loss: 0.4271084666252136\n",
      "Training loss: 0.3270612955093384\n",
      "Training loss: 0.3051592707633972\n",
      "Training loss: 0.3422575891017914\n",
      "Training loss: 0.3446083068847656\n",
      "Training loss: 0.30843213200569153\n",
      "Training loss: 0.3114204704761505\n",
      "Training loss: 0.31861719489097595\n",
      "Training loss: 0.306272029876709\n",
      "Training loss: 0.29285743832588196\n",
      "Training loss: 0.2656601071357727\n",
      "Training loss: 0.3380719721317291\n",
      "Training loss: 0.3420068621635437\n",
      "Training loss: 0.356906533241272\n",
      "Training loss: 0.40606632828712463\n",
      "Training loss: 0.33647027611732483\n",
      "Training loss: 0.325323224067688\n",
      "Training loss: 0.36648598313331604\n",
      "Training loss: 0.30032578110694885\n",
      "Training loss: 0.2779064476490021\n",
      "Training loss: 0.30293846130371094\n",
      "Training loss: 0.3379897475242615\n",
      "Training loss: 0.2557738423347473\n",
      "Training loss: 0.3482772409915924\n",
      "Training loss: 0.3181910812854767\n",
      "Training loss: 0.3455178737640381\n",
      "Training loss: 0.33305102586746216\n",
      "Training loss: 0.37612152099609375\n",
      "Training loss: 0.33814799785614014\n",
      "Training loss: 0.31467318534851074\n",
      "Training loss: 0.3479050397872925\n",
      "Training loss: 0.3262033462524414\n",
      "Training loss: 0.32057756185531616\n",
      "Training loss: 0.3569033741950989\n",
      "Training loss: 0.34380534291267395\n",
      "Training loss: 0.2527000308036804\n",
      "Training loss: 0.4149341881275177\n",
      "Training loss: 0.350702702999115\n",
      "Training loss: 0.30754444003105164\n",
      "Training loss: 0.401906818151474\n",
      "Training loss: 0.3189832270145416\n",
      "Training loss: 0.30090150237083435\n",
      "Training loss: 0.35312536358833313\n",
      "Training loss: 0.3300545811653137\n",
      "Training loss: 0.41295284032821655\n",
      "Training loss: 0.3330095708370209\n",
      "Training loss: 0.32585301995277405\n",
      "Training loss: 0.3626088798046112\n",
      "Training loss: 0.3625261187553406\n",
      "Training loss: 0.36299097537994385\n",
      "Training loss: 0.3280521333217621\n",
      "Training loss: 0.3415386974811554\n",
      "Training loss: 0.34600239992141724\n",
      "Training loss: 0.3920442759990692\n",
      "Training loss: 0.36346331238746643\n",
      "Training loss: 0.42457371950149536\n",
      "Training loss: 0.32177025079727173\n",
      "Training loss: 0.297603577375412\n",
      "Training loss: 0.33502286672592163\n",
      "Training loss: 0.3384513556957245\n",
      "Training loss: 0.3019484877586365\n",
      "Training loss: 0.30512961745262146\n",
      "Training loss: 0.31110072135925293\n",
      "Training loss: 0.2988455593585968\n",
      "Training loss: 0.2849404215812683\n",
      "Training loss: 0.256082683801651\n",
      "Training loss: 0.3326853811740875\n",
      "Training loss: 0.33677980303764343\n",
      "Training loss: 0.3503810465335846\n",
      "Training loss: 0.40243831276893616\n",
      "Training loss: 0.32945987582206726\n",
      "Training loss: 0.31920281052589417\n",
      "Training loss: 0.3628641963005066\n",
      "Training loss: 0.2928721010684967\n",
      "Training loss: 0.26873451471328735\n",
      "Training loss: 0.2942333221435547\n",
      "Training loss: 0.33170202374458313\n",
      "Training loss: 0.246011883020401\n",
      "Training loss: 0.34437045454978943\n",
      "Training loss: 0.31184667348861694\n",
      "Training loss: 0.3390704393386841\n",
      "Training loss: 0.32849523425102234\n",
      "Training loss: 0.37179747223854065\n",
      "Training loss: 0.33330631256103516\n",
      "Training loss: 0.3079398274421692\n",
      "Training loss: 0.34312283992767334\n",
      "Training loss: 0.3200516104698181\n",
      "Training loss: 0.3143799901008606\n",
      "Training loss: 0.35220032930374146\n",
      "Training loss: 0.33902955055236816\n",
      "Training loss: 0.2447991669178009\n",
      "Training loss: 0.4127982258796692\n",
      "Training loss: 0.34725552797317505\n",
      "Training loss: 0.30195724964141846\n",
      "Training loss: 0.39982545375823975\n",
      "Training loss: 0.31227174401283264\n",
      "Training loss: 0.2939724326133728\n",
      "Training loss: 0.3498256802558899\n",
      "Training loss: 0.3237224221229553\n",
      "Training loss: 0.4122495651245117\n",
      "Training loss: 0.32900938391685486\n",
      "Training loss: 0.3186568021774292\n",
      "Training loss: 0.35946333408355713\n",
      "Training loss: 0.35867437720298767\n",
      "Training loss: 0.35798734426498413\n",
      "Training loss: 0.3230111598968506\n",
      "Training loss: 0.3367978036403656\n",
      "Training loss: 0.3411606550216675\n",
      "Training loss: 0.39092782139778137\n",
      "Training loss: 0.3604029715061188\n",
      "Training loss: 0.422914981842041\n",
      "Training loss: 0.31771373748779297\n",
      "Training loss: 0.29149705171585083\n",
      "Training loss: 0.3291466236114502\n",
      "Training loss: 0.3335822820663452\n",
      "Training loss: 0.29679760336875916\n",
      "Training loss: 0.30016469955444336\n",
      "Training loss: 0.3049452006816864\n",
      "Training loss: 0.292784720659256\n",
      "Training loss: 0.2784450948238373\n",
      "Training loss: 0.24803003668785095\n",
      "Training loss: 0.328483521938324\n",
      "Training loss: 0.33274608850479126\n",
      "Training loss: 0.34510117769241333\n",
      "Training loss: 0.39977172017097473\n",
      "Training loss: 0.3237224817276001\n",
      "Training loss: 0.31427544355392456\n",
      "Training loss: 0.36020326614379883\n",
      "Training loss: 0.2867966294288635\n",
      "Training loss: 0.26105666160583496\n",
      "Training loss: 0.28694504499435425\n",
      "Training loss: 0.32661235332489014\n",
      "Training loss: 0.23779591917991638\n",
      "Training loss: 0.34148725867271423\n",
      "Training loss: 0.3067399859428406\n",
      "Training loss: 0.33379605412483215\n",
      "Training loss: 0.32498157024383545\n",
      "Training loss: 0.36848241090774536\n",
      "Training loss: 0.3295394778251648\n",
      "Training loss: 0.3024551272392273\n",
      "Training loss: 0.3393546938896179\n",
      "Training loss: 0.31510236859321594\n",
      "Training loss: 0.30937108397483826\n",
      "Training loss: 0.3484930098056793\n",
      "Training loss: 0.335299015045166\n",
      "Training loss: 0.23822498321533203\n",
      "Training loss: 0.4114255905151367\n",
      "Training loss: 0.34473949670791626\n",
      "Training loss: 0.2974897623062134\n",
      "Training loss: 0.3984869122505188\n",
      "Training loss: 0.3067007064819336\n",
      "Training loss: 0.2882762551307678\n",
      "Training loss: 0.3474196791648865\n",
      "Training loss: 0.3185187578201294\n",
      "Training loss: 0.4122147262096405\n",
      "Training loss: 0.3259425461292267\n",
      "Training loss: 0.3126547038555145\n",
      "Training loss: 0.35720986127853394\n",
      "Training loss: 0.3557233214378357\n",
      "Training loss: 0.35399606823921204\n",
      "Training loss: 0.31897228956222534\n",
      "Training loss: 0.3330531418323517\n",
      "Training loss: 0.3372967541217804\n",
      "Training loss: 0.3905218839645386\n",
      "Training loss: 0.35814470052719116\n",
      "Training loss: 0.4219013452529907\n",
      "Training loss: 0.31459447741508484\n",
      "Training loss: 0.28651854395866394\n",
      "Training loss: 0.3243340253829956\n",
      "Training loss: 0.3297085762023926\n",
      "Training loss: 0.29267585277557373\n",
      "Training loss: 0.296221524477005\n",
      "Training loss: 0.29986387491226196\n",
      "Training loss: 0.2877987027168274\n",
      "Training loss: 0.27307283878326416\n",
      "Training loss: 0.24119503796100616\n",
      "Training loss: 0.3251909613609314\n",
      "Training loss: 0.3296273648738861\n",
      "Training loss: 0.3407963216304779\n",
      "Training loss: 0.3978327214717865\n",
      "Training loss: 0.318989098072052\n",
      "Training loss: 0.310278058052063\n",
      "Training loss: 0.3582659363746643\n",
      "Training loss: 0.28180307149887085\n",
      "Training loss: 0.2545740604400635\n",
      "Training loss: 0.28079330921173096\n",
      "Training loss: 0.3224641978740692\n",
      "Training loss: 0.2308226227760315\n",
      "Training loss: 0.33937448263168335\n",
      "Training loss: 0.3025973439216614\n",
      "Training loss: 0.3294502794742584\n",
      "Training loss: 0.32226523756980896\n",
      "Training loss: 0.3659400939941406\n",
      "Training loss: 0.3265983760356903\n",
      "Training loss: 0.29795220494270325\n",
      "Training loss: 0.33636870980262756\n",
      "Training loss: 0.31109151244163513\n",
      "Training loss: 0.3052929639816284\n",
      "Training loss: 0.3455544412136078\n",
      "Training loss: 0.3323773741722107\n",
      "Training loss: 0.2327064424753189\n",
      "Training loss: 0.41061732172966003\n",
      "Training loss: 0.3429257273674011\n",
      "Training loss: 0.2938936948776245\n",
      "Training loss: 0.3976956009864807\n",
      "Training loss: 0.30203545093536377\n",
      "Training loss: 0.28355732560157776\n",
      "Training loss: 0.34568724036216736\n",
      "Training loss: 0.31421148777008057\n",
      "Training loss: 0.4126482307910919\n",
      "Training loss: 0.3235916495323181\n",
      "Training loss: 0.3076132535934448\n",
      "Training loss: 0.3556276559829712\n",
      "Training loss: 0.3534640073776245\n",
      "Training loss: 0.3507956564426422\n",
      "Training loss: 0.3157166540622711\n",
      "Training loss: 0.3300832509994507\n",
      "Training loss: 0.33419302105903625\n",
      "Training loss: 0.390624076128006\n",
      "Training loss: 0.3564932346343994\n",
      "Training loss: 0.42136311531066895\n",
      "Training loss: 0.3121936321258545\n",
      "Training loss: 0.2824282646179199\n",
      "Training loss: 0.32036352157592773\n",
      "Training loss: 0.3266121745109558\n",
      "Training loss: 0.2893572747707367\n",
      "Training loss: 0.29307395219802856\n",
      "Training loss: 0.2956400513648987\n",
      "Training loss: 0.2836678624153137\n",
      "Training loss: 0.268598347902298\n",
      "Training loss: 0.235345721244812\n",
      "Training loss: 0.32260286808013916\n",
      "Training loss: 0.32721588015556335\n",
      "Training loss: 0.3372623920440674\n",
      "Training loss: 0.396447092294693\n",
      "Training loss: 0.3150564730167389\n",
      "Training loss: 0.307013601064682\n",
      "Training loss: 0.3568771481513977\n",
      "Training loss: 0.27766868472099304\n",
      "Training loss: 0.24905923008918762\n",
      "Training loss: 0.27556365728378296\n",
      "Training loss: 0.3190634846687317\n",
      "Training loss: 0.2248602956533432\n",
      "Training loss: 0.33784520626068115\n",
      "Training loss: 0.29921361804008484\n",
      "Training loss: 0.3258477449417114\n",
      "Training loss: 0.32016420364379883\n",
      "Training loss: 0.36399415135383606\n",
      "Training loss: 0.324297159910202\n",
      "Training loss: 0.29422980546951294\n",
      "Training loss: 0.33399155735969543\n",
      "Training loss: 0.3078201711177826\n",
      "Training loss: 0.30195119976997375\n",
      "Training loss: 0.34321480989456177\n",
      "Training loss: 0.3300867974758148\n",
      "Training loss: 0.22803828120231628\n",
      "Training loss: 0.4102262258529663\n",
      "Training loss: 0.3416445851325989\n",
      "Training loss: 0.2909826934337616\n",
      "Training loss: 0.39730745553970337\n",
      "Training loss: 0.29809844493865967\n",
      "Training loss: 0.27962130308151245\n",
      "Training loss: 0.34446507692337036\n",
      "Training loss: 0.31062331795692444\n",
      "Training loss: 0.4134044945240021\n",
      "Training loss: 0.32179415225982666\n",
      "Training loss: 0.30335259437561035\n",
      "Training loss: 0.3545522391796112\n",
      "Training loss: 0.35173988342285156\n",
      "Training loss: 0.34821850061416626\n",
      "Training loss: 0.31307917833328247\n",
      "Training loss: 0.32772088050842285\n",
      "Training loss: 0.33168545365333557\n",
      "Training loss: 0.391085684299469\n",
      "Training loss: 0.35530319809913635\n",
      "Training loss: 0.42117446660995483\n",
      "Training loss: 0.31034761667251587\n",
      "Training loss: 0.2790448069572449\n",
      "Training loss: 0.31706634163856506\n",
      "Training loss: 0.32412782311439514\n",
      "Training loss: 0.2866712212562561\n",
      "Training loss: 0.2905515730381012\n",
      "Training loss: 0.2921077013015747\n",
      "Training loss: 0.2802242040634155\n",
      "Training loss: 0.2648489475250244\n",
      "Training loss: 0.2303040474653244\n",
      "Training loss: 0.32056477665901184\n",
      "Training loss: 0.32535436749458313\n",
      "Training loss: 0.334343820810318\n",
      "Training loss: 0.39548376202583313\n",
      "Training loss: 0.31176871061325073\n",
      "Training loss: 0.30433201789855957\n",
      "Training loss: 0.355905681848526\n",
      "Training loss: 0.2742234468460083\n",
      "Training loss: 0.24433667957782745\n",
      "Training loss: 0.2710898816585541\n",
      "Training loss: 0.3162612318992615\n",
      "Training loss: 0.21972942352294922\n",
      "Training loss: 0.33675968647003174\n",
      "Training loss: 0.29643285274505615\n",
      "Training loss: 0.3228451609611511\n",
      "Training loss: 0.3185412585735321\n",
      "Training loss: 0.36251091957092285\n",
      "Training loss: 0.32249534130096436\n",
      "Training loss: 0.2911340296268463\n",
      "Training loss: 0.33209213614463806\n",
      "Training loss: 0.3051370680332184\n",
      "Training loss: 0.29919731616973877\n",
      "Training loss: 0.34134507179260254\n",
      "Training loss: 0.32829219102859497\n",
      "Training loss: 0.22406280040740967\n",
      "Training loss: 0.410142183303833\n",
      "Training loss: 0.3407685160636902\n",
      "Training loss: 0.288614958524704\n",
      "Training loss: 0.3972150385379791\n",
      "Training loss: 0.29475319385528564\n",
      "Training loss: 0.2763189375400543\n",
      "Training loss: 0.34363067150115967\n",
      "Training loss: 0.3076173961162567\n",
      "Training loss: 0.4143771827220917\n",
      "Training loss: 0.3204267919063568\n",
      "Training loss: 0.2997322678565979\n",
      "Training loss: 0.35386013984680176\n",
      "Training loss: 0.35043203830718994\n",
      "Training loss: 0.3461359441280365\n",
      "Training loss: 0.3109336197376251\n",
      "Training loss: 0.32583796977996826\n",
      "Training loss: 0.3296494483947754\n",
      "Training loss: 0.39179670810699463\n",
      "Training loss: 0.3544650971889496\n",
      "Training loss: 0.4212408661842346\n",
      "Training loss: 0.3089328110218048\n",
      "Training loss: 0.27622899413108826\n",
      "Training loss: 0.31431227922439575\n",
      "Training loss: 0.3221290707588196\n",
      "Training loss: 0.2844872772693634\n",
      "Training loss: 0.2885241210460663\n",
      "Training loss: 0.28913775086402893\n",
      "Training loss: 0.27733755111694336\n",
      "Training loss: 0.261690616607666\n",
      "Training loss: 0.2259315699338913\n",
      "Training loss: 0.31895875930786133\n",
      "Training loss: 0.32392266392707825\n",
      "Training loss: 0.331920325756073\n",
      "Training loss: 0.3948427736759186\n",
      "Training loss: 0.3090048134326935\n",
      "Training loss: 0.3021177351474762\n",
      "Training loss: 0.35525232553482056\n",
      "Training loss: 0.271336168050766\n",
      "Training loss: 0.24026909470558167\n",
      "Training loss: 0.26724132895469666\n",
      "Training loss: 0.31394168734550476\n",
      "Training loss: 0.21528907120227814\n",
      "Training loss: 0.33601218461990356\n",
      "Training loss: 0.2941354215145111\n",
      "Training loss: 0.32033100724220276\n",
      "Training loss: 0.31729158759117126\n",
      "Training loss: 0.36138808727264404\n",
      "Training loss: 0.321085661649704\n",
      "Training loss: 0.28854551911354065\n",
      "Training loss: 0.3305701017379761\n",
      "Training loss: 0.3029254972934723\n",
      "Training loss: 0.2969163954257965\n",
      "Training loss: 0.33984655141830444\n",
      "Training loss: 0.3268892765045166\n",
      "Training loss: 0.22065700590610504\n",
      "Training loss: 0.41028231382369995\n",
      "Training loss: 0.3402007520198822\n",
      "Training loss: 0.28668129444122314\n",
      "Training loss: 0.3973376154899597\n",
      "Training loss: 0.2918936610221863\n",
      "Training loss: 0.2735334634780884\n",
      "Training loss: 0.3430909216403961\n",
      "Training loss: 0.30508652329444885\n",
      "Training loss: 0.41548827290534973\n",
      "Training loss: 0.31939494609832764\n",
      "Training loss: 0.2966412901878357\n",
      "Training loss: 0.3534574508666992\n",
      "Training loss: 0.3494490087032318\n",
      "Training loss: 0.3444483280181885\n",
      "Training loss: 0.30918246507644653\n",
      "Training loss: 0.3243355453014374\n",
      "Training loss: 0.32798880338668823\n",
      "Training loss: 0.39267489314079285\n",
      "Training loss: 0.3538956344127655\n",
      "Training loss: 0.42149120569229126\n",
      "Training loss: 0.307854562997818\n",
      "Training loss: 0.2738726735115051\n",
      "Training loss: 0.3119995892047882\n",
      "Training loss: 0.320517897605896\n",
      "Training loss: 0.2827046513557434\n",
      "Training loss: 0.28689107298851013\n",
      "Training loss: 0.28662893176078796\n",
      "Training loss: 0.2749059796333313\n",
      "Training loss: 0.2590179145336151\n",
      "Training loss: 0.22211883962154388\n",
      "Training loss: 0.3176937997341156\n",
      "Training loss: 0.322828084230423\n",
      "Training loss: 0.3298979103565216\n",
      "Training loss: 0.39444732666015625\n",
      "Training loss: 0.30666983127593994\n",
      "Training loss: 0.3002808094024658\n",
      "Training loss: 0.3548412621021271\n",
      "Training loss: 0.2689041495323181\n",
      "Training loss: 0.23674771189689636\n",
      "Training loss: 0.26391446590423584\n",
      "Training loss: 0.31201428174972534\n",
      "Training loss: 0.21142739057540894\n",
      "Training loss: 0.3355221152305603\n",
      "Training loss: 0.2922278344631195\n",
      "Training loss: 0.31821727752685547\n",
      "Training loss: 0.31633469462394714\n",
      "Training loss: 0.36054688692092896\n",
      "Training loss: 0.3199850916862488\n",
      "Training loss: 0.28637105226516724\n",
      "Training loss: 0.3293475806713104\n",
      "Training loss: 0.30109432339668274\n",
      "Training loss: 0.29501867294311523\n",
      "Training loss: 0.3386427164077759\n",
      "Training loss: 0.3257971704006195\n",
      "Training loss: 0.2177240401506424\n",
      "Training loss: 0.4105835258960724\n",
      "Training loss: 0.3398672342300415\n",
      "Training loss: 0.2850968837738037\n",
      "Training loss: 0.39761409163475037\n",
      "Training loss: 0.28943607211112976\n",
      "Training loss: 0.27117305994033813\n",
      "Training loss: 0.3427748680114746\n",
      "Training loss: 0.30294618010520935\n",
      "Training loss: 0.41667938232421875\n",
      "Training loss: 0.31862568855285645\n",
      "Training loss: 0.2939912676811218\n",
      "Training loss: 0.35327252745628357\n",
      "Training loss: 0.34872013330459595\n",
      "Training loss: 0.3430776298046112\n",
      "Training loss: 0.3077494502067566\n",
      "Training loss: 0.32313650846481323\n",
      "Training loss: 0.3266286253929138\n",
      "Training loss: 0.3936590254306793\n",
      "Training loss: 0.35353103280067444\n",
      "Training loss: 0.42187121510505676\n",
      "Training loss: 0.3070398271083832\n",
      "Training loss: 0.27189111709594727\n",
      "Training loss: 0.3100484013557434\n",
      "Training loss: 0.3192174434661865\n",
      "Training loss: 0.2812446355819702\n",
      "Training loss: 0.28557395935058594\n",
      "Training loss: 0.28450095653533936\n",
      "Training loss: 0.27284887433052063\n",
      "Training loss: 0.2567472457885742\n",
      "Training loss: 0.21877869963645935\n",
      "Training loss: 0.3166988790035248\n",
      "Training loss: 0.3219985365867615\n",
      "Training loss: 0.3282025456428528\n",
      "Training loss: 0.39423808455467224\n",
      "Training loss: 0.3046884536743164\n",
      "Training loss: 0.29875075817108154\n",
      "Training loss: 0.3546140491962433\n",
      "Training loss: 0.2668464183807373\n",
      "Training loss: 0.2336856871843338\n",
      "Training loss: 0.2610260844230652\n",
      "Training loss: 0.31040701270103455\n",
      "Training loss: 0.20805445313453674\n",
      "Training loss: 0.3352275490760803\n",
      "Training loss: 0.29063698649406433\n",
      "Training loss: 0.31643402576446533\n",
      "Training loss: 0.31560781598091125\n",
      "Training loss: 0.35992562770843506\n",
      "Training loss: 0.3191290497779846\n",
      "Training loss: 0.28453680872917175\n",
      "Training loss: 0.32836416363716125\n",
      "Training loss: 0.2995719611644745\n",
      "Training loss: 0.2934332489967346\n",
      "Training loss: 0.33767375349998474\n",
      "Training loss: 0.3249524235725403\n",
      "Training loss: 0.21518664062023163\n",
      "Training loss: 0.41099783778190613\n",
      "Training loss: 0.33971089124679565\n",
      "Training loss: 0.2837948501110077\n",
      "Training loss: 0.3979979157447815\n",
      "Training loss: 0.28731393814086914\n",
      "Training loss: 0.2691645920276642\n",
      "Training loss: 0.3426276743412018\n",
      "Training loss: 0.3011288642883301\n",
      "Training loss: 0.41790810227394104\n",
      "Training loss: 0.3180619776248932\n",
      "Training loss: 0.29171085357666016\n",
      "Training loss: 0.3532501459121704\n",
      "Training loss: 0.3481902480125427\n",
      "Training loss: 0.34196242690086365\n",
      "Training loss: 0.30657437443733215\n",
      "Training loss: 0.32218021154403687\n",
      "Training loss: 0.3255104720592499\n",
      "Training loss: 0.3947029113769531\n",
      "Training loss: 0.35332196950912476\n",
      "Training loss: 0.4223397672176361\n",
      "Training loss: 0.3064318001270294\n",
      "Training loss: 0.27021723985671997\n",
      "Training loss: 0.3083949685096741\n",
      "Training loss: 0.3181672990322113\n",
      "Training loss: 0.28004541993141174\n",
      "Training loss: 0.28451117873191833\n",
      "Training loss: 0.28268933296203613\n",
      "Training loss: 0.271101713180542\n",
      "Training loss: 0.2548114061355591\n",
      "Training loss: 0.21584081649780273\n",
      "Training loss: 0.31591862440109253\n",
      "Training loss: 0.32137733697891235\n",
      "Training loss: 0.32677555084228516\n",
      "Training loss: 0.3941691517829895\n",
      "Training loss: 0.30300042033195496\n",
      "Training loss: 0.297471284866333\n",
      "Training loss: 0.3545256555080414\n",
      "Training loss: 0.2650984227657318\n",
      "Training loss: 0.23101268708705902\n",
      "Training loss: 0.2585088312625885\n",
      "Training loss: 0.3090626299381256\n",
      "Training loss: 0.20509743690490723\n",
      "Training loss: 0.33508065342903137\n",
      "Training loss: 0.28930503129959106\n",
      "Training loss: 0.3149248957633972\n",
      "Training loss: 0.315062016248703\n",
      "Training loss: 0.35947635769844055\n",
      "Training loss: 0.31846684217453003\n",
      "Training loss: 0.2829837203025818\n",
      "Training loss: 0.3275720775127411\n",
      "Training loss: 0.2983018755912781\n",
      "Training loss: 0.29210391640663147\n",
      "Training loss: 0.3368927538394928\n",
      "Training loss: 0.32430461049079895\n",
      "Training loss: 0.21298252046108246\n",
      "Training loss: 0.41148829460144043\n",
      "Training loss: 0.33968740701675415\n",
      "Training loss: 0.2827222943305969\n",
      "Training loss: 0.39845389127731323\n",
      "Training loss: 0.2854737937450409\n",
      "Training loss: 0.2674493193626404\n",
      "Training loss: 0.34260696172714233\n",
      "Training loss: 0.29958024621009827\n",
      "Training loss: 0.41914284229278564\n",
      "Training loss: 0.31765928864479065\n",
      "Training loss: 0.28974205255508423\n",
      "Training loss: 0.35334742069244385\n",
      "Training loss: 0.34781602025032043\n",
      "Training loss: 0.34105396270751953\n",
      "Training loss: 0.30560940504074097\n",
      "Training loss: 0.32141852378845215\n",
      "Training loss: 0.3245880603790283\n",
      "Training loss: 0.3957723081111908\n",
      "Training loss: 0.3532302677631378\n",
      "Training loss: 0.42286503314971924\n",
      "Training loss: 0.30598586797714233\n",
      "Training loss: 0.2687973082065582\n",
      "Training loss: 0.3069882392883301\n",
      "Training loss: 0.31731951236724854\n",
      "Training loss: 0.2790578305721283\n",
      "Training loss: 0.28365376591682434\n",
      "Training loss: 0.28114211559295654\n",
      "Training loss: 0.26961275935173035\n",
      "Training loss: 0.25315606594085693\n",
      "Training loss: 0.21324768662452698\n",
      "Training loss: 0.315309077501297\n",
      "Training loss: 0.3209201395511627\n",
      "Training loss: 0.3255695700645447\n",
      "Training loss: 0.3942042887210846\n",
      "Training loss: 0.30155709385871887\n",
      "Training loss: 0.2963978350162506\n",
      "Training loss: 0.3545408844947815\n",
      "Training loss: 0.26360806822776794\n",
      "Training loss: 0.2286713719367981\n",
      "Training loss: 0.2563077211380005\n",
      "Training loss: 0.30793488025665283\n",
      "Training loss: 0.20249663293361664\n",
      "Training loss: 0.33504414558410645\n",
      "Training loss: 0.2881855070590973\n",
      "Training loss: 0.3136444091796875\n",
      "Training loss: 0.3146587014198303\n",
      "Training loss: 0.3591611385345459\n",
      "Training loss: 0.3179583251476288\n",
      "Training loss: 0.2816644012928009\n",
      "Training loss: 0.3269335925579071\n",
      "Training loss: 0.2972383201122284\n",
      "Training loss: 0.29098543524742126\n",
      "Training loss: 0.33626243472099304\n",
      "Training loss: 0.32381412386894226\n",
      "Training loss: 0.21106116473674774\n",
      "Training loss: 0.41202685236930847\n",
      "Training loss: 0.3397623598575592\n",
      "Training loss: 0.2818371057510376\n",
      "Training loss: 0.39895474910736084\n",
      "Training loss: 0.2838720977306366\n",
      "Training loss: 0.26597994565963745\n",
      "Training loss: 0.3426797389984131\n",
      "Training loss: 0.2982562780380249\n",
      "Training loss: 0.4203600287437439\n",
      "Training loss: 0.3173820376396179\n",
      "Training loss: 0.28803741931915283\n",
      "Training loss: 0.3535313010215759\n",
      "Training loss: 0.3475630581378937\n",
      "Training loss: 0.3403130769729614\n",
      "Training loss: 0.3048161566257477\n",
      "Training loss: 0.32081329822540283\n",
      "Training loss: 0.32382458448410034\n",
      "Training loss: 0.3968414068222046\n",
      "Training loss: 0.3532261848449707\n",
      "Training loss: 0.42342302203178406\n",
      "Training loss: 0.30566689372062683\n",
      "Training loss: 0.2675882875919342\n",
      "Training loss: 0.30578702688217163\n",
      "Training loss: 0.3166355490684509\n",
      "Training loss: 0.2782426178455353\n",
      "Training loss: 0.28296253085136414\n",
      "Training loss: 0.2798168361186981\n",
      "Training loss: 0.26833969354629517\n",
      "Training loss: 0.25173676013946533\n",
      "Training loss: 0.21095213294029236\n",
      "Training loss: 0.3148353695869446\n",
      "Training loss: 0.3205915093421936\n",
      "Training loss: 0.3245468735694885\n",
      "Training loss: 0.39431554079055786\n",
      "Training loss: 0.30031901597976685\n",
      "Training loss: 0.2954941987991333\n",
      "Training loss: 0.35463228821754456\n",
      "Training loss: 0.2623334228992462\n",
      "Training loss: 0.2266145497560501\n",
      "Training loss: 0.25437742471694946\n",
      "Training loss: 0.30698657035827637\n",
      "Training loss: 0.20020271837711334\n",
      "Training loss: 0.3350890874862671\n",
      "Training loss: 0.2872413992881775\n",
      "Training loss: 0.3125552237033844\n",
      "Training loss: 0.31436726450920105\n",
      "Training loss: 0.35895001888275146\n",
      "Training loss: 0.31757181882858276\n",
      "Training loss: 0.2805402874946594\n",
      "Training loss: 0.32641875743865967\n",
      "Training loss: 0.29634523391723633\n",
      "Training loss: 0.2900412976741791\n",
      "Training loss: 0.3357532322406769\n",
      "Training loss: 0.32344862818717957\n",
      "Training loss: 0.20938092470169067\n",
      "Training loss: 0.41259193420410156\n",
      "Training loss: 0.33990877866744995\n",
      "Training loss: 0.2811051905155182\n",
      "Training loss: 0.3994798958301544\n",
      "Training loss: 0.28247347474098206\n",
      "Training loss: 0.26471710205078125\n",
      "Training loss: 0.3428202271461487\n",
      "Training loss: 0.2971210479736328\n",
      "Training loss: 0.42154359817504883\n",
      "Training loss: 0.31720253825187683\n",
      "Training loss: 0.28655770421028137\n",
      "Training loss: 0.3537759780883789\n",
      "Training loss: 0.34740424156188965\n",
      "Training loss: 0.33970850706100464\n",
      "Training loss: 0.30416378378868103\n",
      "Training loss: 0.3203338384628296\n",
      "Training loss: 0.323190301656723\n",
      "Training loss: 0.3978915512561798\n",
      "Training loss: 0.353286474943161\n",
      "Training loss: 0.42399564385414124\n",
      "Training loss: 0.30544716119766235\n",
      "Training loss: 0.26655519008636475\n",
      "Training loss: 0.3047577440738678\n",
      "Training loss: 0.31608447432518005\n",
      "Training loss: 0.27756839990615845\n",
      "Training loss: 0.2824062705039978\n",
      "Training loss: 0.2786789536476135\n",
      "Training loss: 0.26724833250045776\n",
      "Training loss: 0.25051721930503845\n",
      "Training loss: 0.20891495048999786\n",
      "Training loss: 0.3144698143005371\n",
      "Training loss: 0.32036352157592773\n",
      "Training loss: 0.32367652654647827\n",
      "Training loss: 0.39448097348213196\n",
      "Training loss: 0.29925382137298584\n",
      "Training loss: 0.2947312593460083\n",
      "Training loss: 0.3547782003879547\n",
      "Training loss: 0.26124000549316406\n",
      "Training loss: 0.22480306029319763\n",
      "Training loss: 0.25268030166625977\n",
      "Training loss: 0.3061871826648712\n",
      "Training loss: 0.19817475974559784\n",
      "Training loss: 0.3351929783821106\n",
      "Training loss: 0.2864426076412201\n",
      "Training loss: 0.31162700057029724\n",
      "Training loss: 0.314163476228714\n",
      "Training loss: 0.3588193953037262\n",
      "Training loss: 0.31728193163871765\n",
      "Training loss: 0.2795799672603607\n",
      "Training loss: 0.32600343227386475\n",
      "Training loss: 0.295592725276947\n",
      "Training loss: 0.28924208879470825\n",
      "Training loss: 0.33534157276153564\n",
      "Training loss: 0.3231825828552246\n",
      "Training loss: 0.20790760219097137\n",
      "Training loss: 0.41316714882850647\n",
      "Training loss: 0.34010568261146545\n",
      "Training loss: 0.2804991900920868\n",
      "Training loss: 0.40001392364501953\n",
      "Training loss: 0.2812485098838806\n",
      "Training loss: 0.26362916827201843\n",
      "Training loss: 0.3430083394050598\n",
      "Training loss: 0.29614508152008057\n",
      "Training loss: 0.42268139123916626\n",
      "Training loss: 0.31709834933280945\n",
      "Training loss: 0.2852703928947449\n",
      "Training loss: 0.35406142473220825\n",
      "Training loss: 0.3473180830478668\n",
      "Training loss: 0.33921489119529724\n",
      "Training loss: 0.303627073764801\n",
      "Training loss: 0.3199555575847626\n",
      "Training loss: 0.3226620554924011\n",
      "Training loss: 0.3989090919494629\n",
      "Training loss: 0.35339289903640747\n",
      "Training loss: 0.42456886172294617\n",
      "Training loss: 0.3053044378757477\n",
      "Training loss: 0.2656693756580353\n",
      "Training loss: 0.3038731515407562\n",
      "Training loss: 0.3156413733959198\n",
      "Training loss: 0.2770095467567444\n",
      "Training loss: 0.28195956349372864\n",
      "Training loss: 0.27769964933395386\n",
      "Training loss: 0.26631033420562744\n",
      "Training loss: 0.24946728348731995\n",
      "Training loss: 0.20710313320159912\n",
      "Training loss: 0.3141900897026062\n",
      "Training loss: 0.3202139139175415\n",
      "Training loss: 0.32293328642845154\n",
      "Training loss: 0.3946831524372101\n",
      "Training loss: 0.29833489656448364\n",
      "Training loss: 0.2940853536128998\n",
      "Training loss: 0.35496240854263306\n",
      "Training loss: 0.26029956340789795\n",
      "Training loss: 0.22320416569709778\n",
      "Training loss: 0.2511848509311676\n",
      "Training loss: 0.3055118918418884\n",
      "Training loss: 0.19637808203697205\n",
      "Training loss: 0.33533820509910583\n",
      "Training loss: 0.2857646942138672\n",
      "Training loss: 0.31083443760871887\n",
      "Training loss: 0.31402796506881714\n",
      "Training loss: 0.35875022411346436\n",
      "Training loss: 0.3170683979988098\n",
      "Training loss: 0.27875757217407227\n",
      "Training loss: 0.3256683647632599\n",
      "Training loss: 0.2949570417404175\n",
      "Training loss: 0.28856360912323\n",
      "Training loss: 0.3350083529949188\n",
      "Training loss: 0.3229953646659851\n",
      "Training loss: 0.20661260187625885\n",
      "Training loss: 0.41374000906944275\n",
      "Training loss: 0.34033679962158203\n",
      "Training loss: 0.2799968123435974\n",
      "Training loss: 0.40054500102996826\n",
      "Training loss: 0.28017300367355347\n",
      "Training loss: 0.2626899480819702\n",
      "Training loss: 0.3432285487651825\n",
      "Training loss: 0.2953038811683655\n",
      "Training loss: 0.42376527190208435\n",
      "Training loss: 0.3170519769191742\n",
      "Training loss: 0.28414830565452576\n",
      "Training loss: 0.35437214374542236\n",
      "Training loss: 0.3472871780395508\n",
      "Training loss: 0.3388117551803589\n",
      "Training loss: 0.30318573117256165\n",
      "Training loss: 0.31965869665145874\n",
      "Training loss: 0.32222023606300354\n",
      "Training loss: 0.3998843729496002\n",
      "Training loss: 0.3535313010215759\n",
      "Training loss: 0.4251324236392975\n",
      "Training loss: 0.30522119998931885\n",
      "Training loss: 0.2649076282978058\n",
      "Training loss: 0.30311042070388794\n",
      "Training loss: 0.3152860105037689\n",
      "Training loss: 0.2765456438064575\n",
      "Training loss: 0.28160199522972107\n",
      "Training loss: 0.2768552303314209\n",
      "Training loss: 0.26550227403640747\n",
      "Training loss: 0.2485615611076355\n",
      "Training loss: 0.20548900961875916\n",
      "Training loss: 0.3139783442020416\n",
      "Training loss: 0.32012486457824707\n",
      "Training loss: 0.322296679019928\n",
      "Training loss: 0.3949089050292969\n",
      "Training loss: 0.29754015803337097\n",
      "Training loss: 0.29353705048561096\n",
      "Training loss: 0.3551713824272156\n",
      "Training loss: 0.2594887614250183\n",
      "Training loss: 0.22179032862186432\n",
      "Training loss: 0.24986471235752106\n",
      "Training loss: 0.3049403131008148\n",
      "Training loss: 0.1947837769985199\n",
      "Training loss: 0.33551108837127686\n",
      "Training loss: 0.28518760204315186\n",
      "Training loss: 0.31015658378601074\n",
      "Training loss: 0.31394535303115845\n",
      "Training loss: 0.3587276041507721\n",
      "Training loss: 0.31691503524780273\n",
      "Training loss: 0.2780517339706421\n",
      "Training loss: 0.3253979980945587\n",
      "Training loss: 0.2944185137748718\n",
      "Training loss: 0.2879859507083893\n",
      "Training loss: 0.33473852276802063\n",
      "Training loss: 0.32287031412124634\n",
      "Training loss: 0.20547203719615936\n",
      "Training loss: 0.4143013656139374\n",
      "Training loss: 0.3405892252922058\n",
      "Training loss: 0.2795799672603607\n",
      "Training loss: 0.4010644257068634\n",
      "Training loss: 0.27922651171684265\n",
      "Training loss: 0.26187705993652344\n",
      "Training loss: 0.3434686064720154\n",
      "Training loss: 0.2945772409439087\n",
      "Training loss: 0.42479026317596436\n",
      "Training loss: 0.3170492947101593\n",
      "Training loss: 0.2831684350967407\n",
      "Training loss: 0.35469621419906616\n",
      "Training loss: 0.3472979962825775\n",
      "Training loss: 0.33848243951797485\n",
      "Training loss: 0.30282285809516907\n",
      "Training loss: 0.31942710280418396\n",
      "Training loss: 0.3218497931957245\n",
      "Training loss: 0.40081098675727844\n",
      "Training loss: 0.3536907136440277\n",
      "Training loss: 0.4256789982318878\n",
      "Training loss: 0.3051833212375641\n",
      "Training loss: 0.26425057649612427\n",
      "Training loss: 0.3024512231349945\n",
      "Training loss: 0.31500184535980225\n",
      "Training loss: 0.2761598825454712\n",
      "Training loss: 0.2813168466091156\n",
      "Training loss: 0.27612584829330444\n",
      "Training loss: 0.2648048400878906\n",
      "Training loss: 0.24777938425540924\n",
      "Training loss: 0.2040490061044693\n",
      "Training loss: 0.31382036209106445\n",
      "Training loss: 0.32008224725723267\n",
      "Training loss: 0.32174980640411377\n",
      "Training loss: 0.3951479494571686\n",
      "Training loss: 0.29685139656066895\n",
      "Training loss: 0.29307037591934204\n",
      "Training loss: 0.35539528727531433\n",
      "Training loss: 0.25878822803497314\n",
      "Training loss: 0.2205381989479065\n",
      "Training loss: 0.2486974149942398\n",
      "Training loss: 0.304455429315567\n",
      "Training loss: 0.1933670938014984\n",
      "Training loss: 0.335701048374176\n",
      "Training loss: 0.2846950888633728\n",
      "Training loss: 0.30957597494125366\n",
      "Training loss: 0.3139033317565918\n",
      "Training loss: 0.3587397634983063\n",
      "Training loss: 0.3168087303638458\n",
      "Training loss: 0.2774447500705719\n",
      "Training loss: 0.32518014311790466\n",
      "Training loss: 0.29396122694015503\n",
      "Training loss: 0.28749310970306396\n",
      "Training loss: 0.33451977372169495\n",
      "Training loss: 0.32279375195503235\n",
      "Training loss: 0.20446567237377167\n",
      "Training loss: 0.4148443937301636\n",
      "Training loss: 0.34085315465927124\n",
      "Training loss: 0.2792336344718933\n",
      "Training loss: 0.40156587958335876\n",
      "Training loss: 0.27839216589927673\n",
      "Training loss: 0.26117241382598877\n",
      "Training loss: 0.34371912479400635\n",
      "Training loss: 0.2939481735229492\n",
      "Training loss: 0.4257534444332123\n",
      "Training loss: 0.31707900762557983\n",
      "Training loss: 0.28231143951416016\n",
      "Training loss: 0.35502445697784424\n",
      "Training loss: 0.34733954071998596\n",
      "Training loss: 0.33821356296539307\n",
      "Training loss: 0.30252477526664734\n",
      "Training loss: 0.31924793124198914\n",
      "Training loss: 0.32153812050819397\n",
      "Training loss: 0.40168464183807373\n",
      "Training loss: 0.3538626730442047\n",
      "Training loss: 0.4262029528617859\n",
      "Training loss: 0.30517977476119995\n",
      "Training loss: 0.26368242502212524\n",
      "Training loss: 0.30188000202178955\n",
      "Training loss: 0.31477537751197815\n",
      "Training loss: 0.2758384943008423\n",
      "Training loss: 0.2810905873775482\n",
      "Training loss: 0.27549487352371216\n",
      "Training loss: 0.2642017602920532\n",
      "Training loss: 0.24710291624069214\n",
      "Training loss: 0.20276296138763428\n",
      "Training loss: 0.31370463967323303\n",
      "Training loss: 0.3200746178627014\n",
      "Training loss: 0.32127857208251953\n",
      "Training loss: 0.39539211988449097\n",
      "Training loss: 0.2962532043457031\n",
      "Training loss: 0.2926723062992096\n",
      "Training loss: 0.35562610626220703\n",
      "Training loss: 0.2581818103790283\n",
      "Training loss: 0.2194279134273529\n",
      "Training loss: 0.2476639747619629\n",
      "Training loss: 0.30404356122016907\n",
      "Training loss: 0.19210681319236755\n",
      "Training loss: 0.3359000086784363\n",
      "Training loss: 0.28427356481552124\n",
      "Training loss: 0.309078186750412\n",
      "Training loss: 0.31389203667640686\n",
      "Training loss: 0.3587772250175476\n",
      "Training loss: 0.3167390823364258\n",
      "Training loss: 0.276921808719635\n",
      "Training loss: 0.3250042498111725\n",
      "Training loss: 0.2935717701911926\n",
      "Training loss: 0.2870713770389557\n",
      "Training loss: 0.3343421518802643\n",
      "Training loss: 0.32275474071502686\n",
      "Training loss: 0.20357640087604523\n",
      "Training loss: 0.4153643846511841\n",
      "Training loss: 0.34112095832824707\n",
      "Training loss: 0.27894559502601624\n",
      "Training loss: 0.4020450711250305\n",
      "Training loss: 0.2776552438735962\n",
      "Training loss: 0.26056039333343506\n",
      "Training loss: 0.34397292137145996\n",
      "Training loss: 0.2934027314186096\n",
      "Training loss: 0.4266538619995117\n",
      "Training loss: 0.31713223457336426\n",
      "Training loss: 0.28156110644340515\n",
      "Training loss: 0.3553500771522522\n",
      "Training loss: 0.3474031686782837\n",
      "Training loss: 0.3379937410354614\n",
      "Training loss: 0.3022802472114563\n",
      "Training loss: 0.31911060214042664\n",
      "Training loss: 0.32127469778060913\n",
      "Training loss: 0.40250325202941895\n",
      "Training loss: 0.3540404736995697\n",
      "Training loss: 0.4267007112503052\n",
      "Training loss: 0.30520153045654297\n",
      "Training loss: 0.26318979263305664\n",
      "Training loss: 0.30138373374938965\n",
      "Training loss: 0.3145959973335266\n",
      "Training loss: 0.2755703926086426\n",
      "Training loss: 0.28091225028038025\n",
      "Training loss: 0.27494844794273376\n",
      "Training loss: 0.26367971301078796\n",
      "Training loss: 0.24651756882667542\n",
      "Training loss: 0.2016133964061737\n",
      "Training loss: 0.31362178921699524\n",
      "Training loss: 0.3200933635234833\n",
      "Training loss: 0.3208713233470917\n",
      "Training loss: 0.39563554525375366\n",
      "Training loss: 0.29573291540145874\n",
      "Training loss: 0.29233208298683167\n",
      "Training loss: 0.35585784912109375\n",
      "Training loss: 0.2576559782028198\n",
      "Training loss: 0.21844249963760376\n",
      "Training loss: 0.24674804508686066\n",
      "Training loss: 0.3036929666996002\n",
      "Training loss: 0.19098472595214844\n",
      "Training loss: 0.3361017107963562\n",
      "Training loss: 0.2839118242263794\n",
      "Training loss: 0.3086509108543396\n",
      "Training loss: 0.3139035999774933\n",
      "Training loss: 0.3588325083255768\n",
      "Training loss: 0.3166976869106293\n",
      "Training loss: 0.2764705717563629\n",
      "Training loss: 0.3248623311519623\n",
      "Training loss: 0.2932392358779907\n",
      "Training loss: 0.28670984506607056\n",
      "Training loss: 0.33419784903526306\n",
      "Training loss: 0.3227447271347046\n",
      "Training loss: 0.20278970897197723\n",
      "Training loss: 0.41585779190063477\n",
      "Training loss: 0.34138694405555725\n",
      "Training loss: 0.2787059545516968\n",
      "Training loss: 0.4024988114833832\n",
      "Training loss: 0.2770036458969116\n",
      "Training loss: 0.2600283622741699\n",
      "Training loss: 0.3442246913909912\n",
      "Training loss: 0.2929288148880005\n",
      "Training loss: 0.42749089002609253\n",
      "Training loss: 0.31720203161239624\n",
      "Training loss: 0.2809031903743744\n",
      "Training loss: 0.3556675314903259\n",
      "Training loss: 0.3474820554256439\n",
      "Training loss: 0.33781421184539795\n",
      "Training loss: 0.3020798861980438\n",
      "Training loss: 0.31900668144226074\n",
      "Training loss: 0.3210517168045044\n",
      "Training loss: 0.40326589345932007\n",
      "Training loss: 0.3542192578315735\n",
      "Training loss: 0.4271698296070099\n",
      "Training loss: 0.3052416443824768\n",
      "Training loss: 0.262761652469635\n",
      "Training loss: 0.3009517788887024\n",
      "Training loss: 0.3144541382789612\n",
      "Training loss: 0.2753462791442871\n",
      "Training loss: 0.2807726263999939\n",
      "Training loss: 0.2744745910167694\n",
      "Training loss: 0.2632269263267517\n",
      "Training loss: 0.24601027369499207\n",
      "Training loss: 0.20058538019657135\n",
      "Training loss: 0.31356462836265564\n",
      "Training loss: 0.3201308846473694\n",
      "Training loss: 0.3205186724662781\n",
      "Training loss: 0.3958736062049866\n",
      "Training loss: 0.2952793836593628\n",
      "Training loss: 0.29204046726226807\n",
      "Training loss: 0.35608622431755066\n",
      "Training loss: 0.257199227809906\n",
      "Training loss: 0.21756717562675476\n",
      "Training loss: 0.2459355592727661\n",
      "Training loss: 0.3033939599990845\n",
      "Training loss: 0.18998509645462036\n",
      "Training loss: 0.3363014757633209\n",
      "Training loss: 0.2836008071899414\n",
      "Training loss: 0.30828362703323364\n",
      "Training loss: 0.3139318823814392\n",
      "Training loss: 0.3589000105857849\n",
      "Training loss: 0.316677987575531\n",
      "Training loss: 0.27608057856559753\n",
      "Training loss: 0.324747771024704\n",
      "Training loss: 0.29295462369918823\n",
      "Training loss: 0.28639906644821167\n",
      "Training loss: 0.3340803384780884\n",
      "Training loss: 0.32275626063346863\n",
      "Training loss: 0.2020929604768753\n",
      "Training loss: 0.4163229763507843\n",
      "Training loss: 0.3416467010974884\n",
      "Training loss: 0.278506338596344\n",
      "Training loss: 0.4029254913330078\n",
      "Training loss: 0.27642691135406494\n",
      "Training loss: 0.2595651149749756\n",
      "Training loss: 0.34447014331817627\n",
      "Training loss: 0.29251646995544434\n",
      "Training loss: 0.42826634645462036\n",
      "Training loss: 0.3172827363014221\n",
      "Training loss: 0.28032588958740234\n",
      "Training loss: 0.355973482131958\n",
      "Training loss: 0.3475707471370697\n",
      "Training loss: 0.3376676142215729\n",
      "Training loss: 0.3019160032272339\n",
      "Training loss: 0.3189290761947632\n",
      "Training loss: 0.32086193561553955\n",
      "Training loss: 0.40397316217422485\n",
      "Training loss: 0.35439532995224\n",
      "Training loss: 0.4276091456413269\n",
      "Training loss: 0.3052945137023926\n",
      "Training loss: 0.26238882541656494\n",
      "Training loss: 0.30057498812675476\n",
      "Training loss: 0.3143433630466461\n",
      "Training loss: 0.27515867352485657\n",
      "Training loss: 0.280664324760437\n",
      "Training loss: 0.274063378572464\n",
      "Training loss: 0.2628338932991028\n",
      "Training loss: 0.24557065963745117\n",
      "Training loss: 0.19966566562652588\n",
      "Training loss: 0.31352707743644714\n",
      "Training loss: 0.32018181681632996\n",
      "Training loss: 0.3202122747898102\n",
      "Training loss: 0.3961031436920166\n",
      "Training loss: 0.29488372802734375\n",
      "Training loss: 0.2917901277542114\n",
      "Training loss: 0.35630765557289124\n",
      "Training loss: 0.25680193305015564\n",
      "Training loss: 0.21678927540779114\n",
      "Training loss: 0.24521443247795105\n",
      "Training loss: 0.3031386733055115\n",
      "Training loss: 0.1890942007303238\n",
      "Training loss: 0.33649590611457825\n",
      "Training loss: 0.2833326458930969\n",
      "Training loss: 0.30796775221824646\n",
      "Training loss: 0.31397196650505066\n",
      "Training loss: 0.35897502303123474\n",
      "Training loss: 0.3166743516921997\n",
      "Training loss: 0.2757430374622345\n",
      "Training loss: 0.3246553838253021\n",
      "Training loss: 0.29271066188812256\n",
      "Training loss: 0.28613144159317017\n",
      "Training loss: 0.33398446440696716\n",
      "Training loss: 0.32278376817703247\n",
      "Training loss: 0.20147539675235748\n",
      "Training loss: 0.41675856709480286\n",
      "Training loss: 0.34189707040786743\n",
      "Training loss: 0.27833983302116394\n",
      "Training loss: 0.4033241868019104\n",
      "Training loss: 0.27591580152511597\n",
      "Training loss: 0.2591610550880432\n",
      "Training loss: 0.3447065055370331\n",
      "Training loss: 0.2921570837497711\n",
      "Training loss: 0.42898204922676086\n",
      "Training loss: 0.3173699676990509\n",
      "Training loss: 0.2798188328742981\n",
      "Training loss: 0.356265127658844\n",
      "Training loss: 0.34766507148742676\n",
      "Training loss: 0.33754777908325195\n",
      "Training loss: 0.30178213119506836\n",
      "Training loss: 0.3188723921775818\n",
      "Training loss: 0.3207000494003296\n",
      "Training loss: 0.4046258330345154\n",
      "Training loss: 0.35456588864326477\n",
      "Training loss: 0.4280182719230652\n",
      "Training loss: 0.30535566806793213\n",
      "Training loss: 0.2620634436607361\n",
      "Training loss: 0.3002457618713379\n",
      "Training loss: 0.31425657868385315\n",
      "Training loss: 0.2750014066696167\n",
      "Training loss: 0.2805812656879425\n",
      "Training loss: 0.2737061381340027\n",
      "Training loss: 0.26249244809150696\n",
      "Training loss: 0.24518927931785583\n",
      "Training loss: 0.19884279370307922\n",
      "Training loss: 0.3135043680667877\n",
      "Training loss: 0.3202415108680725\n",
      "Training loss: 0.3199456036090851\n",
      "Training loss: 0.3963218927383423\n",
      "Training loss: 0.2945380210876465\n",
      "Training loss: 0.29157477617263794\n",
      "Training loss: 0.35651999711990356\n",
      "Training loss: 0.2564559876918793\n",
      "Training loss: 0.21609771251678467\n",
      "Training loss: 0.24457408487796783\n",
      "Training loss: 0.30292025208473206\n",
      "Training loss: 0.1882999837398529\n",
      "Training loss: 0.33668252825737\n",
      "Training loss: 0.28310126066207886\n",
      "Training loss: 0.307695597410202\n",
      "Training loss: 0.3140200078487396\n",
      "Training loss: 0.3590542674064636\n",
      "Training loss: 0.3166828751564026\n",
      "Training loss: 0.2754504382610321\n",
      "Training loss: 0.3245807886123657\n",
      "Training loss: 0.2925007939338684\n",
      "Training loss: 0.2859004735946655\n",
      "Training loss: 0.333905965089798\n",
      "Training loss: 0.3228225111961365\n",
      "Training loss: 0.20092765986919403\n",
      "Training loss: 0.41716429591178894\n",
      "Training loss: 0.3421359062194824\n",
      "Training loss: 0.27820077538490295\n",
      "Training loss: 0.4036944806575775\n",
      "Training loss: 0.2754627764225006\n",
      "Training loss: 0.2588084042072296\n",
      "Training loss: 0.3449316918849945\n",
      "Training loss: 0.2918434739112854\n",
      "Training loss: 0.429639995098114\n",
      "Training loss: 0.3174605965614319\n",
      "Training loss: 0.2793731391429901\n",
      "Training loss: 0.35654065012931824\n",
      "Training loss: 0.3477618098258972\n",
      "Training loss: 0.3374500274658203\n",
      "Training loss: 0.3016730844974518\n",
      "Training loss: 0.31883206963539124\n",
      "Training loss: 0.32056161761283875\n",
      "Training loss: 0.40522611141204834\n",
      "Training loss: 0.3547293245792389\n",
      "Training loss: 0.4283970296382904\n",
      "Training loss: 0.3054218590259552\n",
      "Training loss: 0.2617790699005127\n",
      "Training loss: 0.29995766282081604\n",
      "Training loss: 0.3141900897026062\n",
      "Training loss: 0.27486926317214966\n",
      "Training loss: 0.28051847219467163\n",
      "Training loss: 0.27339571714401245\n",
      "Training loss: 0.2621954679489136\n",
      "Training loss: 0.24485842883586884\n",
      "Training loss: 0.198106586933136\n",
      "Training loss: 0.31349295377731323\n",
      "Training loss: 0.3203067183494568\n",
      "Training loss: 0.31971293687820435\n",
      "Training loss: 0.3965281844139099\n",
      "Training loss: 0.29423561692237854\n",
      "Training loss: 0.2913891673088074\n",
      "Training loss: 0.3567214608192444\n",
      "Training loss: 0.2561543583869934\n",
      "Training loss: 0.21548274159431458\n",
      "Training loss: 0.24400529265403748\n",
      "Training loss: 0.3027331233024597\n",
      "Training loss: 0.18759192526340485\n",
      "Training loss: 0.3368597626686096\n",
      "Training loss: 0.2829008400440216\n",
      "Training loss: 0.30746111273765564\n",
      "Training loss: 0.31407278776168823\n",
      "Training loss: 0.35913485288619995\n",
      "Training loss: 0.31669992208480835\n",
      "Training loss: 0.27519655227661133\n",
      "Training loss: 0.3245202898979187\n",
      "Training loss: 0.2923199236392975\n",
      "Training loss: 0.2857007682323456\n",
      "Training loss: 0.3338416814804077\n",
      "Training loss: 0.32286888360977173\n",
      "Training loss: 0.2004416584968567\n",
      "Training loss: 0.41754043102264404\n",
      "Training loss: 0.34236183762550354\n",
      "Training loss: 0.27808454632759094\n",
      "Training loss: 0.40403690934181213\n",
      "Training loss: 0.2750609219074249\n",
      "Training loss: 0.25850042700767517\n",
      "Training loss: 0.34514427185058594\n",
      "Training loss: 0.2915695309638977\n",
      "Training loss: 0.430242657661438\n",
      "Training loss: 0.31755170226097107\n",
      "Training loss: 0.27898114919662476\n",
      "Training loss: 0.35679924488067627\n",
      "Training loss: 0.34785839915275574\n",
      "Training loss: 0.33736997842788696\n",
      "Training loss: 0.301584392786026\n",
      "Training loss: 0.3188045024871826\n",
      "Training loss: 0.32044240832328796\n",
      "Training loss: 0.4057758152484894\n",
      "Training loss: 0.35488370060920715\n",
      "Training loss: 0.42874640226364136\n",
      "Training loss: 0.305490106344223\n",
      "Training loss: 0.2615298628807068\n",
      "Training loss: 0.29970505833625793\n",
      "Training loss: 0.31413891911506653\n",
      "Training loss: 0.27475813031196594\n",
      "Training loss: 0.2804720997810364\n",
      "Training loss: 0.2731259763240814\n",
      "Training loss: 0.2619374096393585\n",
      "Training loss: 0.24457144737243652\n",
      "Training loss: 0.1974479705095291\n",
      "Training loss: 0.31348949670791626\n",
      "Training loss: 0.32037508487701416\n",
      "Training loss: 0.31950926780700684\n",
      "Training loss: 0.39672109484672546\n",
      "Training loss: 0.29397106170654297\n",
      "Training loss: 0.29122915863990784\n",
      "Training loss: 0.35691148042678833\n",
      "Training loss: 0.25589102506637573\n",
      "Training loss: 0.21493598818778992\n",
      "Training loss: 0.24350015819072723\n",
      "Training loss: 0.3025725483894348\n",
      "Training loss: 0.1869608461856842\n",
      "Training loss: 0.3370265066623688\n",
      "Training loss: 0.28272706270217896\n",
      "Training loss: 0.3072589933872223\n",
      "Training loss: 0.31412816047668457\n",
      "Training loss: 0.35921481251716614\n",
      "Training loss: 0.31672289967536926\n",
      "Training loss: 0.27497607469558716\n",
      "Training loss: 0.32447147369384766\n",
      "Training loss: 0.2921639382839203\n",
      "Training loss: 0.2855277359485626\n",
      "Training loss: 0.33378875255584717\n",
      "Training loss: 0.32292017340660095\n",
      "Training loss: 0.2000102996826172\n",
      "Training loss: 0.4178873598575592\n",
      "Training loss: 0.34257373213768005\n",
      "Training loss: 0.2779873013496399\n",
      "Training loss: 0.4043520987033844\n",
      "Training loss: 0.27470430731773376\n",
      "Training loss: 0.2582312226295471\n",
      "Training loss: 0.3453435003757477\n",
      "Training loss: 0.29132989048957825\n",
      "Training loss: 0.43079331517219543\n",
      "Training loss: 0.31764158606529236\n",
      "Training loss: 0.2786361873149872\n",
      "Training loss: 0.35704031586647034\n",
      "Training loss: 0.3479529917240143\n",
      "Training loss: 0.33730459213256836\n",
      "Training loss: 0.3015124201774597\n",
      "Training loss: 0.3187866508960724\n",
      "Training loss: 0.32033979892730713\n",
      "Training loss: 0.4062780737876892\n",
      "Training loss: 0.3550286889076233\n",
      "Training loss: 0.4290671944618225\n",
      "Training loss: 0.3055587410926819\n",
      "Training loss: 0.2613113522529602\n",
      "Training loss: 0.29948344826698303\n",
      "Training loss: 0.3141001760959625\n",
      "Training loss: 0.27466443181037903\n",
      "Training loss: 0.2804386019706726\n",
      "Training loss: 0.2728913128376007\n",
      "Training loss: 0.2617127597332001\n",
      "Training loss: 0.24432231485843658\n",
      "Training loss: 0.19685909152030945\n",
      "Training loss: 0.31349191069602966\n",
      "Training loss: 0.32044413685798645\n",
      "Training loss: 0.31933093070983887\n",
      "Training loss: 0.3969005048274994\n",
      "Training loss: 0.29373928904533386\n",
      "Training loss: 0.29109087586402893\n",
      "Training loss: 0.3570886254310608\n",
      "Training loss: 0.2556609511375427\n",
      "Training loss: 0.21444988250732422\n",
      "Training loss: 0.2430514395236969\n",
      "Training loss: 0.3024345934391022\n",
      "Training loss: 0.186398446559906\n",
      "Training loss: 0.33718210458755493\n",
      "Training loss: 0.28257614374160767\n",
      "Training loss: 0.30708444118499756\n",
      "Training loss: 0.3141842186450958\n",
      "Training loss: 0.35929277539253235\n",
      "Training loss: 0.3167497217655182\n",
      "Training loss: 0.27478426694869995\n",
      "Training loss: 0.3244316577911377\n",
      "Training loss: 0.29202887415885925\n",
      "Training loss: 0.285377562046051\n",
      "Training loss: 0.3337450325489044\n",
      "Training loss: 0.32297396659851074\n",
      "Training loss: 0.1996273696422577\n",
      "Training loss: 0.41820642352104187\n",
      "Training loss: 0.34277135133743286\n",
      "Training loss: 0.2779057025909424\n",
      "Training loss: 0.4046410620212555\n",
      "Training loss: 0.2743879556655884\n",
      "Training loss: 0.25799551606178284\n",
      "Training loss: 0.34552907943725586\n",
      "Training loss: 0.2911200225353241\n",
      "Training loss: 0.43129515647888184\n",
      "Training loss: 0.31772881746292114\n",
      "Training loss: 0.2783324122428894\n",
      "Training loss: 0.3572636842727661\n",
      "Training loss: 0.3480442762374878\n",
      "Training loss: 0.33725106716156006\n",
      "Training loss: 0.301454097032547\n",
      "Training loss: 0.31877636909484863\n",
      "Training loss: 0.32025110721588135\n",
      "Training loss: 0.4067351818084717\n",
      "Training loss: 0.3551637530326843\n",
      "Training loss: 0.4293604791164398\n",
      "Training loss: 0.3056260943412781\n",
      "Training loss: 0.261119544506073\n",
      "Training loss: 0.299288809299469\n",
      "Training loss: 0.3140714466571808\n",
      "Training loss: 0.2745853066444397\n",
      "Training loss: 0.2804153561592102\n",
      "Training loss: 0.27268722653388977\n",
      "Training loss: 0.26151734590530396\n",
      "Training loss: 0.2441060096025467\n",
      "Training loss: 0.19633281230926514\n",
      "Training loss: 0.3134983777999878\n",
      "Training loss: 0.3205125331878662\n",
      "Training loss: 0.3191744089126587\n",
      "Training loss: 0.3970659375190735\n",
      "Training loss: 0.2935362160205841\n",
      "Training loss: 0.2909713089466095\n",
      "Training loss: 0.35725343227386475\n",
      "Training loss: 0.2554597854614258\n",
      "Training loss: 0.2140178084373474\n",
      "Training loss: 0.24265290796756744\n",
      "Training loss: 0.3023158311843872\n",
      "Training loss: 0.18589739501476288\n",
      "Training loss: 0.33732640743255615\n",
      "Training loss: 0.2824447751045227\n",
      "Training loss: 0.30693358182907104\n",
      "Training loss: 0.3142397105693817\n",
      "Training loss: 0.3593677282333374\n",
      "Training loss: 0.31677889823913574\n",
      "Training loss: 0.2746172249317169\n",
      "Training loss: 0.3243994414806366\n",
      "Training loss: 0.2919118404388428\n",
      "Training loss: 0.2852470278739929\n",
      "Training loss: 0.33370888233184814\n",
      "Training loss: 0.3230283558368683\n",
      "Training loss: 0.19928736984729767\n",
      "Training loss: 0.4184986650943756\n",
      "Training loss: 0.3429543972015381\n",
      "Training loss: 0.2778370678424835\n",
      "Training loss: 0.40490517020225525\n",
      "Training loss: 0.2741071581840515\n",
      "Training loss: 0.25778913497924805\n",
      "Training loss: 0.3457008898258209\n",
      "Training loss: 0.29093605279922485\n",
      "Training loss: 0.43175187706947327\n",
      "Training loss: 0.3178122639656067\n",
      "Training loss: 0.2780648171901703\n",
      "Training loss: 0.3574698567390442\n",
      "Training loss: 0.34813135862350464\n",
      "Training loss: 0.3372073769569397\n",
      "Training loss: 0.30140694975852966\n",
      "Training loss: 0.318771630525589\n",
      "Training loss: 0.3201743960380554\n",
      "Training loss: 0.4071500301361084\n",
      "Training loss: 0.35528865456581116\n",
      "Training loss: 0.4296281933784485\n",
      "Training loss: 0.30569103360176086\n",
      "Training loss: 0.2609507739543915\n",
      "Training loss: 0.2991175651550293\n",
      "Training loss: 0.3140502870082855\n",
      "Training loss: 0.2745184600353241\n",
      "Training loss: 0.2804001271724701\n",
      "Training loss: 0.27250969409942627\n",
      "Training loss: 0.26134732365608215\n",
      "Training loss: 0.24391837418079376\n",
      "Training loss: 0.19586268067359924\n",
      "Training loss: 0.3135073781013489\n",
      "Training loss: 0.32057926058769226\n",
      "Training loss: 0.31903672218322754\n",
      "Training loss: 0.39721786975860596\n",
      "Training loss: 0.2933582067489624\n",
      "Training loss: 0.29086774587631226\n",
      "Training loss: 0.35740551352500916\n",
      "Training loss: 0.2552837133407593\n",
      "Training loss: 0.21363389492034912\n",
      "Training loss: 0.24229907989501953\n",
      "Training loss: 0.302213579416275\n",
      "Training loss: 0.18545125424861908\n",
      "Training loss: 0.3374595046043396\n",
      "Training loss: 0.28233057260513306\n",
      "Training loss: 0.30680298805236816\n",
      "Training loss: 0.3142937123775482\n",
      "Training loss: 0.3594389855861664\n",
      "Training loss: 0.3168090879917145\n",
      "Training loss: 0.27447161078453064\n",
      "Training loss: 0.3243730664253235\n",
      "Training loss: 0.29181021451950073\n",
      "Training loss: 0.285133421421051\n",
      "Training loss: 0.33367887139320374\n",
      "Training loss: 0.32308223843574524\n",
      "Training loss: 0.1989854872226715\n",
      "Training loss: 0.41876542568206787\n",
      "Training loss: 0.34312334656715393\n",
      "Training loss: 0.277779221534729\n",
      "Training loss: 0.40514564514160156\n",
      "Training loss: 0.27385810017585754\n",
      "Training loss: 0.2576081454753876\n",
      "Training loss: 0.345859169960022\n",
      "Training loss: 0.2907746732234955\n",
      "Training loss: 0.43216559290885925\n",
      "Training loss: 0.31789150834083557\n",
      "Training loss: 0.27782896161079407\n",
      "Training loss: 0.35765910148620605\n",
      "Training loss: 0.34821346402168274\n",
      "Training loss: 0.33717161417007446\n",
      "Training loss: 0.30136898159980774\n",
      "Training loss: 0.3187711536884308\n",
      "Training loss: 0.32010766863822937\n",
      "Training loss: 0.40752577781677246\n",
      "Training loss: 0.35540375113487244\n",
      "Training loss: 0.4298712909221649\n",
      "Training loss: 0.3057529628276825\n",
      "Training loss: 0.2608022391796112\n",
      "Training loss: 0.2989669442176819\n",
      "Training loss: 0.3140351176261902\n",
      "Training loss: 0.2744618356227875\n",
      "Training loss: 0.28039127588272095\n",
      "Training loss: 0.27235540747642517\n",
      "Training loss: 0.2611994743347168\n",
      "Training loss: 0.24375566840171814\n",
      "Training loss: 0.1954430490732193\n",
      "Training loss: 0.31351786851882935\n",
      "Training loss: 0.3206433951854706\n",
      "Training loss: 0.3189154267311096\n",
      "Training loss: 0.39735662937164307\n",
      "Training loss: 0.29320210218429565\n",
      "Training loss: 0.2907780408859253\n",
      "Training loss: 0.35754531621932983\n",
      "Training loss: 0.25512951612472534\n",
      "Training loss: 0.21329298615455627\n",
      "Training loss: 0.24198512732982635\n",
      "Training loss: 0.3021252453327179\n",
      "Training loss: 0.185054212808609\n",
      "Training loss: 0.33758148550987244\n",
      "Training loss: 0.28223079442977905\n",
      "Training loss: 0.306689977645874\n",
      "Training loss: 0.31434541940689087\n",
      "Training loss: 0.35950592160224915\n",
      "Training loss: 0.3168395161628723\n",
      "Training loss: 0.2743445634841919\n",
      "Training loss: 0.32435157895088196\n",
      "Training loss: 0.29172199964523315\n",
      "Training loss: 0.2850343585014343\n",
      "Training loss: 0.33365383744239807\n",
      "Training loss: 0.3231344223022461\n",
      "Training loss: 0.19871748983860016\n",
      "Training loss: 0.41900840401649475\n",
      "Training loss: 0.3432783782482147\n",
      "Training loss: 0.2777302861213684\n",
      "Training loss: 0.4053640067577362\n",
      "Training loss: 0.27363720536231995\n",
      "Training loss: 0.2574496269226074\n",
      "Training loss: 0.3460043668746948\n",
      "Training loss: 0.290632963180542\n",
      "Training loss: 0.4325406551361084\n",
      "Training loss: 0.31796568632125854\n",
      "Training loss: 0.2776211202144623\n",
      "Training loss: 0.35783231258392334\n",
      "Training loss: 0.348290354013443\n",
      "Training loss: 0.3371422588825226\n",
      "Training loss: 0.30133843421936035\n",
      "Training loss: 0.3187735080718994\n",
      "Training loss: 0.32004958391189575\n",
      "Training loss: 0.40786492824554443\n",
      "Training loss: 0.35550907254219055\n",
      "Training loss: 0.43009164929389954\n",
      "Training loss: 0.3058111369609833\n",
      "Training loss: 0.26067137718200684\n",
      "Training loss: 0.29883432388305664\n",
      "Training loss: 0.31402453780174255\n",
      "Training loss: 0.2744137942790985\n",
      "Training loss: 0.280387282371521\n",
      "Training loss: 0.2722213566303253\n",
      "Training loss: 0.2610710561275482\n",
      "Training loss: 0.24361461400985718\n",
      "Training loss: 0.19506877660751343\n",
      "Training loss: 0.3135290741920471\n",
      "Training loss: 0.32070451974868774\n",
      "Training loss: 0.3188084661960602\n",
      "Training loss: 0.3974827527999878\n",
      "Training loss: 0.29306530952453613\n",
      "Training loss: 0.2907002866268158\n",
      "Training loss: 0.3576734960079193\n",
      "Training loss: 0.2549944519996643\n",
      "Training loss: 0.212990403175354\n",
      "Training loss: 0.24170663952827454\n",
      "Training loss: 0.3020489513874054\n",
      "Training loss: 0.18470102548599243\n",
      "Training loss: 0.33769285678863525\n",
      "Training loss: 0.28214383125305176\n",
      "Training loss: 0.3065919876098633\n",
      "Training loss: 0.31439438462257385\n",
      "Training loss: 0.3595685064792633\n",
      "Training loss: 0.31686943769454956\n",
      "Training loss: 0.2742336094379425\n",
      "Training loss: 0.32433396577835083\n",
      "Training loss: 0.29164522886276245\n",
      "Training loss: 0.2849479615688324\n",
      "Training loss: 0.33363282680511475\n",
      "Training loss: 0.32318446040153503\n",
      "Training loss: 0.19847968220710754\n",
      "Training loss: 0.41922900080680847\n",
      "Training loss: 0.3434201180934906\n",
      "Training loss: 0.2776888310909271\n",
      "Training loss: 0.405561625957489\n",
      "Training loss: 0.27344146370887756\n",
      "Training loss: 0.2573103904724121\n",
      "Training loss: 0.34613701701164246\n",
      "Training loss: 0.2905083894729614\n",
      "Training loss: 0.4328794777393341\n",
      "Training loss: 0.318034827709198\n",
      "Training loss: 0.2774377167224884\n",
      "Training loss: 0.35799020528793335\n",
      "Training loss: 0.3483617603778839\n",
      "Training loss: 0.3371184468269348\n",
      "Training loss: 0.30131393671035767\n",
      "Training loss: 0.31877797842025757\n",
      "Training loss: 0.31999894976615906\n",
      "Training loss: 0.40817075967788696\n",
      "Training loss: 0.35560527443885803\n",
      "Training loss: 0.4302908182144165\n",
      "Training loss: 0.3058655858039856\n",
      "Training loss: 0.26055604219436646\n",
      "Training loss: 0.2987174987792969\n",
      "Training loss: 0.3140173554420471\n",
      "Training loss: 0.27437299489974976\n",
      "Training loss: 0.28038689494132996\n",
      "Training loss: 0.27210476994514465\n",
      "Training loss: 0.2609594166278839\n",
      "Training loss: 0.2434922754764557\n",
      "Training loss: 0.19473522901535034\n",
      "Training loss: 0.31354042887687683\n",
      "Training loss: 0.32076215744018555\n",
      "Training loss: 0.3187140226364136\n",
      "Training loss: 0.3975971043109894\n",
      "Training loss: 0.29294532537460327\n",
      "Training loss: 0.29063284397125244\n",
      "Training loss: 0.3577902019023895\n",
      "Training loss: 0.2548760771751404\n",
      "Training loss: 0.21272197365760803\n",
      "Training loss: 0.2414597123861313\n",
      "Training loss: 0.3019829988479614\n",
      "Training loss: 0.18438716232776642\n",
      "Training loss: 0.3377941846847534\n",
      "Training loss: 0.2820679843425751\n",
      "Training loss: 0.3065069317817688\n",
      "Training loss: 0.31444036960601807\n",
      "Training loss: 0.3596267104148865\n",
      "Training loss: 0.3168983459472656\n",
      "Training loss: 0.2741365134716034\n",
      "Training loss: 0.3243194818496704\n",
      "Training loss: 0.2915783226490021\n",
      "Training loss: 0.284872442483902\n",
      "Training loss: 0.3336152732372284\n",
      "Training loss: 0.3232315480709076\n",
      "Training loss: 0.1982685625553131\n",
      "Training loss: 0.4194287955760956\n",
      "Training loss: 0.34354931116104126\n",
      "Training loss: 0.27765345573425293\n",
      "Training loss: 0.40574023127555847\n",
      "Training loss: 0.2732679843902588\n",
      "Training loss: 0.2571881115436554\n",
      "Training loss: 0.346257746219635\n",
      "Training loss: 0.29039880633354187\n",
      "Training loss: 0.43318504095077515\n",
      "Training loss: 0.3180990517139435\n",
      "Training loss: 0.277275949716568\n",
      "Training loss: 0.35813355445861816\n",
      "Training loss: 0.34842774271965027\n",
      "Training loss: 0.3370988368988037\n",
      "Training loss: 0.30129435658454895\n",
      "Training loss: 0.3187837600708008\n",
      "Training loss: 0.31995466351509094\n",
      "Training loss: 0.4084455370903015\n",
      "Training loss: 0.35569244623184204\n",
      "Training loss: 0.4304704964160919\n",
      "Training loss: 0.30591583251953125\n",
      "Training loss: 0.2604542374610901\n",
      "Training loss: 0.2986145317554474\n",
      "Training loss: 0.31401264667510986\n",
      "Training loss: 0.2743383049964905\n",
      "Training loss: 0.28038936853408813\n",
      "Training loss: 0.2720036506652832\n",
      "Training loss: 0.2608627676963806\n",
      "Training loss: 0.24338634312152863\n",
      "Training loss: 0.19443827867507935\n",
      "Training loss: 0.3135513961315155\n",
      "Training loss: 0.3208164572715759\n",
      "Training loss: 0.3186304569244385\n",
      "Training loss: 0.39770013093948364\n",
      "Training loss: 0.29284030199050903\n",
      "Training loss: 0.29057446122169495\n",
      "Training loss: 0.35789623856544495\n",
      "Training loss: 0.2547721564769745\n",
      "Training loss: 0.21248409152030945\n",
      "Training loss: 0.2412409633398056\n",
      "Training loss: 0.3019258975982666\n",
      "Training loss: 0.18410830199718475\n",
      "Training loss: 0.3378860354423523\n",
      "Training loss: 0.28200146555900574\n",
      "Training loss: 0.3064330816268921\n",
      "Training loss: 0.3144832253456116\n",
      "Training loss: 0.35968032479286194\n",
      "Training loss: 0.31692594289779663\n",
      "Training loss: 0.27405160665512085\n",
      "Training loss: 0.3243074119091034\n",
      "Training loss: 0.29151982069015503\n",
      "Training loss: 0.28480640053749084\n",
      "Training loss: 0.33360040187835693\n",
      "Training loss: 0.3232755661010742\n",
      "Training loss: 0.19808128476142883\n",
      "Training loss: 0.4196094274520874\n",
      "Training loss: 0.34366655349731445\n",
      "Training loss: 0.2776232659816742\n",
      "Training loss: 0.40590113401412964\n",
      "Training loss: 0.2731143534183502\n",
      "Training loss: 0.2570808529853821\n",
      "Training loss: 0.3463672995567322\n",
      "Training loss: 0.2903023958206177\n",
      "Training loss: 0.4334604740142822\n",
      "Training loss: 0.31815803050994873\n",
      "Training loss: 0.27713316679000854\n",
      "Training loss: 0.3582635521888733\n",
      "Training loss: 0.34848830103874207\n",
      "Training loss: 0.33708280324935913\n",
      "Training loss: 0.30127859115600586\n",
      "Training loss: 0.3187905550003052\n",
      "Training loss: 0.3199160695075989\n",
      "Training loss: 0.4086923599243164\n",
      "Training loss: 0.35577139258384705\n",
      "Training loss: 0.4306320548057556\n",
      "Training loss: 0.3059619963169098\n",
      "Training loss: 0.2603643834590912\n",
      "Training loss: 0.29852375388145447\n",
      "Training loss: 0.31401029229164124\n",
      "Training loss: 0.27430883049964905\n",
      "Training loss: 0.28039389848709106\n",
      "Training loss: 0.2719159424304962\n",
      "Training loss: 0.2607790231704712\n",
      "Training loss: 0.24329471588134766\n",
      "Training loss: 0.19417399168014526\n",
      "Training loss: 0.31356173753738403\n",
      "Training loss: 0.3208672106266022\n",
      "Training loss: 0.3185564875602722\n",
      "Training loss: 0.397792786359787\n",
      "Training loss: 0.29274827241897583\n",
      "Training loss: 0.290523886680603\n",
      "Training loss: 0.35799241065979004\n",
      "Training loss: 0.25468093156814575\n",
      "Training loss: 0.21227332949638367\n",
      "Training loss: 0.24104727804660797\n",
      "Training loss: 0.301876425743103\n",
      "Training loss: 0.1838608682155609\n",
      "Training loss: 0.337969034910202\n",
      "Training loss: 0.28194350004196167\n",
      "Training loss: 0.30636870861053467\n",
      "Training loss: 0.3145228624343872\n",
      "Training loss: 0.35972973704338074\n",
      "Training loss: 0.31695201992988586\n",
      "Training loss: 0.2739771008491516\n",
      "Training loss: 0.3242975175380707\n",
      "Training loss: 0.2914687991142273\n",
      "Training loss: 0.2847486436367035\n",
      "Training loss: 0.33358803391456604\n",
      "Training loss: 0.32331621646881104\n",
      "Training loss: 0.19791515171527863\n",
      "Training loss: 0.41977229714393616\n",
      "Training loss: 0.3437725901603699\n",
      "Training loss: 0.27759721875190735\n",
      "Training loss: 0.40604594349861145\n",
      "Training loss: 0.27297845482826233\n",
      "Training loss: 0.25698623061180115\n",
      "Training loss: 0.34646642208099365\n",
      "Training loss: 0.2902173697948456\n",
      "Training loss: 0.43370798230171204\n",
      "Training loss: 0.31821221113204956\n",
      "Training loss: 0.27700698375701904\n",
      "Training loss: 0.3583809435367584\n",
      "Training loss: 0.348543643951416\n",
      "Training loss: 0.33706989884376526\n",
      "Training loss: 0.30126598477363586\n",
      "Training loss: 0.3187977075576782\n",
      "Training loss: 0.319882333278656\n",
      "Training loss: 0.4089135527610779\n",
      "Training loss: 0.35584285855293274\n",
      "Training loss: 0.4307772219181061\n",
      "Training loss: 0.30600419640541077\n",
      "Training loss: 0.2602851092815399\n",
      "Training loss: 0.29844382405281067\n",
      "Training loss: 0.314008891582489\n",
      "Training loss: 0.27428364753723145\n",
      "Training loss: 0.2803998291492462\n",
      "Training loss: 0.27183985710144043\n",
      "Training loss: 0.26070651412010193\n",
      "Training loss: 0.24321547150611877\n",
      "Training loss: 0.19393914937973022\n",
      "Training loss: 0.31357133388519287\n",
      "Training loss: 0.32091426849365234\n",
      "Training loss: 0.31849101185798645\n",
      "Training loss: 0.3978758156299591\n",
      "Training loss: 0.2926676571369171\n",
      "Training loss: 0.29048001766204834\n",
      "Training loss: 0.3580794334411621\n",
      "Training loss: 0.2546009421348572\n",
      "Training loss: 0.2120867520570755\n",
      "Training loss: 0.24087581038475037\n",
      "Training loss: 0.3018333911895752\n",
      "Training loss: 0.18364129960536957\n",
      "Training loss: 0.33804377913475037\n",
      "Training loss: 0.2818928360939026\n",
      "Training loss: 0.306312620639801\n",
      "Training loss: 0.31455937027931213\n",
      "Training loss: 0.35977500677108765\n",
      "Training loss: 0.31697651743888855\n",
      "Training loss: 0.2739117443561554\n",
      "Training loss: 0.32428932189941406\n",
      "Training loss: 0.29142412543296814\n",
      "Training loss: 0.2846980690956116\n",
      "Training loss: 0.3335775136947632\n",
      "Training loss: 0.3233535885810852\n",
      "Training loss: 0.19776779413223267\n",
      "Training loss: 0.41991904377937317\n",
      "Training loss: 0.3438684642314911\n",
      "Training loss: 0.2775747776031494\n",
      "Training loss: 0.40617573261260986\n",
      "Training loss: 0.2728581428527832\n",
      "Training loss: 0.25690314173698425\n",
      "Training loss: 0.3465559482574463\n",
      "Training loss: 0.2901424467563629\n",
      "Training loss: 0.4339306950569153\n",
      "Training loss: 0.31826165318489075\n",
      "Training loss: 0.2768956422805786\n",
      "Training loss: 0.35848677158355713\n",
      "Training loss: 0.348594069480896\n",
      "Training loss: 0.3370591700077057\n",
      "Training loss: 0.3012559413909912\n",
      "Training loss: 0.3188050389289856\n",
      "Training loss: 0.31985288858413696\n",
      "Training loss: 0.4091113209724426\n",
      "Training loss: 0.3559068441390991\n",
      "Training loss: 0.43090736865997314\n",
      "Training loss: 0.3060424029827118\n",
      "Training loss: 0.2602149248123169\n",
      "Training loss: 0.298373281955719\n",
      "Training loss: 0.3140083849430084\n",
      "Training loss: 0.2742622494697571\n",
      "Training loss: 0.2804068922996521\n",
      "Training loss: 0.2717741131782532\n",
      "Training loss: 0.26064401865005493\n",
      "Training loss: 0.2431471347808838\n",
      "Training loss: 0.19373047351837158\n",
      "Training loss: 0.31357988715171814\n",
      "Training loss: 0.32095810770988464\n",
      "Training loss: 0.3184328079223633\n",
      "Training loss: 0.3979499936103821\n",
      "Training loss: 0.2925972640514374\n",
      "Training loss: 0.29044222831726074\n",
      "Training loss: 0.3581576943397522\n",
      "Training loss: 0.25453054904937744\n",
      "Training loss: 0.21192172169685364\n",
      "Training loss: 0.24072423577308655\n",
      "Training loss: 0.3017961084842682\n",
      "Training loss: 0.1834467351436615\n",
      "Training loss: 0.3381110727787018\n",
      "Training loss: 0.2818485200405121\n",
      "Training loss: 0.30626362562179565\n",
      "Training loss: 0.31459277868270874\n",
      "Training loss: 0.359816312789917\n",
      "Training loss: 0.31699931621551514\n",
      "Training loss: 0.2738542854785919\n",
      "Training loss: 0.3242824375629425\n",
      "Training loss: 0.2913852632045746\n",
      "Training loss: 0.2846536636352539\n",
      "Training loss: 0.333568811416626\n",
      "Training loss: 0.3233875334262848\n",
      "Training loss: 0.19763709604740143\n",
      "Training loss: 0.4200509488582611\n",
      "Training loss: 0.3439546227455139\n",
      "Training loss: 0.2775551676750183\n",
      "Training loss: 0.4062923192977905\n",
      "Training loss: 0.27275192737579346\n",
      "Training loss: 0.2568298280239105\n",
      "Training loss: 0.34663650393486023\n",
      "Training loss: 0.2900763154029846\n",
      "Training loss: 0.4341309368610382\n",
      "Training loss: 0.31830665469169617\n",
      "Training loss: 0.27679696679115295\n",
      "Training loss: 0.35858219861984253\n",
      "Training loss: 0.3486397862434387\n",
      "Training loss: 0.3370506763458252\n",
      "Training loss: 0.30124783515930176\n",
      "Training loss: 0.31881222128868103\n",
      "Training loss: 0.319827139377594\n",
      "Training loss: 0.4092883765697479\n",
      "Training loss: 0.3559645414352417\n",
      "Training loss: 0.4310237765312195\n",
      "Training loss: 0.3060770034790039\n",
      "Training loss: 0.26015302538871765\n",
      "Training loss: 0.29831117391586304\n",
      "Training loss: 0.31400853395462036\n",
      "Training loss: 0.2742439806461334\n",
      "Training loss: 0.2804144620895386\n",
      "Training loss: 0.2717170715332031\n",
      "Training loss: 0.2605900764465332\n",
      "Training loss: 0.24308808147907257\n",
      "Training loss: 0.19354543089866638\n",
      "Training loss: 0.3135877251625061\n",
      "Training loss: 0.32099851965904236\n",
      "Training loss: 0.31838130950927734\n",
      "Training loss: 0.39801621437072754\n",
      "Training loss: 0.29253557324409485\n",
      "Training loss: 0.29040950536727905\n",
      "Training loss: 0.35822793841362\n",
      "Training loss: 0.2544686794281006\n",
      "Training loss: 0.2117757797241211\n",
      "Training loss: 0.24059022963047028\n",
      "Training loss: 0.301763653755188\n",
      "Training loss: 0.1832743138074875\n",
      "Training loss: 0.3381713628768921\n",
      "Training loss: 0.28180980682373047\n",
      "Training loss: 0.3062207102775574\n",
      "Training loss: 0.31462329626083374\n",
      "Training loss: 0.35985395312309265\n",
      "Training loss: 0.3170204162597656\n",
      "Training loss: 0.2738037407398224\n",
      "Training loss: 0.3242766559123993\n",
      "Training loss: 0.29135122895240784\n",
      "Training loss: 0.28461483120918274\n",
      "Training loss: 0.33356142044067383\n",
      "Training loss: 0.3234182298183441\n",
      "Training loss: 0.1975211501121521\n",
      "Training loss: 0.420169472694397\n",
      "Training loss: 0.3440321087837219\n",
      "Training loss: 0.2775380313396454\n",
      "Training loss: 0.4063965976238251\n",
      "Training loss: 0.27265802025794983\n",
      "Training loss: 0.2567654252052307\n",
      "Training loss: 0.3467089831829071\n",
      "Training loss: 0.29001784324645996\n",
      "Training loss: 0.4343107342720032\n",
      "Training loss: 0.31834742426872253\n",
      "Training loss: 0.27670973539352417\n",
      "Training loss: 0.35866785049438477\n",
      "Training loss: 0.3486810624599457\n",
      "Training loss: 0.33704373240470886\n",
      "Training loss: 0.30124130845069885\n",
      "Training loss: 0.31881919503211975\n",
      "Training loss: 0.3198045790195465\n",
      "Training loss: 0.409446120262146\n",
      "Training loss: 0.35601603984832764\n",
      "Training loss: 0.43112802505493164\n",
      "Training loss: 0.30610811710357666\n",
      "Training loss: 0.26009806990623474\n",
      "Training loss: 0.29825639724731445\n",
      "Training loss: 0.31400924921035767\n",
      "Training loss: 0.2742285132408142\n",
      "Training loss: 0.28042253851890564\n",
      "Training loss: 0.2716679871082306\n",
      "Training loss: 0.2605438828468323\n",
      "Training loss: 0.2430371642112732\n",
      "Training loss: 0.1933811753988266\n",
      "Training loss: 0.3135943114757538\n",
      "Training loss: 0.3210359513759613\n",
      "Training loss: 0.3183353543281555\n",
      "Training loss: 0.3980749845504761\n",
      "Training loss: 0.29248178005218506\n",
      "Training loss: 0.2903812825679779\n",
      "Training loss: 0.35829129815101624\n",
      "Training loss: 0.25441429018974304\n",
      "Training loss: 0.2116468846797943\n",
      "Training loss: 0.24047182500362396\n",
      "Training loss: 0.30173546075820923\n",
      "Training loss: 0.18312163650989532\n",
      "Training loss: 0.33822542428970337\n",
      "Training loss: 0.28177589178085327\n",
      "Training loss: 0.30618298053741455\n",
      "Training loss: 0.3146510124206543\n",
      "Training loss: 0.35988831520080566\n",
      "Training loss: 0.31703993678092957\n",
      "Training loss: 0.2737591564655304\n",
      "Training loss: 0.3242720365524292\n",
      "Training loss: 0.29132139682769775\n",
      "Training loss: 0.28458061814308167\n",
      "Training loss: 0.3335552215576172\n",
      "Training loss: 0.3234458267688751\n",
      "Training loss: 0.19741839170455933\n",
      "Training loss: 0.42027583718299866\n",
      "Training loss: 0.3441016674041748\n",
      "Training loss: 0.2775229215621948\n",
      "Training loss: 0.40648987889289856\n",
      "Training loss: 0.2725752592086792\n",
      "Training loss: 0.2567083537578583\n",
      "Training loss: 0.3467738628387451\n",
      "Training loss: 0.2899661660194397\n",
      "Training loss: 0.43447190523147583\n",
      "Training loss: 0.31838443875312805\n",
      "Training loss: 0.2766324281692505\n",
      "Training loss: 0.3587445914745331\n",
      "Training loss: 0.3487183451652527\n",
      "Training loss: 0.33703821897506714\n",
      "Training loss: 0.30123603343963623\n",
      "Training loss: 0.3188258707523346\n",
      "Training loss: 0.3197849690914154\n",
      "Training loss: 0.4095868468284607\n",
      "Training loss: 0.3560621738433838\n",
      "Training loss: 0.4312209188938141\n",
      "Training loss: 0.3061359226703644\n",
      "Training loss: 0.26004964113235474\n",
      "Training loss: 0.2982081174850464\n",
      "Training loss: 0.3140100836753845\n",
      "Training loss: 0.2742152214050293\n",
      "Training loss: 0.28043073415756226\n",
      "Training loss: 0.27162566781044006\n",
      "Training loss: 0.26050424575805664\n",
      "Training loss: 0.24299350380897522\n",
      "Training loss: 0.19323575496673584\n",
      "Training loss: 0.313600093126297\n",
      "Training loss: 0.3210704028606415\n",
      "Training loss: 0.31829461455345154\n",
      "Training loss: 0.3981272578239441\n",
      "Training loss: 0.29243484139442444\n",
      "Training loss: 0.2903570234775543\n",
      "Training loss: 0.35834792256355286\n",
      "Training loss: 0.2543664276599884\n",
      "Training loss: 0.2115330696105957\n",
      "Training loss: 0.2403673529624939\n",
      "Training loss: 0.30171090364456177\n",
      "Training loss: 0.18298660218715668\n",
      "Training loss: 0.33827367424964905\n",
      "Training loss: 0.281746506690979\n",
      "Training loss: 0.3061499297618866\n",
      "Training loss: 0.3146761655807495\n",
      "Training loss: 0.35991960763931274\n",
      "Training loss: 0.31705793738365173\n",
      "Training loss: 0.27371978759765625\n",
      "Training loss: 0.32426807284355164\n",
      "Training loss: 0.2912954092025757\n",
      "Training loss: 0.2845507264137268\n",
      "Training loss: 0.3335501551628113\n",
      "Training loss: 0.3234702944755554\n",
      "Training loss: 0.19732719659805298\n",
      "Training loss: 0.42037126421928406\n",
      "Training loss: 0.3441637456417084\n",
      "Training loss: 0.2775093615055084\n",
      "Training loss: 0.4065731465816498\n",
      "Training loss: 0.2725023031234741\n",
      "Training loss: 0.25665798783302307\n",
      "Training loss: 0.3468320369720459\n",
      "Training loss: 0.2899203598499298\n",
      "Training loss: 0.43461644649505615\n",
      "Training loss: 0.3184177875518799\n",
      "Training loss: 0.2765638828277588\n",
      "Training loss: 0.35881340503692627\n",
      "Training loss: 0.34875184297561646\n",
      "Training loss: 0.3370339274406433\n",
      "Training loss: 0.3012318015098572\n",
      "Training loss: 0.31883203983306885\n",
      "Training loss: 0.31976792216300964\n",
      "Training loss: 0.4097123146057129\n",
      "Training loss: 0.35610339045524597\n",
      "Training loss: 0.43130379915237427\n",
      "Training loss: 0.3061607778072357\n",
      "Training loss: 0.26000678539276123\n",
      "Training loss: 0.29816558957099915\n",
      "Training loss: 0.3140108585357666\n",
      "Training loss: 0.27420398592948914\n",
      "Training loss: 0.28043895959854126\n",
      "Training loss: 0.2715892195701599\n",
      "Training loss: 0.2604703903198242\n",
      "Training loss: 0.24295569956302643\n",
      "Training loss: 0.1931069940328598\n",
      "Training loss: 0.3136048913002014\n",
      "Training loss: 0.3211021423339844\n",
      "Training loss: 0.31825828552246094\n",
      "Training loss: 0.3981734812259674\n",
      "Training loss: 0.29239389300346375\n",
      "Training loss: 0.2903361916542053\n",
      "Training loss: 0.3583984971046448\n",
      "Training loss: 0.2543242573738098\n",
      "Training loss: 0.21143263578414917\n",
      "Training loss: 0.24027514457702637\n",
      "Training loss: 0.301689475774765\n",
      "Training loss: 0.18286706507205963\n",
      "Training loss: 0.3383168876171112\n",
      "Training loss: 0.28172072768211365\n",
      "Training loss: 0.30612069368362427\n",
      "Training loss: 0.3146989047527313\n",
      "Training loss: 0.35994797945022583\n",
      "Training loss: 0.3170745074748993\n",
      "Training loss: 0.2736849784851074\n",
      "Training loss: 0.3242647051811218\n",
      "Training loss: 0.29127246141433716\n",
      "Training loss: 0.28452444076538086\n",
      "Training loss: 0.3335459232330322\n",
      "Training loss: 0.3234921991825104\n",
      "Training loss: 0.19724631309509277\n",
      "Training loss: 0.4204567074775696\n",
      "Training loss: 0.3442194163799286\n",
      "Training loss: 0.2774972915649414\n",
      "Training loss: 0.40664753317832947\n",
      "Training loss: 0.2724379003047943\n",
      "Training loss: 0.2566135823726654\n",
      "Training loss: 0.3468840718269348\n",
      "Training loss: 0.289879709482193\n",
      "Training loss: 0.4347466230392456\n",
      "Training loss: 0.31844764947891235\n",
      "Training loss: 0.2765030264854431\n",
      "Training loss: 0.358875036239624\n",
      "Training loss: 0.34878191351890564\n",
      "Training loss: 0.33703047037124634\n",
      "Training loss: 0.30122819542884827\n",
      "Training loss: 0.31883782148361206\n",
      "Training loss: 0.31975311040878296\n",
      "Training loss: 0.409824013710022\n",
      "Training loss: 0.3561401665210724\n",
      "Training loss: 0.4313777685165405\n",
      "Training loss: 0.3061828315258026\n",
      "Training loss: 0.25996866822242737\n",
      "Training loss: 0.2981281280517578\n",
      "Training loss: 0.3140113949775696\n",
      "Training loss: 0.2741944193840027\n",
      "Training loss: 0.2804470956325531\n",
      "Training loss: 0.27155789732933044\n",
      "Training loss: 0.26044169068336487\n",
      "Training loss: 0.24292334914207458\n",
      "Training loss: 0.1929929405450821\n",
      "Training loss: 0.313608855009079\n",
      "Training loss: 0.3211314380168915\n",
      "Training loss: 0.3182259500026703\n",
      "Training loss: 0.39821434020996094\n",
      "Training loss: 0.2923581898212433\n",
      "Training loss: 0.2903183102607727\n",
      "Training loss: 0.35844361782073975\n",
      "Training loss: 0.25428709387779236\n",
      "Training loss: 0.21134395897388458\n",
      "Training loss: 0.240193709731102\n",
      "Training loss: 0.30167073011398315\n",
      "Training loss: 0.1827612817287445\n",
      "Training loss: 0.33835548162460327\n",
      "Training loss: 0.2816983461380005\n",
      "Training loss: 0.306094765663147\n",
      "Training loss: 0.314719557762146\n",
      "Training loss: 0.35997387766838074\n",
      "Training loss: 0.31708964705467224\n",
      "Training loss: 0.273654043674469\n",
      "Training loss: 0.3242619037628174\n",
      "Training loss: 0.29125261306762695\n",
      "Training loss: 0.284501314163208\n",
      "Training loss: 0.3335425853729248\n",
      "Training loss: 0.32351136207580566\n",
      "Training loss: 0.19717447459697723\n",
      "Training loss: 0.4205334484577179\n",
      "Training loss: 0.3442690074443817\n",
      "Training loss: 0.2774864137172699\n",
      "Training loss: 0.4067140221595764\n",
      "Training loss: 0.2723812460899353\n",
      "Training loss: 0.2565739154815674\n",
      "Training loss: 0.3469306528568268\n",
      "Training loss: 0.2898435890674591\n",
      "Training loss: 0.43486344814300537\n",
      "Training loss: 0.31847473978996277\n",
      "Training loss: 0.2764488458633423\n",
      "Training loss: 0.3589302599430084\n",
      "Training loss: 0.3488088548183441\n",
      "Training loss: 0.33702796697616577\n",
      "Training loss: 0.3012252449989319\n",
      "Training loss: 0.31884321570396423\n",
      "Training loss: 0.319740355014801\n",
      "Training loss: 0.40992361307144165\n",
      "Training loss: 0.3561728000640869\n",
      "Training loss: 0.4314436912536621\n",
      "Training loss: 0.30620235204696655\n",
      "Training loss: 0.25993502140045166\n",
      "Training loss: 0.2980950176715851\n",
      "Training loss: 0.3140120804309845\n",
      "Training loss: 0.27418631315231323\n",
      "Training loss: 0.2804550528526306\n",
      "Training loss: 0.2715311050415039\n",
      "Training loss: 0.26041731238365173\n",
      "Training loss: 0.24289576709270477\n",
      "Training loss: 0.19289201498031616\n",
      "Training loss: 0.3136120140552521\n",
      "Training loss: 0.3211584985256195\n",
      "Training loss: 0.31819698214530945\n",
      "Training loss: 0.39825040102005005\n",
      "Training loss: 0.29232722520828247\n",
      "Training loss: 0.290303111076355\n",
      "Training loss: 0.35848429799079895\n",
      "Training loss: 0.25425422191619873\n",
      "Training loss: 0.21126583218574524\n",
      "Training loss: 0.24012194573879242\n",
      "Training loss: 0.3016544580459595\n",
      "Training loss: 0.1826678216457367\n",
      "Training loss: 0.3383897840976715\n",
      "Training loss: 0.28167879581451416\n",
      "Training loss: 0.30607184767723083\n",
      "Training loss: 0.31473806500434875\n",
      "Training loss: 0.35999730229377747\n",
      "Training loss: 0.31710347533226013\n",
      "Training loss: 0.2736266255378723\n",
      "Training loss: 0.3242596387863159\n",
      "Training loss: 0.29123514890670776\n",
      "Training loss: 0.2844809889793396\n",
      "Training loss: 0.33353984355926514\n",
      "Training loss: 0.3235282003879547\n",
      "Training loss: 0.19711069762706757\n",
      "Training loss: 0.42060205340385437\n",
      "Training loss: 0.34431320428848267\n",
      "Training loss: 0.2774764895439148\n",
      "Training loss: 0.406773179769516\n",
      "Training loss: 0.2723313570022583\n",
      "Training loss: 0.25653886795043945\n",
      "Training loss: 0.3469722270965576\n",
      "Training loss: 0.28981152176856995\n",
      "Training loss: 0.43496838212013245\n",
      "Training loss: 0.31849899888038635\n",
      "Training loss: 0.27640053629875183\n",
      "Training loss: 0.35897958278656006\n",
      "Training loss: 0.3488328754901886\n",
      "Training loss: 0.3370261788368225\n",
      "Training loss: 0.3012227416038513\n",
      "Training loss: 0.3188481330871582\n",
      "Training loss: 0.3197292685508728\n",
      "Training loss: 0.4100121855735779\n",
      "Training loss: 0.35620203614234924\n",
      "Training loss: 0.43150240182876587\n",
      "Training loss: 0.3062196671962738\n",
      "Training loss: 0.25990521907806396\n",
      "Training loss: 0.29806599020957947\n",
      "Training loss: 0.3140127658843994\n",
      "Training loss: 0.27417951822280884\n",
      "Training loss: 0.28046274185180664\n",
      "Training loss: 0.2715081572532654\n",
      "Training loss: 0.2603967785835266\n",
      "Training loss: 0.24287204444408417\n",
      "Training loss: 0.19280283153057098\n",
      "Training loss: 0.31361445784568787\n",
      "Training loss: 0.32118335366249084\n",
      "Training loss: 0.3181712329387665\n",
      "Training loss: 0.39828208088874817\n",
      "Training loss: 0.292300283908844\n",
      "Training loss: 0.2902902066707611\n",
      "Training loss: 0.3585200607776642\n",
      "Training loss: 0.2542252540588379\n",
      "Training loss: 0.2111968994140625\n",
      "Training loss: 0.24005866050720215\n",
      "Training loss: 0.30164021253585815\n",
      "Training loss: 0.18258509039878845\n",
      "Training loss: 0.33842042088508606\n",
      "Training loss: 0.2816618084907532\n",
      "Training loss: 0.3060513734817505\n",
      "Training loss: 0.31475481390953064\n",
      "Training loss: 0.36001884937286377\n",
      "Training loss: 0.31711626052856445\n",
      "Training loss: 0.27360209822654724\n",
      "Training loss: 0.3242577612400055\n",
      "Training loss: 0.2912199795246124\n",
      "Training loss: 0.28446313738822937\n",
      "Training loss: 0.333537757396698\n",
      "Training loss: 0.32354265451431274\n",
      "Training loss: 0.19705399870872498\n",
      "Training loss: 0.4206637442111969\n",
      "Training loss: 0.3443526029586792\n",
      "Training loss: 0.27746737003326416\n",
      "Training loss: 0.4068260192871094\n",
      "Training loss: 0.2722874581813812\n",
      "Training loss: 0.25650787353515625\n",
      "Training loss: 0.3470093011856079\n",
      "Training loss: 0.28978291153907776\n",
      "Training loss: 0.43506282567977905\n",
      "Training loss: 0.31852075457572937\n",
      "Training loss: 0.2763575613498688\n",
      "Training loss: 0.3590236008167267\n",
      "Training loss: 0.3488544225692749\n",
      "Training loss: 0.33702486753463745\n",
      "Training loss: 0.3012206554412842\n",
      "Training loss: 0.31885260343551636\n",
      "Training loss: 0.31971973180770874\n",
      "Training loss: 0.4100910425186157\n",
      "Training loss: 0.3562279939651489\n",
      "Training loss: 0.4315545856952667\n",
      "Training loss: 0.30623483657836914\n",
      "Training loss: 0.2598786950111389\n",
      "Training loss: 0.29804033041000366\n",
      "Training loss: 0.31401291489601135\n",
      "Training loss: 0.274173766374588\n",
      "Training loss: 0.28047025203704834\n",
      "Training loss: 0.2714887261390686\n",
      "Training loss: 0.2603796720504761\n",
      "Training loss: 0.24285206198692322\n",
      "Training loss: 0.19272392988204956\n",
      "Training loss: 0.31361618638038635\n",
      "Training loss: 0.32120633125305176\n",
      "Training loss: 0.31814810633659363\n",
      "Training loss: 0.3983100652694702\n",
      "Training loss: 0.29227691888809204\n",
      "Training loss: 0.29027920961380005\n",
      "Training loss: 0.35855215787887573\n",
      "Training loss: 0.25419971346855164\n",
      "Training loss: 0.21113614737987518\n",
      "Training loss: 0.2400028556585312\n",
      "Training loss: 0.3016277551651001\n",
      "Training loss: 0.18251203000545502\n",
      "Training loss: 0.3384476900100708\n",
      "Training loss: 0.28164708614349365\n",
      "Training loss: 0.3060332238674164\n",
      "Training loss: 0.3147698938846588\n",
      "Training loss: 0.36003822088241577\n",
      "Training loss: 0.31712788343429565\n",
      "Training loss: 0.27358031272888184\n",
      "Training loss: 0.32425621151924133\n",
      "Training loss: 0.2912066876888275\n",
      "Training loss: 0.28444743156433105\n",
      "Training loss: 0.3335360288619995\n",
      "Training loss: 0.3235553801059723\n",
      "Training loss: 0.197003573179245\n",
      "Training loss: 0.4207189977169037\n",
      "Training loss: 0.34438788890838623\n",
      "Training loss: 0.2774590849876404\n",
      "Training loss: 0.4068731963634491\n",
      "Training loss: 0.272248774766922\n",
      "Training loss: 0.2564801275730133\n",
      "Training loss: 0.3470425009727478\n",
      "Training loss: 0.28975749015808105\n",
      "Training loss: 0.4351477324962616\n",
      "Training loss: 0.31854045391082764\n",
      "Training loss: 0.2763192653656006\n",
      "Training loss: 0.359063059091568\n",
      "Training loss: 0.34887367486953735\n",
      "Training loss: 0.3370240032672882\n",
      "Training loss: 0.3012188673019409\n",
      "Training loss: 0.3188566565513611\n",
      "Training loss: 0.3197114169597626\n",
      "Training loss: 0.4101611077785492\n",
      "Training loss: 0.35625097155570984\n",
      "Training loss: 0.43160125613212585\n",
      "Training loss: 0.3062482178211212\n",
      "Training loss: 0.25985512137413025\n",
      "Training loss: 0.29801762104034424\n",
      "Training loss: 0.31401297450065613\n",
      "Training loss: 0.2741689085960388\n",
      "Training loss: 0.28047752380371094\n",
      "Training loss: 0.271472305059433\n",
      "Training loss: 0.2603655159473419\n",
      "Training loss: 0.24283522367477417\n",
      "Training loss: 0.19265416264533997\n",
      "Training loss: 0.3136172592639923\n",
      "Training loss: 0.3212275505065918\n",
      "Training loss: 0.3181273639202118\n",
      "Training loss: 0.39833444356918335\n",
      "Training loss: 0.29225674271583557\n",
      "Training loss: 0.29026997089385986\n",
      "Training loss: 0.3585810959339142\n",
      "Training loss: 0.2541770339012146\n",
      "Training loss: 0.21108263731002808\n",
      "Training loss: 0.2399536669254303\n",
      "Training loss: 0.3016168475151062\n",
      "Training loss: 0.18244732916355133\n",
      "Training loss: 0.33847200870513916\n",
      "Training loss: 0.28163430094718933\n",
      "Training loss: 0.3060169816017151\n",
      "Training loss: 0.31478351354599\n",
      "Training loss: 0.36005592346191406\n",
      "Training loss: 0.31713855266571045\n",
      "Training loss: 0.27356085181236267\n",
      "Training loss: 0.3242548704147339\n",
      "Training loss: 0.2911947965621948\n",
      "Training loss: 0.2844335734844208\n",
      "Training loss: 0.33353477716445923\n",
      "Training loss: 0.3235663175582886\n",
      "Training loss: 0.1969587206840515\n",
      "Training loss: 0.4207685589790344\n",
      "Training loss: 0.34441906213760376\n",
      "Training loss: 0.27745136618614197\n",
      "Training loss: 0.4069153070449829\n",
      "Training loss: 0.2722148597240448\n",
      "Training loss: 0.2564556300640106\n",
      "Training loss: 0.3470720648765564\n",
      "Training loss: 0.28973469138145447\n",
      "Training loss: 0.43522441387176514\n",
      "Training loss: 0.3185582160949707\n",
      "Training loss: 0.27628493309020996\n",
      "Training loss: 0.35909828543663025\n",
      "Training loss: 0.3488908112049103\n",
      "Training loss: 0.3370235562324524\n",
      "Training loss: 0.30121731758117676\n",
      "Training loss: 0.3188602924346924\n",
      "Training loss: 0.3197041153907776\n",
      "Training loss: 0.4102235734462738\n",
      "Training loss: 0.3562714755535126\n",
      "Training loss: 0.43164271116256714\n",
      "Training loss: 0.3062599003314972\n",
      "Training loss: 0.2598341703414917\n",
      "Training loss: 0.29799771308898926\n",
      "Training loss: 0.3140132427215576\n",
      "Training loss: 0.2741648554801941\n",
      "Training loss: 0.28048446774482727\n",
      "Training loss: 0.27145838737487793\n",
      "Training loss: 0.26035386323928833\n",
      "Training loss: 0.24282070994377136\n",
      "Training loss: 0.19259242713451385\n",
      "Training loss: 0.31361785531044006\n",
      "Training loss: 0.3212471306324005\n",
      "Training loss: 0.3181087374687195\n",
      "Training loss: 0.3983559310436249\n",
      "Training loss: 0.2922391891479492\n",
      "Training loss: 0.2902621924877167\n",
      "Training loss: 0.35860663652420044\n",
      "Training loss: 0.25415700674057007\n",
      "Training loss: 0.21103540062904358\n",
      "Training loss: 0.23991020023822784\n",
      "Training loss: 0.30160725116729736\n",
      "Training loss: 0.182390034198761\n",
      "Training loss: 0.338493674993515\n",
      "Training loss: 0.2816232442855835\n",
      "Training loss: 0.30600225925445557\n",
      "Training loss: 0.31479573249816895\n",
      "Training loss: 0.36007219552993774\n",
      "Training loss: 0.3171483874320984\n",
      "Training loss: 0.2735433280467987\n",
      "Training loss: 0.32425370812416077\n",
      "Training loss: 0.2911846935749054\n",
      "Training loss: 0.2844213843345642\n",
      "Training loss: 0.33353391289711\n",
      "Training loss: 0.32357555627822876\n",
      "Training loss: 0.19691871106624603\n",
      "Training loss: 0.4208131432533264\n",
      "Training loss: 0.34444695711135864\n",
      "Training loss: 0.27744412422180176\n",
      "Training loss: 0.40695297718048096\n",
      "Training loss: 0.27218499779701233\n",
      "Training loss: 0.256433367729187\n",
      "Training loss: 0.3470984697341919\n",
      "Training loss: 0.28971415758132935\n",
      "Training loss: 0.43529385328292847\n",
      "Training loss: 0.31857407093048096\n",
      "Training loss: 0.2762540578842163\n",
      "Training loss: 0.3591298460960388\n",
      "Training loss: 0.34890618920326233\n",
      "Training loss: 0.33702346682548523\n",
      "Training loss: 0.30121585726737976\n",
      "Training loss: 0.31886357069015503\n",
      "Training loss: 0.3196980953216553\n",
      "Training loss: 0.4102794826030731\n",
      "Training loss: 0.35628968477249146\n",
      "Training loss: 0.4316795766353607\n",
      "Training loss: 0.30627015233039856\n",
      "Training loss: 0.2598155736923218\n",
      "Training loss: 0.2979801297187805\n",
      "Training loss: 0.3140130043029785\n",
      "Training loss: 0.27416151762008667\n",
      "Training loss: 0.2804911136627197\n",
      "Training loss: 0.27144667506217957\n",
      "Training loss: 0.26034435629844666\n",
      "Training loss: 0.24280858039855957\n",
      "Training loss: 0.19253778457641602\n",
      "Training loss: 0.3136180639266968\n",
      "Training loss: 0.321265310049057\n",
      "Training loss: 0.31809210777282715\n",
      "Training loss: 0.39837488532066345\n",
      "Training loss: 0.29222404956817627\n",
      "Training loss: 0.29025572538375854\n",
      "Training loss: 0.3586294651031494\n",
      "Training loss: 0.2541392147541046\n",
      "Training loss: 0.2109937071800232\n",
      "Training loss: 0.23987187445163727\n",
      "Training loss: 0.30159881711006165\n",
      "Training loss: 0.18233934044837952\n",
      "Training loss: 0.338513046503067\n",
      "Training loss: 0.2816137671470642\n",
      "Training loss: 0.30598893761634827\n",
      "Training loss: 0.31480687856674194\n",
      "Training loss: 0.360087126493454\n",
      "Training loss: 0.31715744733810425\n",
      "Training loss: 0.2735275626182556\n",
      "Training loss: 0.3242529332637787\n",
      "Training loss: 0.2911757230758667\n",
      "Training loss: 0.2844105660915375\n",
      "Training loss: 0.3335334360599518\n",
      "Training loss: 0.3235832154750824\n",
      "Training loss: 0.19688290357589722\n",
      "Training loss: 0.42085349559783936\n",
      "Training loss: 0.3444717228412628\n",
      "Training loss: 0.2774372100830078\n",
      "Training loss: 0.40698671340942383\n",
      "Training loss: 0.2721588611602783\n",
      "Training loss: 0.2564135491847992\n",
      "Training loss: 0.3471221327781677\n",
      "Training loss: 0.28969573974609375\n",
      "Training loss: 0.435357004404068\n",
      "Training loss: 0.31858837604522705\n",
      "Training loss: 0.2762262225151062\n",
      "Training loss: 0.35915815830230713\n",
      "Training loss: 0.3489198386669159\n",
      "Training loss: 0.33702385425567627\n",
      "Training loss: 0.3012146055698395\n",
      "Training loss: 0.3188665509223938\n",
      "Training loss: 0.31969279050827026\n",
      "Training loss: 0.41032928228378296\n",
      "Training loss: 0.3563060164451599\n",
      "Training loss: 0.4317125380039215\n",
      "Training loss: 0.30627915263175964\n",
      "Training loss: 0.25979894399642944\n",
      "Training loss: 0.29796451330184937\n",
      "Training loss: 0.31401270627975464\n",
      "Training loss: 0.2741587460041046\n",
      "Training loss: 0.28049740195274353\n",
      "Training loss: 0.271436870098114\n",
      "Training loss: 0.26033666729927063\n",
      "Training loss: 0.24279841780662537\n",
      "Training loss: 0.19248946011066437\n",
      "Training loss: 0.3136177957057953\n",
      "Training loss: 0.3212820589542389\n",
      "Training loss: 0.31807708740234375\n",
      "Training loss: 0.39839139580726624\n",
      "Training loss: 0.29221096634864807\n",
      "Training loss: 0.29025033116340637\n",
      "Training loss: 0.35864970088005066\n",
      "Training loss: 0.25412341952323914\n",
      "Training loss: 0.21095693111419678\n",
      "Training loss: 0.23983797430992126\n",
      "Training loss: 0.3015914261341095\n",
      "Training loss: 0.18229426443576813\n",
      "Training loss: 0.33853036165237427\n",
      "Training loss: 0.28160566091537476\n",
      "Training loss: 0.30597686767578125\n",
      "Training loss: 0.3148168623447418\n",
      "Training loss: 0.3601008355617523\n",
      "Training loss: 0.3171658217906952\n",
      "Training loss: 0.2735132575035095\n",
      "Training loss: 0.3242521286010742\n",
      "Training loss: 0.29116788506507874\n",
      "Training loss: 0.284400999546051\n",
      "Training loss: 0.3335331976413727\n",
      "Training loss: 0.3235897123813629\n",
      "Training loss: 0.19685088098049164\n",
      "Training loss: 0.420889675617218\n",
      "Training loss: 0.3444938659667969\n",
      "Training loss: 0.2774307429790497\n",
      "Training loss: 0.4070168137550354\n",
      "Training loss: 0.27213582396507263\n",
      "Training loss: 0.25639572739601135\n",
      "Training loss: 0.3471432626247406\n",
      "Training loss: 0.2896791696548462\n",
      "Training loss: 0.4354135990142822\n",
      "Training loss: 0.3186010718345642\n",
      "Training loss: 0.27620112895965576\n",
      "Training loss: 0.359183669090271\n",
      "Training loss: 0.34893205761909485\n",
      "Training loss: 0.3370242714881897\n",
      "Training loss: 0.3012133836746216\n",
      "Training loss: 0.3188692033290863\n",
      "Training loss: 0.3196885585784912\n",
      "Training loss: 0.410373717546463\n",
      "Training loss: 0.35632044076919556\n",
      "Training loss: 0.4317420423030853\n",
      "Training loss: 0.3062869906425476\n",
      "Training loss: 0.25978410243988037\n",
      "Training loss: 0.2979508340358734\n",
      "Training loss: 0.3140121102333069\n",
      "Training loss: 0.27415648102760315\n",
      "Training loss: 0.280503511428833\n",
      "Training loss: 0.2714287340641022\n",
      "Training loss: 0.2603307068347931\n",
      "Training loss: 0.2427901178598404\n",
      "Training loss: 0.19244660437107086\n",
      "Training loss: 0.3136172592639923\n",
      "Training loss: 0.3212977647781372\n",
      "Training loss: 0.3180634379386902\n",
      "Training loss: 0.39840587973594666\n",
      "Training loss: 0.2921997308731079\n",
      "Training loss: 0.29024600982666016\n",
      "Training loss: 0.35866814851760864\n",
      "Training loss: 0.25410932302474976\n",
      "Training loss: 0.2109244465827942\n",
      "Training loss: 0.2398080676794052\n",
      "Training loss: 0.30158495903015137\n",
      "Training loss: 0.18225432932376862\n",
      "Training loss: 0.3385457992553711\n",
      "Training loss: 0.2815985679626465\n",
      "Training loss: 0.30596598982810974\n",
      "Training loss: 0.3148258924484253\n",
      "Training loss: 0.36011338233947754\n",
      "Training loss: 0.31717348098754883\n",
      "Training loss: 0.27350038290023804\n",
      "Training loss: 0.3242517113685608\n",
      "Training loss: 0.29116106033325195\n",
      "Training loss: 0.2843925654888153\n",
      "Training loss: 0.33353328704833984\n",
      "Training loss: 0.32359492778778076\n",
      "Training loss: 0.19682221114635468\n",
      "Training loss: 0.42092248797416687\n",
      "Training loss: 0.3445136249065399\n",
      "Training loss: 0.2774246633052826\n",
      "Training loss: 0.4070439040660858\n",
      "Training loss: 0.2721155285835266\n",
      "Training loss: 0.25637975335121155\n",
      "Training loss: 0.3471621870994568\n",
      "Training loss: 0.28966420888900757\n",
      "Training loss: 0.4354659914970398\n",
      "Training loss: 0.31861281394958496\n",
      "Training loss: 0.2761785089969635\n",
      "Training loss: 0.3592064082622528\n",
      "Training loss: 0.34894296526908875\n",
      "Training loss: 0.3370249271392822\n",
      "Training loss: 0.3012123107910156\n",
      "Training loss: 0.3188716173171997\n",
      "Training loss: 0.31968462467193604\n",
      "Training loss: 0.41041356325149536\n",
      "Training loss: 0.356333464384079\n",
      "Training loss: 0.4317684471607208\n",
      "Training loss: 0.3062937259674072\n",
      "Training loss: 0.25977084040641785\n",
      "Training loss: 0.29793864488601685\n",
      "Training loss: 0.3140116035938263\n",
      "Training loss: 0.2741546332836151\n",
      "Training loss: 0.28050926327705383\n",
      "Training loss: 0.2714220881462097\n",
      "Training loss: 0.2603261470794678\n",
      "Training loss: 0.24278301000595093\n",
      "Training loss: 0.19240859150886536\n",
      "Training loss: 0.3136163651943207\n",
      "Training loss: 0.3213123679161072\n",
      "Training loss: 0.31805115938186646\n",
      "Training loss: 0.3984186053276062\n",
      "Training loss: 0.2921900153160095\n",
      "Training loss: 0.290242463350296\n",
      "Training loss: 0.3586842715740204\n",
      "Training loss: 0.25409674644470215\n",
      "Training loss: 0.21089568734169006\n",
      "Training loss: 0.23978157341480255\n",
      "Training loss: 0.30157917737960815\n",
      "Training loss: 0.18221883475780487\n",
      "Training loss: 0.33855968713760376\n",
      "Training loss: 0.2815924882888794\n",
      "Training loss: 0.30595600605010986\n",
      "Training loss: 0.3148341178894043\n",
      "Training loss: 0.3601250946521759\n",
      "Training loss: 0.3171805739402771\n",
      "Training loss: 0.2734886705875397\n",
      "Training loss: 0.32425132393836975\n",
      "Training loss: 0.2911550998687744\n",
      "Training loss: 0.2843851149082184\n",
      "Training loss: 0.33353349566459656\n",
      "Training loss: 0.32359927892684937\n",
      "Training loss: 0.19679641723632812\n",
      "Training loss: 0.4209522306919098\n",
      "Training loss: 0.34453126788139343\n",
      "Training loss: 0.2774188220500946\n",
      "Training loss: 0.40706828236579895\n",
      "Training loss: 0.2720976769924164\n",
      "Training loss: 0.2563653290271759\n",
      "Training loss: 0.34717923402786255\n",
      "Training loss: 0.2896506190299988\n",
      "Training loss: 0.43551337718963623\n",
      "Training loss: 0.3186233341693878\n",
      "Training loss: 0.276157945394516\n",
      "Training loss: 0.35922694206237793\n",
      "Training loss: 0.3489527702331543\n",
      "Training loss: 0.3370257616043091\n",
      "Training loss: 0.30121123790740967\n",
      "Training loss: 0.31887373328208923\n",
      "Training loss: 0.31968164443969727\n",
      "Training loss: 0.41044941544532776\n",
      "Training loss: 0.3563450574874878\n",
      "Training loss: 0.43179216980934143\n",
      "Training loss: 0.3062996566295624\n",
      "Training loss: 0.25975897908210754\n",
      "Training loss: 0.2979278862476349\n",
      "Training loss: 0.3140106797218323\n",
      "Training loss: 0.2741531729698181\n",
      "Training loss: 0.28051474690437317\n",
      "Training loss: 0.2714166045188904\n",
      "Training loss: 0.2603226900100708\n",
      "Training loss: 0.2427772581577301\n",
      "Training loss: 0.19237489998340607\n",
      "Training loss: 0.31361526250839233\n",
      "Training loss: 0.32132598757743835\n",
      "Training loss: 0.31803998351097107\n",
      "Training loss: 0.3984295725822449\n",
      "Training loss: 0.29218173027038574\n",
      "Training loss: 0.2902396321296692\n",
      "Training loss: 0.3586989939212799\n",
      "Training loss: 0.2540854811668396\n",
      "Training loss: 0.21087026596069336\n",
      "Training loss: 0.23975808918476105\n",
      "Training loss: 0.30157414078712463\n",
      "Training loss: 0.18218716979026794\n",
      "Training loss: 0.33857208490371704\n",
      "Training loss: 0.2815873324871063\n",
      "Training loss: 0.3059467673301697\n",
      "Training loss: 0.31484150886535645\n",
      "Training loss: 0.3601357936859131\n",
      "Training loss: 0.31718719005584717\n",
      "Training loss: 0.2734779417514801\n",
      "Training loss: 0.32425084710121155\n",
      "Training loss: 0.29114988446235657\n",
      "Training loss: 0.2843785583972931\n",
      "Training loss: 0.3335339426994324\n",
      "Training loss: 0.3236026465892792\n",
      "Training loss: 0.1967732161283493\n",
      "Training loss: 0.42097920179367065\n",
      "Training loss: 0.3445470631122589\n",
      "Training loss: 0.2774132788181305\n",
      "Training loss: 0.4070901572704315\n",
      "Training loss: 0.27208200097084045\n",
      "Training loss: 0.2563522756099701\n",
      "Training loss: 0.34719446301460266\n",
      "Training loss: 0.2896382808685303\n",
      "Training loss: 0.4355565905570984\n",
      "Training loss: 0.3186326324939728\n",
      "Training loss: 0.2761392593383789\n",
      "Training loss: 0.3592455983161926\n",
      "Training loss: 0.3489615023136139\n",
      "Training loss: 0.33702680468559265\n",
      "Training loss: 0.3012102544307709\n",
      "Training loss: 0.31887561082839966\n",
      "Training loss: 0.3196790814399719\n",
      "Training loss: 0.4104815721511841\n",
      "Training loss: 0.35635533928871155\n",
      "Training loss: 0.4318133592605591\n",
      "Training loss: 0.3063047230243683\n",
      "Training loss: 0.259748250246048\n",
      "Training loss: 0.2979183495044708\n",
      "Training loss: 0.31401005387306213\n",
      "Training loss: 0.274152010679245\n",
      "Training loss: 0.2805200219154358\n",
      "Training loss: 0.2714122533798218\n",
      "Training loss: 0.26032036542892456\n",
      "Training loss: 0.24277259409427643\n",
      "Training loss: 0.19234490394592285\n",
      "Training loss: 0.3136139214038849\n",
      "Training loss: 0.3213387727737427\n",
      "Training loss: 0.3180297911167145\n",
      "Training loss: 0.39843931794166565\n",
      "Training loss: 0.2921746075153351\n",
      "Training loss: 0.2902374565601349\n",
      "Training loss: 0.35871198773384094\n",
      "Training loss: 0.25407537817955017\n",
      "Training loss: 0.21084773540496826\n",
      "Training loss: 0.23973725736141205\n",
      "Training loss: 0.3015696406364441\n",
      "Training loss: 0.18215903639793396\n",
      "Training loss: 0.33858323097229004\n",
      "Training loss: 0.28158292174339294\n",
      "Training loss: 0.30593836307525635\n",
      "Training loss: 0.3148483335971832\n",
      "Training loss: 0.3601459264755249\n",
      "Training loss: 0.31719332933425903\n",
      "Training loss: 0.27346816658973694\n",
      "Training loss: 0.32425063848495483\n",
      "Training loss: 0.2911452054977417\n",
      "Training loss: 0.28437259793281555\n",
      "Training loss: 0.33353444933891296\n",
      "Training loss: 0.3236052691936493\n",
      "Training loss: 0.19675229489803314\n",
      "Training loss: 0.4210037291049957\n",
      "Training loss: 0.3445611894130707\n",
      "Training loss: 0.2774079442024231\n",
      "Training loss: 0.4071098566055298\n",
      "Training loss: 0.27206817269325256\n",
      "Training loss: 0.25634026527404785\n",
      "Training loss: 0.34720826148986816\n",
      "Training loss: 0.28962698578834534\n",
      "Training loss: 0.43559613823890686\n",
      "Training loss: 0.31864133477211\n",
      "Training loss: 0.27612221240997314\n",
      "Training loss: 0.3592621982097626\n",
      "Training loss: 0.34896931052207947\n",
      "Training loss: 0.3370278477668762\n",
      "Training loss: 0.30120933055877686\n",
      "Training loss: 0.3188772201538086\n",
      "Training loss: 0.3196769952774048\n",
      "Training loss: 0.4105103611946106\n",
      "Training loss: 0.3563646972179413\n",
      "Training loss: 0.4318324327468872\n",
      "Training loss: 0.30630919337272644\n",
      "Training loss: 0.2597386837005615\n",
      "Training loss: 0.29790985584259033\n",
      "Training loss: 0.31400930881500244\n",
      "Training loss: 0.27415114641189575\n",
      "Training loss: 0.2805250883102417\n",
      "Training loss: 0.2714087963104248\n",
      "Training loss: 0.2603188455104828\n",
      "Training loss: 0.24276886880397797\n",
      "Training loss: 0.19231821596622467\n",
      "Training loss: 0.3136124610900879\n",
      "Training loss: 0.32135066390037537\n",
      "Training loss: 0.3180205821990967\n",
      "Training loss: 0.39844775199890137\n",
      "Training loss: 0.292168527841568\n",
      "Training loss: 0.2902357876300812\n",
      "Training loss: 0.35872358083724976\n",
      "Training loss: 0.2540664076805115\n",
      "Training loss: 0.21082772314548492\n",
      "Training loss: 0.23971879482269287\n",
      "Training loss: 0.30156567692756653\n",
      "Training loss: 0.18213380873203278\n",
      "Training loss: 0.3385933041572571\n",
      "Training loss: 0.28157928586006165\n",
      "Training loss: 0.30593061447143555\n",
      "Training loss: 0.3148545026779175\n",
      "Training loss: 0.36015525460243225\n",
      "Training loss: 0.31719905138015747\n",
      "Training loss: 0.27345919609069824\n",
      "Training loss: 0.32425037026405334\n",
      "Training loss: 0.2911410629749298\n",
      "Training loss: 0.2843673527240753\n",
      "Training loss: 0.3335350751876831\n",
      "Training loss: 0.32360735535621643\n",
      "Training loss: 0.19673332571983337\n",
      "Training loss: 0.4210261404514313\n",
      "Training loss: 0.34457388520240784\n",
      "Training loss: 0.2774029076099396\n",
      "Training loss: 0.4071277678012848\n",
      "Training loss: 0.272055983543396\n",
      "Training loss: 0.25632938742637634\n",
      "Training loss: 0.34722059965133667\n",
      "Training loss: 0.28961676359176636\n",
      "Training loss: 0.43563276529312134\n",
      "Training loss: 0.31864914298057556\n",
      "Training loss: 0.27610668540000916\n",
      "Training loss: 0.3592774271965027\n",
      "Training loss: 0.34897640347480774\n",
      "Training loss: 0.33702901005744934\n",
      "Training loss: 0.3012084662914276\n",
      "Training loss: 0.3188788890838623\n",
      "Training loss: 0.3196750283241272\n",
      "Training loss: 0.41053634881973267\n",
      "Training loss: 0.3563728630542755\n",
      "Training loss: 0.43184953927993774\n",
      "Training loss: 0.3063129782676697\n",
      "Training loss: 0.2597300112247467\n",
      "Training loss: 0.29790225625038147\n",
      "Training loss: 0.31400835514068604\n",
      "Training loss: 0.2741504907608032\n",
      "Training loss: 0.2805299460887909\n",
      "Training loss: 0.2714061737060547\n",
      "Training loss: 0.2603181004524231\n",
      "Training loss: 0.24276593327522278\n",
      "Training loss: 0.19229435920715332\n",
      "Training loss: 0.3136107325553894\n",
      "Training loss: 0.3213618993759155\n",
      "Training loss: 0.318011999130249\n",
      "Training loss: 0.3984551429748535\n",
      "Training loss: 0.29216331243515015\n",
      "Training loss: 0.2902345657348633\n",
      "Training loss: 0.3587343990802765\n",
      "Training loss: 0.25405827164649963\n",
      "Training loss: 0.21080990135669708\n",
      "Training loss: 0.2397022843360901\n",
      "Training loss: 0.3015620708465576\n",
      "Training loss: 0.18211127817630768\n",
      "Training loss: 0.33860233426094055\n",
      "Training loss: 0.2815759479999542\n",
      "Training loss: 0.3059234619140625\n",
      "Training loss: 0.31486019492149353\n",
      "Training loss: 0.3601638376712799\n",
      "Training loss: 0.31720441579818726\n",
      "Training loss: 0.2734510004520416\n",
      "Training loss: 0.32425037026405334\n",
      "Training loss: 0.29113754630088806\n",
      "Training loss: 0.28436264395713806\n",
      "Training loss: 0.3335358202457428\n",
      "Training loss: 0.3236088156700134\n",
      "Training loss: 0.19671614468097687\n",
      "Training loss: 0.4210467040538788\n",
      "Training loss: 0.3445852994918823\n",
      "Training loss: 0.277398020029068\n",
      "Training loss: 0.4071439504623413\n",
      "Training loss: 0.27204519510269165\n",
      "Training loss: 0.2563195526599884\n",
      "Training loss: 0.3472318649291992\n",
      "Training loss: 0.28960734605789185\n",
      "Training loss: 0.43566662073135376\n",
      "Training loss: 0.31865638494491577\n",
      "Training loss: 0.27609243988990784\n",
      "Training loss: 0.35929131507873535\n",
      "Training loss: 0.3489828109741211\n",
      "Training loss: 0.33703020215034485\n",
      "Training loss: 0.30120766162872314\n",
      "Training loss: 0.31888020038604736\n",
      "Training loss: 0.3196735084056854\n",
      "Training loss: 0.41055989265441895\n",
      "Training loss: 0.3563803434371948\n",
      "Training loss: 0.43186503648757935\n",
      "Training loss: 0.3063162565231323\n",
      "Training loss: 0.2597220838069916\n",
      "Training loss: 0.29789552092552185\n",
      "Training loss: 0.31400740146636963\n",
      "Training loss: 0.2741500437259674\n",
      "Training loss: 0.28053462505340576\n",
      "Training loss: 0.2714042365550995\n",
      "Training loss: 0.2603181004524231\n",
      "Training loss: 0.24276375770568848\n",
      "Training loss: 0.19227305054664612\n",
      "Training loss: 0.31360897421836853\n",
      "Training loss: 0.32137247920036316\n",
      "Training loss: 0.31800419092178345\n",
      "Training loss: 0.39846163988113403\n",
      "Training loss: 0.29215890169143677\n",
      "Training loss: 0.2902337312698364\n",
      "Training loss: 0.3587438464164734\n",
      "Training loss: 0.25405094027519226\n",
      "Training loss: 0.21079407632350922\n",
      "Training loss: 0.23968759179115295\n",
      "Training loss: 0.3015589118003845\n",
      "Training loss: 0.18209104239940643\n",
      "Training loss: 0.33861055970191956\n",
      "Training loss: 0.2815733850002289\n",
      "Training loss: 0.30591684579849243\n",
      "Training loss: 0.3148653507232666\n",
      "Training loss: 0.36017194390296936\n",
      "Training loss: 0.3172094225883484\n",
      "Training loss: 0.27344340085983276\n",
      "Training loss: 0.3242502510547638\n",
      "Training loss: 0.2911345362663269\n",
      "Training loss: 0.284358412027359\n",
      "Training loss: 0.33353665471076965\n",
      "Training loss: 0.32360976934432983\n",
      "Training loss: 0.19670051336288452\n",
      "Training loss: 0.42106544971466064\n",
      "Training loss: 0.34459561109542847\n",
      "Training loss: 0.2773933410644531\n",
      "Training loss: 0.4071587026119232\n",
      "Training loss: 0.27203571796417236\n",
      "Training loss: 0.25631049275398254\n",
      "Training loss: 0.347242146730423\n",
      "Training loss: 0.289598673582077\n",
      "Training loss: 0.4356973171234131\n",
      "Training loss: 0.3186628818511963\n",
      "Training loss: 0.27607932686805725\n",
      "Training loss: 0.3593038320541382\n",
      "Training loss: 0.3489885628223419\n",
      "Training loss: 0.33703142404556274\n",
      "Training loss: 0.30120688676834106\n",
      "Training loss: 0.3188813328742981\n",
      "Training loss: 0.31967228651046753\n",
      "Training loss: 0.41058117151260376\n",
      "Training loss: 0.35638704895973206\n",
      "Training loss: 0.4318791925907135\n",
      "Training loss: 0.30631908774375916\n",
      "Training loss: 0.25971490144729614\n",
      "Training loss: 0.29788947105407715\n",
      "Training loss: 0.31400635838508606\n",
      "Training loss: 0.2741497755050659\n",
      "Training loss: 0.28053906559944153\n",
      "Training loss: 0.27140283584594727\n",
      "Training loss: 0.2603185772895813\n",
      "Training loss: 0.2427622377872467\n",
      "Training loss: 0.1922539919614792\n",
      "Training loss: 0.3136071264743805\n",
      "Training loss: 0.32138240337371826\n",
      "Training loss: 0.3179970681667328\n",
      "Training loss: 0.3984673023223877\n",
      "Training loss: 0.2921551465988159\n",
      "Training loss: 0.29023319482803345\n",
      "Training loss: 0.3587523400783539\n",
      "Training loss: 0.2540442943572998\n",
      "Training loss: 0.21077996492385864\n",
      "Training loss: 0.23967446386814117\n",
      "Training loss: 0.30155614018440247\n",
      "Training loss: 0.18207287788391113\n",
      "Training loss: 0.3386179804801941\n",
      "Training loss: 0.2815711200237274\n",
      "Training loss: 0.30591076612472534\n",
      "Training loss: 0.31487008929252625\n",
      "Training loss: 0.36017951369285583\n",
      "Training loss: 0.31721407175064087\n",
      "Training loss: 0.27343639731407166\n",
      "Training loss: 0.32425013184547424\n",
      "Training loss: 0.29113173484802246\n",
      "Training loss: 0.28435465693473816\n",
      "Training loss: 0.3335375189781189\n",
      "Training loss: 0.3236103355884552\n",
      "Training loss: 0.1966862827539444\n",
      "Training loss: 0.4210827648639679\n",
      "Training loss: 0.3446049392223358\n",
      "Training loss: 0.27738890051841736\n",
      "Training loss: 0.4071720242500305\n",
      "Training loss: 0.27202728390693665\n",
      "Training loss: 0.2563023269176483\n",
      "Training loss: 0.34725135564804077\n",
      "Training loss: 0.2895907461643219\n",
      "Training loss: 0.4357263743877411\n",
      "Training loss: 0.31866884231567383\n",
      "Training loss: 0.276067316532135\n",
      "Training loss: 0.35931527614593506\n",
      "Training loss: 0.3489936888217926\n",
      "Training loss: 0.3370327055454254\n",
      "Training loss: 0.30120617151260376\n",
      "Training loss: 0.31888246536254883\n",
      "Training loss: 0.319671094417572\n",
      "Training loss: 0.41060054302215576\n",
      "Training loss: 0.3563932180404663\n",
      "Training loss: 0.43189185857772827\n",
      "Training loss: 0.30632150173187256\n",
      "Training loss: 0.25970837473869324\n",
      "Training loss: 0.297884076833725\n",
      "Training loss: 0.3140050768852234\n",
      "Training loss: 0.27414965629577637\n",
      "Training loss: 0.28054335713386536\n",
      "Training loss: 0.2714020013809204\n",
      "Training loss: 0.26031941175460815\n",
      "Training loss: 0.24276115000247955\n",
      "Training loss: 0.19223685562610626\n",
      "Training loss: 0.3136051595211029\n",
      "Training loss: 0.3213917911052704\n",
      "Training loss: 0.31799042224884033\n",
      "Training loss: 0.39847221970558167\n",
      "Training loss: 0.2921519875526428\n",
      "Training loss: 0.29023295640945435\n",
      "Training loss: 0.358760267496109\n",
      "Training loss: 0.2540383040904999\n",
      "Training loss: 0.21076729893684387\n",
      "Training loss: 0.23966272175312042\n",
      "Training loss: 0.3015536069869995\n",
      "Training loss: 0.18205641210079193\n",
      "Training loss: 0.3386246860027313\n",
      "Training loss: 0.2815692126750946\n",
      "Training loss: 0.3059050142765045\n",
      "Training loss: 0.3148745000362396\n",
      "Training loss: 0.36018651723861694\n",
      "Training loss: 0.317218542098999\n",
      "Training loss: 0.27342987060546875\n",
      "Training loss: 0.32425013184547424\n",
      "Training loss: 0.2911292314529419\n",
      "Training loss: 0.28435128927230835\n",
      "Training loss: 0.33353835344314575\n",
      "Training loss: 0.32361042499542236\n",
      "Training loss: 0.1966732293367386\n",
      "Training loss: 0.42109882831573486\n",
      "Training loss: 0.3446134030818939\n",
      "Training loss: 0.277384489774704\n",
      "Training loss: 0.40718427300453186\n",
      "Training loss: 0.2720198631286621\n",
      "Training loss: 0.2562945485115051\n",
      "Training loss: 0.3472598195075989\n",
      "Training loss: 0.28958335518836975\n",
      "Training loss: 0.4357532560825348\n",
      "Training loss: 0.31867438554763794\n",
      "Training loss: 0.276056170463562\n",
      "Training loss: 0.3593257963657379\n",
      "Training loss: 0.34899842739105225\n",
      "Training loss: 0.3370339870452881\n",
      "Training loss: 0.30120551586151123\n",
      "Training loss: 0.3188835382461548\n",
      "Training loss: 0.3196703791618347\n",
      "Training loss: 0.41061800718307495\n",
      "Training loss: 0.35639867186546326\n",
      "Training loss: 0.43190333247184753\n",
      "Training loss: 0.3063235878944397\n",
      "Training loss: 0.2597024440765381\n",
      "Training loss: 0.297879159450531\n",
      "Training loss: 0.3140040636062622\n",
      "Training loss: 0.2741495668888092\n",
      "Training loss: 0.28054746985435486\n",
      "Training loss: 0.271401584148407\n",
      "Training loss: 0.2603207230567932\n",
      "Training loss: 0.24276043474674225\n",
      "Training loss: 0.19222144782543182\n",
      "Training loss: 0.3136031925678253\n",
      "Training loss: 0.32140061259269714\n",
      "Training loss: 0.31798434257507324\n",
      "Training loss: 0.3984765112400055\n",
      "Training loss: 0.29214930534362793\n",
      "Training loss: 0.29023292660713196\n",
      "Training loss: 0.35876744985580444\n",
      "Training loss: 0.2540329098701477\n",
      "Training loss: 0.21075597405433655\n",
      "Training loss: 0.23965215682983398\n",
      "Training loss: 0.30155134201049805\n",
      "Training loss: 0.18204160034656525\n",
      "Training loss: 0.338630735874176\n",
      "Training loss: 0.28156769275665283\n",
      "Training loss: 0.3058995306491852\n",
      "Training loss: 0.3148786425590515\n",
      "Training loss: 0.36019328236579895\n",
      "Training loss: 0.3172226846218109\n",
      "Training loss: 0.2734238803386688\n",
      "Training loss: 0.32425007224082947\n",
      "Training loss: 0.2911269962787628\n",
      "Training loss: 0.28434816002845764\n",
      "Training loss: 0.333539217710495\n",
      "Training loss: 0.3236103057861328\n",
      "Training loss: 0.19666126370429993\n",
      "Training loss: 0.42111364006996155\n",
      "Training loss: 0.34462103247642517\n",
      "Training loss: 0.2773802578449249\n",
      "Training loss: 0.40719544887542725\n",
      "Training loss: 0.27201327681541443\n",
      "Training loss: 0.2562876045703888\n",
      "Training loss: 0.3472675085067749\n",
      "Training loss: 0.28957656025886536\n",
      "Training loss: 0.43577802181243896\n",
      "Training loss: 0.31867942214012146\n",
      "Training loss: 0.2760457992553711\n",
      "Training loss: 0.3593355119228363\n",
      "Training loss: 0.3490027189254761\n",
      "Training loss: 0.33703529834747314\n",
      "Training loss: 0.3012048900127411\n",
      "Training loss: 0.31888431310653687\n",
      "Training loss: 0.31966954469680786\n",
      "Training loss: 0.4106341004371643\n",
      "Training loss: 0.3564036786556244\n",
      "Training loss: 0.4319137930870056\n",
      "Training loss: 0.30632543563842773\n",
      "Training loss: 0.25969696044921875\n",
      "Training loss: 0.29787477850914\n",
      "Training loss: 0.31400278210639954\n",
      "Training loss: 0.27414965629577637\n",
      "Training loss: 0.28055140376091003\n",
      "Training loss: 0.27140143513679504\n",
      "Training loss: 0.2603222727775574\n",
      "Training loss: 0.24276016652584076\n",
      "Training loss: 0.19220753014087677\n",
      "Training loss: 0.31360116600990295\n",
      "Training loss: 0.3214089572429657\n",
      "Training loss: 0.3179786801338196\n",
      "Training loss: 0.39848029613494873\n",
      "Training loss: 0.29214704036712646\n",
      "Training loss: 0.29023298621177673\n",
      "Training loss: 0.3587740957736969\n",
      "Training loss: 0.25402796268463135\n",
      "Training loss: 0.21074578166007996\n",
      "Training loss: 0.23964262008666992\n",
      "Training loss: 0.3015493154525757\n",
      "Training loss: 0.18202811479568481\n",
      "Training loss: 0.3386363387107849\n",
      "Training loss: 0.2815665602684021\n",
      "Training loss: 0.30589449405670166\n",
      "Training loss: 0.31488239765167236\n",
      "Training loss: 0.3601996600627899\n",
      "Training loss: 0.3172266483306885\n",
      "Training loss: 0.27341821789741516\n",
      "Training loss: 0.32425007224082947\n",
      "Training loss: 0.2911251187324524\n",
      "Training loss: 0.2843453884124756\n",
      "Training loss: 0.333540141582489\n",
      "Training loss: 0.3236099183559418\n",
      "Training loss: 0.19665023684501648\n",
      "Training loss: 0.42112740874290466\n",
      "Training loss: 0.34462806582450867\n",
      "Training loss: 0.277376264333725\n",
      "Training loss: 0.4072057604789734\n",
      "Training loss: 0.27200737595558167\n",
      "Training loss: 0.2562810778617859\n",
      "Training loss: 0.3472745418548584\n",
      "Training loss: 0.2895702123641968\n",
      "Training loss: 0.4358013868331909\n",
      "Training loss: 0.31868407130241394\n",
      "Training loss: 0.27603626251220703\n",
      "Training loss: 0.35934436321258545\n",
      "Training loss: 0.349006712436676\n",
      "Training loss: 0.3370366394519806\n",
      "Training loss: 0.3012043237686157\n",
      "Training loss: 0.31888505816459656\n",
      "Training loss: 0.3196691870689392\n",
      "Training loss: 0.4106486141681671\n",
      "Training loss: 0.3564082384109497\n",
      "Training loss: 0.4319235384464264\n",
      "Training loss: 0.3063269555568695\n",
      "Training loss: 0.2596919536590576\n",
      "Training loss: 0.29787078499794006\n",
      "Training loss: 0.3140016198158264\n",
      "Training loss: 0.2741497755050659\n",
      "Training loss: 0.28055521845817566\n",
      "Training loss: 0.27140164375305176\n",
      "Training loss: 0.2603241503238678\n",
      "Training loss: 0.24276037514209747\n",
      "Training loss: 0.19219493865966797\n",
      "Training loss: 0.3135991096496582\n",
      "Training loss: 0.3214169144630432\n",
      "Training loss: 0.3179733455181122\n",
      "Training loss: 0.39848363399505615\n",
      "Training loss: 0.29214513301849365\n",
      "Training loss: 0.290233314037323\n",
      "Training loss: 0.35877999663352966\n",
      "Training loss: 0.2540234923362732\n",
      "Training loss: 0.21073661744594574\n",
      "Training loss: 0.23963408172130585\n",
      "Training loss: 0.30154746770858765\n",
      "Training loss: 0.18201583623886108\n",
      "Training loss: 0.3386414647102356\n",
      "Training loss: 0.28156542778015137\n",
      "Training loss: 0.3058898150920868\n",
      "Training loss: 0.3148858845233917\n",
      "Training loss: 0.3602055609226227\n",
      "Training loss: 0.31723037362098694\n",
      "Training loss: 0.2734130024909973\n",
      "Training loss: 0.3242500126361847\n",
      "Training loss: 0.29112327098846436\n",
      "Training loss: 0.28434282541275024\n",
      "Training loss: 0.33354103565216064\n",
      "Training loss: 0.3236093521118164\n",
      "Training loss: 0.19664008915424347\n",
      "Training loss: 0.4211401045322418\n",
      "Training loss: 0.344634473323822\n",
      "Training loss: 0.2773723602294922\n",
      "Training loss: 0.4072151482105255\n",
      "Training loss: 0.2720021605491638\n",
      "Training loss: 0.25627490878105164\n",
      "Training loss: 0.3472810387611389\n",
      "Training loss: 0.2895643413066864\n",
      "Training loss: 0.4358232617378235\n",
      "Training loss: 0.3186882734298706\n",
      "Training loss: 0.2760273814201355\n",
      "Training loss: 0.35935261845588684\n",
      "Training loss: 0.34901031851768494\n",
      "Training loss: 0.33703792095184326\n",
      "Training loss: 0.30120378732681274\n",
      "Training loss: 0.31888583302497864\n",
      "Training loss: 0.31966865062713623\n",
      "Training loss: 0.41066211462020874\n",
      "Training loss: 0.3564123511314392\n",
      "Training loss: 0.431932270526886\n",
      "Training loss: 0.30632829666137695\n",
      "Training loss: 0.2596873342990875\n",
      "Training loss: 0.29786720871925354\n",
      "Training loss: 0.3140007555484772\n",
      "Training loss: 0.27414995431900024\n",
      "Training loss: 0.28055885434150696\n",
      "Training loss: 0.2714020907878876\n",
      "Training loss: 0.26032617688179016\n",
      "Training loss: 0.24276059865951538\n",
      "Training loss: 0.1921835094690323\n",
      "Training loss: 0.31359702348709106\n",
      "Training loss: 0.3214243948459625\n",
      "Training loss: 0.31796836853027344\n",
      "Training loss: 0.39848655462265015\n",
      "Training loss: 0.2921435534954071\n",
      "Training loss: 0.29023370146751404\n",
      "Training loss: 0.3587852716445923\n",
      "Training loss: 0.2540193498134613\n",
      "Training loss: 0.21072831749916077\n",
      "Training loss: 0.23962630331516266\n",
      "Training loss: 0.3015458285808563\n",
      "Training loss: 0.1820046305656433\n",
      "Training loss: 0.3386460840702057\n",
      "Training loss: 0.2815646529197693\n",
      "Training loss: 0.3058854043483734\n",
      "Training loss: 0.31488922238349915\n",
      "Training loss: 0.36021122336387634\n",
      "Training loss: 0.3172338902950287\n",
      "Training loss: 0.27340811491012573\n",
      "Training loss: 0.3242499530315399\n",
      "Training loss: 0.29112181067466736\n",
      "Training loss: 0.2843404710292816\n",
      "Training loss: 0.3335418701171875\n",
      "Training loss: 0.3236086070537567\n",
      "Training loss: 0.19663073122501373\n",
      "Training loss: 0.4211520552635193\n",
      "Training loss: 0.3446403741836548\n",
      "Training loss: 0.2773686647415161\n",
      "Training loss: 0.40722379088401794\n",
      "Training loss: 0.27199745178222656\n",
      "Training loss: 0.2562696039676666\n",
      "Training loss: 0.34728705883026123\n",
      "Training loss: 0.28955885767936707\n",
      "Training loss: 0.4358437955379486\n",
      "Training loss: 0.3186924457550049\n",
      "Training loss: 0.2760190963745117\n",
      "Training loss: 0.3593601584434509\n",
      "Training loss: 0.3490135669708252\n",
      "Training loss: 0.3370392322540283\n",
      "Training loss: 0.30120331048965454\n",
      "Training loss: 0.318886399269104\n",
      "Training loss: 0.3196682631969452\n",
      "Training loss: 0.410674512386322\n",
      "Training loss: 0.3564161956310272\n",
      "Training loss: 0.4319404065608978\n",
      "Training loss: 0.30632951855659485\n",
      "Training loss: 0.25968310236930847\n",
      "Training loss: 0.29786384105682373\n",
      "Training loss: 0.3139995038509369\n",
      "Training loss: 0.27415019273757935\n",
      "Training loss: 0.2805623412132263\n",
      "Training loss: 0.2714027166366577\n",
      "Training loss: 0.2603282630443573\n",
      "Training loss: 0.24276122450828552\n",
      "Training loss: 0.19217310845851898\n",
      "Training loss: 0.3135949969291687\n",
      "Training loss: 0.3214315176010132\n",
      "Training loss: 0.3179638087749481\n",
      "Training loss: 0.39848899841308594\n",
      "Training loss: 0.29214224219322205\n",
      "Training loss: 0.29023417830467224\n",
      "Training loss: 0.3587902784347534\n",
      "Training loss: 0.25401559472084045\n",
      "Training loss: 0.21072079241275787\n",
      "Training loss: 0.23961924016475677\n",
      "Training loss: 0.3015442490577698\n",
      "Training loss: 0.18199439346790314\n",
      "Training loss: 0.33865031599998474\n",
      "Training loss: 0.2815641164779663\n",
      "Training loss: 0.30588123202323914\n",
      "Training loss: 0.3148922920227051\n",
      "Training loss: 0.3602166175842285\n",
      "Training loss: 0.3172372579574585\n",
      "Training loss: 0.27340349555015564\n",
      "Training loss: 0.3242500126361847\n",
      "Training loss: 0.291120320558548\n",
      "Training loss: 0.2843382954597473\n",
      "Training loss: 0.33354270458221436\n",
      "Training loss: 0.32360777258872986\n",
      "Training loss: 0.19662201404571533\n",
      "Training loss: 0.4211632311344147\n",
      "Training loss: 0.3446456789970398\n",
      "Training loss: 0.2773650288581848\n",
      "Training loss: 0.407231867313385\n",
      "Training loss: 0.27199333906173706\n",
      "Training loss: 0.2562641501426697\n",
      "Training loss: 0.34729260206222534\n",
      "Training loss: 0.2895537316799164\n",
      "Training loss: 0.4358628988265991\n",
      "Training loss: 0.31869614124298096\n",
      "Training loss: 0.2760113477706909\n",
      "Training loss: 0.3593672811985016\n",
      "Training loss: 0.34901660680770874\n",
      "Training loss: 0.33704042434692383\n",
      "Training loss: 0.3012028634548187\n",
      "Training loss: 0.31888702511787415\n",
      "Training loss: 0.3196679353713989\n",
      "Training loss: 0.410685658454895\n",
      "Training loss: 0.3564196228981018\n",
      "Training loss: 0.43194779753685\n",
      "Training loss: 0.30633050203323364\n",
      "Training loss: 0.2596791684627533\n",
      "Training loss: 0.29786089062690735\n",
      "Training loss: 0.3139985203742981\n",
      "Training loss: 0.2741504907608032\n",
      "Training loss: 0.28056570887565613\n",
      "Training loss: 0.27140355110168457\n",
      "Training loss: 0.26033058762550354\n",
      "Training loss: 0.24276189506053925\n",
      "Training loss: 0.1921636462211609\n",
      "Training loss: 0.31359291076660156\n",
      "Training loss: 0.3214382827281952\n",
      "Training loss: 0.31795942783355713\n",
      "Training loss: 0.3984912037849426\n",
      "Training loss: 0.2921411693096161\n",
      "Training loss: 0.2902347445487976\n",
      "Training loss: 0.3587948977947235\n",
      "Training loss: 0.2540121376514435\n",
      "Training loss: 0.21071398258209229\n",
      "Training loss: 0.23961281776428223\n",
      "Training loss: 0.3015429377555847\n",
      "Training loss: 0.1819850504398346\n",
      "Training loss: 0.3386542499065399\n",
      "Training loss: 0.28156358003616333\n",
      "Training loss: 0.30587732791900635\n",
      "Training loss: 0.3148951530456543\n",
      "Training loss: 0.36022165417671204\n",
      "Training loss: 0.3172404170036316\n",
      "Training loss: 0.2733991742134094\n",
      "Training loss: 0.3242499828338623\n",
      "Training loss: 0.2911190092563629\n",
      "Training loss: 0.2843364477157593\n",
      "Training loss: 0.3335436284542084\n",
      "Training loss: 0.3236067295074463\n",
      "Training loss: 0.19661390781402588\n",
      "Training loss: 0.421173632144928\n",
      "Training loss: 0.3446506857872009\n",
      "Training loss: 0.27736154198646545\n",
      "Training loss: 0.4072393774986267\n",
      "Training loss: 0.2719895541667938\n",
      "Training loss: 0.2562595307826996\n",
      "Training loss: 0.3472977876663208\n",
      "Training loss: 0.28954896330833435\n",
      "Training loss: 0.43588054180145264\n",
      "Training loss: 0.3186997175216675\n",
      "Training loss: 0.2760040760040283\n",
      "Training loss: 0.35937368869781494\n",
      "Training loss: 0.34901943802833557\n",
      "Training loss: 0.3370417058467865\n",
      "Training loss: 0.3012024164199829\n",
      "Training loss: 0.31888750195503235\n",
      "Training loss: 0.3196676969528198\n",
      "Training loss: 0.41069626808166504\n",
      "Training loss: 0.35642287135124207\n",
      "Training loss: 0.4319547414779663\n",
      "Training loss: 0.3063313961029053\n",
      "Training loss: 0.2596755623817444\n",
      "Training loss: 0.29785817861557007\n",
      "Training loss: 0.3139973282814026\n",
      "Training loss: 0.2741508483886719\n",
      "Training loss: 0.280568927526474\n",
      "Training loss: 0.2714044749736786\n",
      "Training loss: 0.2603329122066498\n",
      "Training loss: 0.24276268482208252\n",
      "Training loss: 0.19215500354766846\n",
      "Training loss: 0.3135909140110016\n",
      "Training loss: 0.32144469022750854\n",
      "Training loss: 0.3179554343223572\n",
      "Training loss: 0.39849311113357544\n",
      "Training loss: 0.29214024543762207\n",
      "Training loss: 0.29023540019989014\n",
      "Training loss: 0.35879895091056824\n",
      "Training loss: 0.254008948802948\n",
      "Training loss: 0.21070778369903564\n",
      "Training loss: 0.23960697650909424\n",
      "Training loss: 0.30154165625572205\n",
      "Training loss: 0.18197649717330933\n",
      "Training loss: 0.3386579155921936\n",
      "Training loss: 0.28156328201293945\n",
      "Training loss: 0.3058736324310303\n",
      "Training loss: 0.3148978352546692\n",
      "Training loss: 0.360226571559906\n",
      "Training loss: 0.3172434866428375\n",
      "Training loss: 0.2733951210975647\n",
      "Training loss: 0.32424992322921753\n",
      "Training loss: 0.2911180555820465\n",
      "Training loss: 0.28433459997177124\n",
      "Training loss: 0.33354446291923523\n",
      "Training loss: 0.32360556721687317\n",
      "Training loss: 0.19660638272762299\n",
      "Training loss: 0.4211834967136383\n",
      "Training loss: 0.34465521574020386\n",
      "Training loss: 0.27735814452171326\n",
      "Training loss: 0.40724626183509827\n",
      "Training loss: 0.2719862461090088\n",
      "Training loss: 0.25625500082969666\n",
      "Training loss: 0.34730249643325806\n",
      "Training loss: 0.2895444631576538\n",
      "Training loss: 0.4358980059623718\n",
      "Training loss: 0.31870290637016296\n",
      "Training loss: 0.27599725127220154\n",
      "Training loss: 0.3593798875808716\n",
      "Training loss: 0.3490220010280609\n",
      "Training loss: 0.3370429277420044\n",
      "Training loss: 0.3012019991874695\n",
      "Training loss: 0.31888797879219055\n",
      "Training loss: 0.31966763734817505\n",
      "Training loss: 0.4107059836387634\n",
      "Training loss: 0.35642582178115845\n",
      "Training loss: 0.43196094036102295\n",
      "Training loss: 0.3063320815563202\n",
      "Training loss: 0.2596721053123474\n",
      "Training loss: 0.2978556454181671\n",
      "Training loss: 0.3139965236186981\n",
      "Training loss: 0.27415117621421814\n",
      "Training loss: 0.2805720567703247\n",
      "Training loss: 0.27140554785728455\n",
      "Training loss: 0.26033535599708557\n",
      "Training loss: 0.24276351928710938\n",
      "Training loss: 0.19214709103107452\n",
      "Training loss: 0.3135889172554016\n",
      "Training loss: 0.3214508891105652\n",
      "Training loss: 0.31795158982276917\n",
      "Training loss: 0.39849478006362915\n",
      "Training loss: 0.29213958978652954\n",
      "Training loss: 0.29023611545562744\n",
      "Training loss: 0.3588029742240906\n",
      "Training loss: 0.2540059983730316\n",
      "Training loss: 0.21070215106010437\n",
      "Training loss: 0.23960162699222565\n",
      "Training loss: 0.30154046416282654\n",
      "Training loss: 0.18196851015090942\n",
      "Training loss: 0.3386612832546234\n",
      "Training loss: 0.28156304359436035\n",
      "Training loss: 0.3058701157569885\n",
      "Training loss: 0.31490036845207214\n",
      "Training loss: 0.36023110151290894\n",
      "Training loss: 0.3172464370727539\n",
      "Training loss: 0.2733913064002991\n",
      "Training loss: 0.32424992322921753\n",
      "Training loss: 0.29111701250076294\n",
      "Training loss: 0.28433293104171753\n",
      "Training loss: 0.33354535698890686\n",
      "Training loss: 0.32360440492630005\n",
      "Training loss: 0.1965993344783783\n",
      "Training loss: 0.4211927056312561\n",
      "Training loss: 0.3446595370769501\n",
      "Training loss: 0.2773549258708954\n",
      "Training loss: 0.40725263953208923\n",
      "Training loss: 0.27198320627212524\n",
      "Training loss: 0.25625088810920715\n",
      "Training loss: 0.3473069667816162\n",
      "Training loss: 0.28954020142555237\n",
      "Training loss: 0.4359138309955597\n",
      "Training loss: 0.31870612502098083\n",
      "Training loss: 0.27599090337753296\n",
      "Training loss: 0.35938555002212524\n",
      "Training loss: 0.34902438521385193\n",
      "Training loss: 0.3370440900325775\n",
      "Training loss: 0.3012017011642456\n",
      "Training loss: 0.3188883662223816\n",
      "Training loss: 0.3196674585342407\n",
      "Training loss: 0.4107147455215454\n",
      "Training loss: 0.35642847418785095\n",
      "Training loss: 0.4319669306278229\n",
      "Training loss: 0.30633270740509033\n",
      "Training loss: 0.25966888666152954\n",
      "Training loss: 0.29785335063934326\n",
      "Training loss: 0.31399548053741455\n",
      "Training loss: 0.27415159344673157\n",
      "Training loss: 0.28057506680488586\n",
      "Training loss: 0.27140671014785767\n",
      "Training loss: 0.26033779978752136\n",
      "Training loss: 0.2427646815776825\n",
      "Training loss: 0.19213980436325073\n",
      "Training loss: 0.313586950302124\n",
      "Training loss: 0.3214566707611084\n",
      "Training loss: 0.31794804334640503\n",
      "Training loss: 0.3984963297843933\n",
      "Training loss: 0.2921389937400818\n",
      "Training loss: 0.29023683071136475\n",
      "Training loss: 0.3588065803050995\n",
      "Training loss: 0.25400328636169434\n",
      "Training loss: 0.2106969654560089\n",
      "Training loss: 0.23959672451019287\n",
      "Training loss: 0.30153942108154297\n",
      "Training loss: 0.18196125328540802\n",
      "Training loss: 0.33866438269615173\n",
      "Training loss: 0.2815629839897156\n",
      "Training loss: 0.3058668076992035\n",
      "Training loss: 0.31490278244018555\n",
      "Training loss: 0.3602353632450104\n",
      "Training loss: 0.3172491788864136\n",
      "Training loss: 0.2733876705169678\n",
      "Training loss: 0.324249804019928\n",
      "Training loss: 0.29111605882644653\n",
      "Training loss: 0.28433144092559814\n",
      "Training loss: 0.33354610204696655\n",
      "Training loss: 0.32360315322875977\n",
      "Training loss: 0.196592777967453\n",
      "Training loss: 0.4212014079093933\n",
      "Training loss: 0.34466353058815\n",
      "Training loss: 0.2773517966270447\n",
      "Training loss: 0.4072587192058563\n",
      "Training loss: 0.27198049426078796\n",
      "Training loss: 0.25624656677246094\n",
      "Training loss: 0.3473111093044281\n",
      "Training loss: 0.2895362377166748\n",
      "Training loss: 0.43592917919158936\n",
      "Training loss: 0.31870895624160767\n",
      "Training loss: 0.27598491311073303\n",
      "Training loss: 0.3593907952308655\n",
      "Training loss: 0.349026620388031\n",
      "Training loss: 0.337045282125473\n",
      "Training loss: 0.30120131373405457\n",
      "Training loss: 0.318888783454895\n",
      "Training loss: 0.31966739892959595\n",
      "Training loss: 0.4107232093811035\n",
      "Training loss: 0.3564310371875763\n",
      "Training loss: 0.4319722056388855\n",
      "Training loss: 0.3063332438468933\n",
      "Training loss: 0.25966596603393555\n",
      "Training loss: 0.29785120487213135\n",
      "Training loss: 0.31399455666542053\n",
      "Training loss: 0.2741519510746002\n",
      "Training loss: 0.2805779278278351\n",
      "Training loss: 0.27140796184539795\n",
      "Training loss: 0.26034030318260193\n",
      "Training loss: 0.24276581406593323\n",
      "Training loss: 0.1921330839395523\n",
      "Training loss: 0.3135850429534912\n",
      "Training loss: 0.3214623034000397\n",
      "Training loss: 0.31794461607933044\n",
      "Training loss: 0.3984975516796112\n",
      "Training loss: 0.29213857650756836\n",
      "Training loss: 0.2902376055717468\n",
      "Training loss: 0.3588101267814636\n",
      "Training loss: 0.25400078296661377\n",
      "Training loss: 0.21069225668907166\n",
      "Training loss: 0.23959225416183472\n",
      "Training loss: 0.3015384376049042\n",
      "Training loss: 0.18195442855358124\n",
      "Training loss: 0.33866721391677856\n",
      "Training loss: 0.281562864780426\n",
      "Training loss: 0.30586370825767517\n",
      "Training loss: 0.3149050176143646\n",
      "Training loss: 0.3602394759654999\n",
      "Training loss: 0.3172517716884613\n",
      "Training loss: 0.27338430285453796\n",
      "Training loss: 0.3242497444152832\n",
      "Training loss: 0.2911152243614197\n",
      "Training loss: 0.2843300402164459\n",
      "Training loss: 0.333546906709671\n",
      "Training loss: 0.32360199093818665\n",
      "Training loss: 0.19658663868904114\n",
      "Training loss: 0.4212096333503723\n",
      "Training loss: 0.3446671962738037\n",
      "Training loss: 0.2773488163948059\n",
      "Training loss: 0.4072643220424652\n",
      "Training loss: 0.2719779908657074\n",
      "Training loss: 0.256242960691452\n",
      "Training loss: 0.34731489419937134\n",
      "Training loss: 0.28953245282173157\n",
      "Training loss: 0.43594348430633545\n",
      "Training loss: 0.3187117576599121\n",
      "Training loss: 0.27597928047180176\n",
      "Training loss: 0.359395831823349\n",
      "Training loss: 0.3490287661552429\n",
      "Training loss: 0.337046355009079\n",
      "Training loss: 0.3012010157108307\n",
      "Training loss: 0.3188891112804413\n",
      "Training loss: 0.31966739892959595\n",
      "Training loss: 0.41073089838027954\n",
      "Training loss: 0.3564334511756897\n",
      "Training loss: 0.4319772720336914\n",
      "Training loss: 0.30633366107940674\n",
      "Training loss: 0.2596631646156311\n",
      "Training loss: 0.29784923791885376\n",
      "Training loss: 0.3139934837818146\n",
      "Training loss: 0.27415233850479126\n",
      "Training loss: 0.28058069944381714\n",
      "Training loss: 0.27140921354293823\n",
      "Training loss: 0.2603427469730377\n",
      "Training loss: 0.2427670955657959\n",
      "Training loss: 0.19212691485881805\n",
      "Training loss: 0.3135831654071808\n",
      "Training loss: 0.3214675486087799\n",
      "Training loss: 0.3179413974285126\n",
      "Training loss: 0.39849868416786194\n",
      "Training loss: 0.2921382188796997\n",
      "Training loss: 0.2902383506298065\n",
      "Training loss: 0.35881322622299194\n",
      "Training loss: 0.25399842858314514\n",
      "Training loss: 0.21068787574768066\n",
      "Training loss: 0.23958809673786163\n",
      "Training loss: 0.30153757333755493\n",
      "Training loss: 0.18194812536239624\n",
      "Training loss: 0.33866989612579346\n",
      "Training loss: 0.28156277537345886\n",
      "Training loss: 0.3058607578277588\n",
      "Training loss: 0.3149071931838989\n",
      "Training loss: 0.36024340987205505\n",
      "Training loss: 0.3172542452812195\n",
      "Training loss: 0.2733811140060425\n",
      "Training loss: 0.3242497742176056\n",
      "Training loss: 0.29111436009407043\n",
      "Training loss: 0.2843286693096161\n",
      "Training loss: 0.3335476517677307\n",
      "Training loss: 0.3236006796360016\n",
      "Training loss: 0.19658085703849792\n",
      "Training loss: 0.42121726274490356\n",
      "Training loss: 0.34467050433158875\n",
      "Training loss: 0.2773459255695343\n",
      "Training loss: 0.40726953744888306\n",
      "Training loss: 0.2719758152961731\n",
      "Training loss: 0.25623950362205505\n",
      "Training loss: 0.34731853008270264\n",
      "Training loss: 0.28952890634536743\n",
      "Training loss: 0.43595677614212036\n",
      "Training loss: 0.3187142312526703\n",
      "Training loss: 0.27597397565841675\n",
      "Training loss: 0.35940051078796387\n",
      "Training loss: 0.34903061389923096\n",
      "Training loss: 0.33704742789268494\n",
      "Training loss: 0.3012006878852844\n",
      "Training loss: 0.3188895285129547\n",
      "Training loss: 0.31966739892959595\n",
      "Training loss: 0.41073814034461975\n",
      "Training loss: 0.3564355671405792\n",
      "Training loss: 0.4319821000099182\n",
      "Training loss: 0.30633410811424255\n",
      "Training loss: 0.25966063141822815\n",
      "Training loss: 0.2978474199771881\n",
      "Training loss: 0.3139925003051758\n",
      "Training loss: 0.2741527557373047\n",
      "Training loss: 0.28058335185050964\n",
      "Training loss: 0.2714104652404785\n",
      "Training loss: 0.2603452503681183\n",
      "Training loss: 0.24276824295520782\n",
      "Training loss: 0.192121222615242\n",
      "Training loss: 0.31358134746551514\n",
      "Training loss: 0.32147255539894104\n",
      "Training loss: 0.3179384469985962\n",
      "Training loss: 0.3984997570514679\n",
      "Training loss: 0.292138010263443\n",
      "Training loss: 0.290239155292511\n",
      "Training loss: 0.3588162660598755\n",
      "Training loss: 0.25399625301361084\n",
      "Training loss: 0.21068386733531952\n",
      "Training loss: 0.23958423733711243\n",
      "Training loss: 0.3015367388725281\n",
      "Training loss: 0.18194232881069183\n",
      "Training loss: 0.338672399520874\n",
      "Training loss: 0.2815628945827484\n",
      "Training loss: 0.3058578670024872\n",
      "Training loss: 0.3149092197418213\n",
      "Training loss: 0.3602471649646759\n",
      "Training loss: 0.3172566890716553\n",
      "Training loss: 0.27337801456451416\n",
      "Training loss: 0.3242497146129608\n",
      "Training loss: 0.29111364483833313\n",
      "Training loss: 0.28432750701904297\n",
      "Training loss: 0.3335484266281128\n",
      "Training loss: 0.32359933853149414\n",
      "Training loss: 0.19657543301582336\n",
      "Training loss: 0.4212246537208557\n",
      "Training loss: 0.344673752784729\n",
      "Training loss: 0.27734312415122986\n",
      "Training loss: 0.4072744846343994\n",
      "Training loss: 0.27197378873825073\n",
      "Training loss: 0.25623616576194763\n",
      "Training loss: 0.34732192754745483\n",
      "Training loss: 0.2895255386829376\n",
      "Training loss: 0.4359697997570038\n",
      "Training loss: 0.31871670484542847\n",
      "Training loss: 0.27596887946128845\n",
      "Training loss: 0.35940492153167725\n",
      "Training loss: 0.3490324318408966\n",
      "Training loss: 0.33704856038093567\n",
      "Training loss: 0.30120038986206055\n",
      "Training loss: 0.31888967752456665\n",
      "Training loss: 0.31966739892959595\n",
      "Training loss: 0.41074490547180176\n",
      "Training loss: 0.3564375638961792\n",
      "Training loss: 0.43198657035827637\n",
      "Training loss: 0.30633437633514404\n",
      "Training loss: 0.25965818762779236\n",
      "Training loss: 0.297845721244812\n",
      "Training loss: 0.3139917254447937\n",
      "Training loss: 0.2741531729698181\n",
      "Training loss: 0.2805858552455902\n",
      "Training loss: 0.2714117765426636\n",
      "Training loss: 0.26034772396087646\n",
      "Training loss: 0.2427695095539093\n",
      "Training loss: 0.1921159327030182\n",
      "Training loss: 0.3135795593261719\n",
      "Training loss: 0.3214774429798126\n",
      "Training loss: 0.3179355561733246\n",
      "Training loss: 0.39850053191185\n",
      "Training loss: 0.29213783144950867\n",
      "Training loss: 0.29023993015289307\n",
      "Training loss: 0.3588188886642456\n",
      "Training loss: 0.2539941966533661\n",
      "Training loss: 0.21068017184734344\n",
      "Training loss: 0.23958070576190948\n",
      "Training loss: 0.3015359342098236\n",
      "Training loss: 0.1819368302822113\n",
      "Training loss: 0.33867478370666504\n",
      "Training loss: 0.28156301379203796\n",
      "Training loss: 0.30585524439811707\n",
      "Training loss: 0.3149110972881317\n",
      "Training loss: 0.36025092005729675\n",
      "Training loss: 0.31725892424583435\n",
      "Training loss: 0.27337512373924255\n",
      "Training loss: 0.3242497742176056\n",
      "Training loss: 0.2911130487918854\n",
      "Training loss: 0.28432637453079224\n",
      "Training loss: 0.3335491716861725\n",
      "Training loss: 0.32359805703163147\n",
      "Training loss: 0.19657032191753387\n",
      "Training loss: 0.4212315082550049\n",
      "Training loss: 0.3446767330169678\n",
      "Training loss: 0.2773404121398926\n",
      "Training loss: 0.4072791635990143\n",
      "Training loss: 0.2719719707965851\n",
      "Training loss: 0.256233274936676\n",
      "Training loss: 0.34732502698898315\n",
      "Training loss: 0.28952234983444214\n",
      "Training loss: 0.43598222732543945\n",
      "Training loss: 0.3187189996242523\n",
      "Training loss: 0.27596408128738403\n",
      "Training loss: 0.3594090938568115\n",
      "Training loss: 0.3490341305732727\n",
      "Training loss: 0.33704954385757446\n",
      "Training loss: 0.30120015144348145\n",
      "Training loss: 0.3188900053501129\n",
      "Training loss: 0.3196672797203064\n",
      "Training loss: 0.4107513427734375\n",
      "Training loss: 0.35643938183784485\n",
      "Training loss: 0.4319906234741211\n",
      "Training loss: 0.3063346743583679\n",
      "Training loss: 0.2596558928489685\n",
      "Training loss: 0.29784420132637024\n",
      "Training loss: 0.3139909505844116\n",
      "Training loss: 0.27415356040000916\n",
      "Training loss: 0.280588299036026\n",
      "Training loss: 0.271413117647171\n",
      "Training loss: 0.2603501081466675\n",
      "Training loss: 0.24277067184448242\n",
      "Training loss: 0.19211100041866302\n",
      "Training loss: 0.3135778307914734\n",
      "Training loss: 0.3214820921421051\n",
      "Training loss: 0.3179328441619873\n",
      "Training loss: 0.3985013961791992\n",
      "Training loss: 0.2921377420425415\n",
      "Training loss: 0.2902407646179199\n",
      "Training loss: 0.35882145166397095\n",
      "Training loss: 0.2539922893047333\n",
      "Training loss: 0.21067672967910767\n",
      "Training loss: 0.23957744240760803\n",
      "Training loss: 0.3015352487564087\n",
      "Training loss: 0.181931734085083\n",
      "Training loss: 0.3386770188808441\n",
      "Training loss: 0.2815632224082947\n",
      "Training loss: 0.3058526813983917\n",
      "Training loss: 0.31491294503211975\n",
      "Training loss: 0.3602542281150818\n",
      "Training loss: 0.31726112961769104\n",
      "Training loss: 0.2733723819255829\n",
      "Training loss: 0.32424965500831604\n",
      "Training loss: 0.29111248254776\n",
      "Training loss: 0.2843252718448639\n",
      "Training loss: 0.3335498571395874\n",
      "Training loss: 0.3235968053340912\n",
      "Training loss: 0.19656549394130707\n",
      "Training loss: 0.4212380349636078\n",
      "Training loss: 0.3446795642375946\n",
      "Training loss: 0.2773378789424896\n",
      "Training loss: 0.4072834849357605\n",
      "Training loss: 0.2719702422618866\n",
      "Training loss: 0.25623002648353577\n",
      "Training loss: 0.3473280668258667\n",
      "Training loss: 0.2895193099975586\n",
      "Training loss: 0.4359937608242035\n",
      "Training loss: 0.3187211751937866\n",
      "Training loss: 0.27595964074134827\n",
      "Training loss: 0.35941293835639954\n",
      "Training loss: 0.34903573989868164\n",
      "Training loss: 0.33705052733421326\n",
      "Training loss: 0.30119988322257996\n",
      "Training loss: 0.3188902735710144\n",
      "Training loss: 0.3196675181388855\n",
      "Training loss: 0.4107572138309479\n",
      "Training loss: 0.3564411401748657\n",
      "Training loss: 0.4319944679737091\n",
      "Training loss: 0.30633482336997986\n",
      "Training loss: 0.2596537470817566\n",
      "Training loss: 0.29784274101257324\n",
      "Training loss: 0.3139899969100952\n",
      "Training loss: 0.27415400743484497\n",
      "Training loss: 0.28059065341949463\n",
      "Training loss: 0.27141445875167847\n",
      "Training loss: 0.2603524327278137\n",
      "Training loss: 0.24277207255363464\n",
      "Training loss: 0.19210641086101532\n",
      "Training loss: 0.3135761618614197\n",
      "Training loss: 0.3214864730834961\n",
      "Training loss: 0.31793028116226196\n",
      "Training loss: 0.39850205183029175\n",
      "Training loss: 0.29213768243789673\n",
      "Training loss: 0.290241539478302\n",
      "Training loss: 0.3588239550590515\n",
      "Training loss: 0.25399050116539\n",
      "Training loss: 0.21067355573177338\n",
      "Training loss: 0.2395743876695633\n",
      "Training loss: 0.3015345633029938\n",
      "Training loss: 0.18192701041698456\n",
      "Training loss: 0.3386789858341217\n",
      "Training loss: 0.28156328201293945\n",
      "Training loss: 0.3058502674102783\n",
      "Training loss: 0.31491461396217346\n",
      "Training loss: 0.36025750637054443\n",
      "Training loss: 0.3172631561756134\n",
      "Training loss: 0.27336978912353516\n",
      "Training loss: 0.3242496848106384\n",
      "Training loss: 0.2911119759082794\n",
      "Training loss: 0.2843242883682251\n",
      "Training loss: 0.3335505425930023\n",
      "Training loss: 0.3235955238342285\n",
      "Training loss: 0.19656096398830414\n",
      "Training loss: 0.42124423384666443\n",
      "Training loss: 0.3446822166442871\n",
      "Training loss: 0.2773354649543762\n",
      "Training loss: 0.40728759765625\n",
      "Training loss: 0.2719687223434448\n",
      "Training loss: 0.2562273144721985\n",
      "Training loss: 0.34733086824417114\n",
      "Training loss: 0.2895165681838989\n",
      "Training loss: 0.4360044598579407\n",
      "Training loss: 0.31872323155403137\n",
      "Training loss: 0.27595534920692444\n",
      "Training loss: 0.35941675305366516\n",
      "Training loss: 0.34903720021247864\n",
      "Training loss: 0.3370514214038849\n",
      "Training loss: 0.30119964480400085\n",
      "Training loss: 0.3188905715942383\n",
      "Training loss: 0.3196674585342407\n",
      "Training loss: 0.410762757062912\n",
      "Training loss: 0.3564428389072418\n",
      "Training loss: 0.4319981336593628\n",
      "Training loss: 0.30633506178855896\n",
      "Training loss: 0.25965166091918945\n",
      "Training loss: 0.2978414297103882\n",
      "Training loss: 0.31398913264274597\n",
      "Training loss: 0.2741543650627136\n",
      "Training loss: 0.2805928885936737\n",
      "Training loss: 0.27141574025154114\n",
      "Training loss: 0.26035475730895996\n",
      "Training loss: 0.24277327954769135\n",
      "Training loss: 0.19210216403007507\n",
      "Training loss: 0.31357458233833313\n",
      "Training loss: 0.32149070501327515\n",
      "Training loss: 0.31792789697647095\n",
      "Training loss: 0.39850255846977234\n",
      "Training loss: 0.29213768243789673\n",
      "Training loss: 0.29024237394332886\n",
      "Training loss: 0.35882604122161865\n",
      "Training loss: 0.2539888322353363\n",
      "Training loss: 0.2106705754995346\n",
      "Training loss: 0.23957154154777527\n",
      "Training loss: 0.301533967256546\n",
      "Training loss: 0.18192258477210999\n",
      "Training loss: 0.3386809229850769\n",
      "Training loss: 0.28156352043151855\n",
      "Training loss: 0.3058479130268097\n",
      "Training loss: 0.3149162530899048\n",
      "Training loss: 0.360260546207428\n",
      "Training loss: 0.3172651529312134\n",
      "Training loss: 0.27336734533309937\n",
      "Training loss: 0.3242497146129608\n",
      "Training loss: 0.29111140966415405\n",
      "Training loss: 0.2843233644962311\n",
      "Training loss: 0.33355116844177246\n",
      "Training loss: 0.32359427213668823\n",
      "Training loss: 0.1965566724538803\n",
      "Training loss: 0.4212500751018524\n",
      "Training loss: 0.34468457102775574\n",
      "Training loss: 0.2773331105709076\n",
      "Training loss: 0.407291442155838\n",
      "Training loss: 0.2719673216342926\n",
      "Training loss: 0.2562248408794403\n",
      "Training loss: 0.34733349084854126\n",
      "Training loss: 0.28951379656791687\n",
      "Training loss: 0.4360148310661316\n",
      "Training loss: 0.318725049495697\n",
      "Training loss: 0.2759513556957245\n",
      "Training loss: 0.35942018032073975\n",
      "Training loss: 0.34903866052627563\n",
      "Training loss: 0.3370523154735565\n",
      "Training loss: 0.30119940638542175\n",
      "Training loss: 0.31889069080352783\n",
      "Training loss: 0.3196675479412079\n",
      "Training loss: 0.4107680916786194\n",
      "Training loss: 0.35644423961639404\n",
      "Training loss: 0.4320015609264374\n",
      "Training loss: 0.30633512139320374\n",
      "Training loss: 0.25964975357055664\n",
      "Training loss: 0.2978401184082031\n",
      "Training loss: 0.3139883279800415\n",
      "Training loss: 0.27415478229522705\n",
      "Training loss: 0.28059500455856323\n",
      "Training loss: 0.2714170217514038\n",
      "Training loss: 0.2603570520877838\n",
      "Training loss: 0.24277454614639282\n",
      "Training loss: 0.19209817051887512\n",
      "Training loss: 0.3135730028152466\n",
      "Training loss: 0.3214947283267975\n",
      "Training loss: 0.3179255723953247\n",
      "Training loss: 0.3985030949115753\n",
      "Training loss: 0.2921377718448639\n",
      "Training loss: 0.2902431786060333\n",
      "Training loss: 0.358828067779541\n",
      "Training loss: 0.253987193107605\n",
      "Training loss: 0.21066787838935852\n",
      "Training loss: 0.23956890404224396\n",
      "Training loss: 0.30153340101242065\n",
      "Training loss: 0.1819184422492981\n",
      "Training loss: 0.33868277072906494\n",
      "Training loss: 0.2815636098384857\n",
      "Training loss: 0.30584579706192017\n",
      "Training loss: 0.3149177134037018\n",
      "Training loss: 0.36026349663734436\n",
      "Training loss: 0.3172670006752014\n",
      "Training loss: 0.27336496114730835\n",
      "Training loss: 0.32424962520599365\n",
      "Training loss: 0.29111096262931824\n",
      "Training loss: 0.2843225598335266\n",
      "Training loss: 0.3335517942905426\n",
      "Training loss: 0.32359299063682556\n",
      "Training loss: 0.19655261933803558\n",
      "Training loss: 0.4212556481361389\n",
      "Training loss: 0.344686895608902\n",
      "Training loss: 0.2773308753967285\n",
      "Training loss: 0.4072951078414917\n",
      "Training loss: 0.27196595072746277\n",
      "Training loss: 0.2562224864959717\n",
      "Training loss: 0.34733593463897705\n",
      "Training loss: 0.2895112931728363\n",
      "Training loss: 0.43602454662323\n",
      "Training loss: 0.3187270164489746\n",
      "Training loss: 0.2759475111961365\n",
      "Training loss: 0.35942342877388\n",
      "Training loss: 0.3490399122238159\n",
      "Training loss: 0.33705323934555054\n",
      "Training loss: 0.30119916796684265\n",
      "Training loss: 0.3188909590244293\n",
      "Training loss: 0.31966766715049744\n",
      "Training loss: 0.41077297925949097\n",
      "Training loss: 0.3564457893371582\n",
      "Training loss: 0.4320046901702881\n",
      "Training loss: 0.3063352704048157\n",
      "Training loss: 0.259647935628891\n",
      "Training loss: 0.29783895611763\n",
      "Training loss: 0.3139875531196594\n",
      "Training loss: 0.2741551697254181\n",
      "Training loss: 0.280597060918808\n",
      "Training loss: 0.2714182734489441\n",
      "Training loss: 0.2603592574596405\n",
      "Training loss: 0.2427755892276764\n",
      "Training loss: 0.19209450483322144\n",
      "Training loss: 0.3135714530944824\n",
      "Training loss: 0.3214986026287079\n",
      "Training loss: 0.3179234266281128\n",
      "Training loss: 0.3985036611557007\n",
      "Training loss: 0.29213786125183105\n",
      "Training loss: 0.2902439534664154\n",
      "Training loss: 0.3588303029537201\n",
      "Training loss: 0.253985732793808\n",
      "Training loss: 0.21066531538963318\n",
      "Training loss: 0.23956646025180817\n",
      "Training loss: 0.3015328347682953\n",
      "Training loss: 0.18191461265087128\n",
      "Training loss: 0.3386845290660858\n",
      "Training loss: 0.2815638482570648\n",
      "Training loss: 0.30584365129470825\n",
      "Training loss: 0.3149191737174988\n",
      "Training loss: 0.36026638746261597\n",
      "Training loss: 0.3172687888145447\n",
      "Training loss: 0.27336275577545166\n",
      "Training loss: 0.3242495357990265\n",
      "Training loss: 0.2911105453968048\n",
      "Training loss: 0.284321665763855\n",
      "Training loss: 0.3335524797439575\n",
      "Training loss: 0.32359176874160767\n",
      "Training loss: 0.19654883444309235\n",
      "Training loss: 0.42126089334487915\n",
      "Training loss: 0.3446890413761139\n",
      "Training loss: 0.2773286700248718\n",
      "Training loss: 0.40729856491088867\n",
      "Training loss: 0.27196481823921204\n",
      "Training loss: 0.25622013211250305\n",
      "Training loss: 0.34733831882476807\n",
      "Training loss: 0.2895088493824005\n",
      "Training loss: 0.436034232378006\n",
      "Training loss: 0.31872862577438354\n",
      "Training loss: 0.27594393491744995\n",
      "Training loss: 0.3594265580177307\n",
      "Training loss: 0.3490411341190338\n",
      "Training loss: 0.3370540738105774\n",
      "Training loss: 0.30119892954826355\n",
      "Training loss: 0.31889107823371887\n",
      "Training loss: 0.3196677565574646\n",
      "Training loss: 0.41077762842178345\n",
      "Training loss: 0.3564470410346985\n",
      "Training loss: 0.432007759809494\n",
      "Training loss: 0.30633533000946045\n",
      "Training loss: 0.2596462368965149\n",
      "Training loss: 0.2978379428386688\n",
      "Training loss: 0.31398695707321167\n",
      "Training loss: 0.27415555715560913\n",
      "Training loss: 0.28059902787208557\n",
      "Training loss: 0.27141958475112915\n",
      "Training loss: 0.2603614330291748\n",
      "Training loss: 0.24277696013450623\n",
      "Training loss: 0.19209106266498566\n",
      "Training loss: 0.3135700523853302\n",
      "Training loss: 0.3215022683143616\n",
      "Training loss: 0.31792134046554565\n",
      "Training loss: 0.39850401878356934\n",
      "Training loss: 0.2921379506587982\n",
      "Training loss: 0.2902446985244751\n",
      "Training loss: 0.35883191227912903\n",
      "Training loss: 0.25398436188697815\n",
      "Training loss: 0.21066296100616455\n",
      "Training loss: 0.2395642101764679\n",
      "Training loss: 0.3015323579311371\n",
      "Training loss: 0.1819109320640564\n",
      "Training loss: 0.3386860489845276\n",
      "Training loss: 0.2815641164779663\n",
      "Training loss: 0.3058416545391083\n",
      "Training loss: 0.314920574426651\n",
      "Training loss: 0.36026909947395325\n",
      "Training loss: 0.31727054715156555\n",
      "Training loss: 0.2733606696128845\n",
      "Training loss: 0.32424959540367126\n",
      "Training loss: 0.29111018776893616\n",
      "Training loss: 0.2843208909034729\n",
      "Training loss: 0.3335530161857605\n",
      "Training loss: 0.32359060645103455\n",
      "Training loss: 0.19654519855976105\n",
      "Training loss: 0.42126595973968506\n",
      "Training loss: 0.34469109773635864\n",
      "Training loss: 0.2773265838623047\n",
      "Training loss: 0.40730172395706177\n",
      "Training loss: 0.2719636857509613\n",
      "Training loss: 0.25621774792671204\n",
      "Training loss: 0.34734052419662476\n",
      "Training loss: 0.28950658440589905\n",
      "Training loss: 0.43604305386543274\n",
      "Training loss: 0.3187302350997925\n",
      "Training loss: 0.275940477848053\n",
      "Training loss: 0.35942941904067993\n",
      "Training loss: 0.34904226660728455\n",
      "Training loss: 0.33705490827560425\n",
      "Training loss: 0.3011987805366516\n",
      "Training loss: 0.3188912272453308\n",
      "Training loss: 0.31966787576675415\n",
      "Training loss: 0.4107819199562073\n",
      "Training loss: 0.3564482629299164\n",
      "Training loss: 0.4320105016231537\n",
      "Training loss: 0.3063353896141052\n",
      "Training loss: 0.2596445679664612\n",
      "Training loss: 0.2978368401527405\n",
      "Training loss: 0.3139863610267639\n",
      "Training loss: 0.27415597438812256\n",
      "Training loss: 0.280600905418396\n",
      "Training loss: 0.27142074704170227\n",
      "Training loss: 0.26036354899406433\n",
      "Training loss: 0.2427782118320465\n",
      "Training loss: 0.1920877993106842\n",
      "Training loss: 0.3135685622692108\n",
      "Training loss: 0.32150572538375854\n",
      "Training loss: 0.31791940331459045\n",
      "Training loss: 0.3985043466091156\n",
      "Training loss: 0.29213809967041016\n",
      "Training loss: 0.2902454137802124\n",
      "Training loss: 0.35883378982543945\n",
      "Training loss: 0.2539830505847931\n",
      "Training loss: 0.21066072583198547\n",
      "Training loss: 0.2395620495080948\n",
      "Training loss: 0.3015318810939789\n",
      "Training loss: 0.181907519698143\n",
      "Training loss: 0.338687539100647\n",
      "Training loss: 0.28156426548957825\n",
      "Training loss: 0.30583977699279785\n",
      "Training loss: 0.31492188572883606\n",
      "Training loss: 0.3602716326713562\n",
      "Training loss: 0.3172721266746521\n",
      "Training loss: 0.27335864305496216\n",
      "Training loss: 0.3242494761943817\n",
      "Training loss: 0.29110974073410034\n",
      "Training loss: 0.28432023525238037\n",
      "Training loss: 0.33355358242988586\n",
      "Training loss: 0.3235894739627838\n",
      "Training loss: 0.19654180109500885\n",
      "Training loss: 0.4212706983089447\n",
      "Training loss: 0.34469300508499146\n",
      "Training loss: 0.2773245871067047\n",
      "Training loss: 0.4073047637939453\n",
      "Training loss: 0.2719627022743225\n",
      "Training loss: 0.2562158703804016\n",
      "Training loss: 0.34734266996383667\n",
      "Training loss: 0.28950437903404236\n",
      "Training loss: 0.43605169653892517\n",
      "Training loss: 0.31873181462287903\n",
      "Training loss: 0.2759372293949127\n",
      "Training loss: 0.35943225026130676\n",
      "Training loss: 0.3490433692932129\n",
      "Training loss: 0.33705559372901917\n",
      "Training loss: 0.3011985719203949\n",
      "Training loss: 0.3188914656639099\n",
      "Training loss: 0.319667786359787\n",
      "Training loss: 0.41078609228134155\n",
      "Training loss: 0.3564494550228119\n",
      "Training loss: 0.43201324343681335\n",
      "Training loss: 0.3063354194164276\n",
      "Training loss: 0.2596430480480194\n",
      "Training loss: 0.2978358864784241\n",
      "Training loss: 0.3139854371547699\n",
      "Training loss: 0.2741563022136688\n",
      "Training loss: 0.28060272336006165\n",
      "Training loss: 0.2714219391345978\n",
      "Training loss: 0.26036548614501953\n",
      "Training loss: 0.2427792251110077\n",
      "Training loss: 0.19208475947380066\n",
      "Training loss: 0.31356725096702576\n",
      "Training loss: 0.32150912284851074\n",
      "Training loss: 0.31791752576828003\n",
      "Training loss: 0.39850473403930664\n",
      "Training loss: 0.2921382188796997\n",
      "Training loss: 0.2902461588382721\n",
      "Training loss: 0.3588355779647827\n",
      "Training loss: 0.2539817690849304\n",
      "Training loss: 0.21065863966941833\n",
      "Training loss: 0.23956000804901123\n",
      "Training loss: 0.30153146386146545\n",
      "Training loss: 0.18190427124500275\n",
      "Training loss: 0.33868905901908875\n",
      "Training loss: 0.28156447410583496\n",
      "Training loss: 0.30583810806274414\n",
      "Training loss: 0.31492307782173157\n",
      "Training loss: 0.360274076461792\n",
      "Training loss: 0.31727367639541626\n",
      "Training loss: 0.27335673570632935\n",
      "Training loss: 0.32424938678741455\n",
      "Training loss: 0.29110953211784363\n",
      "Training loss: 0.2843194901943207\n",
      "Training loss: 0.33355408906936646\n",
      "Training loss: 0.32358843088150024\n",
      "Training loss: 0.19653856754302979\n",
      "Training loss: 0.42127519845962524\n",
      "Training loss: 0.34469491243362427\n",
      "Training loss: 0.27732276916503906\n",
      "Training loss: 0.4073077142238617\n",
      "Training loss: 0.27196168899536133\n",
      "Training loss: 0.25621360540390015\n",
      "Training loss: 0.34734460711479187\n",
      "Training loss: 0.2895022928714752\n",
      "Training loss: 0.4360595643520355\n",
      "Training loss: 0.3187333345413208\n",
      "Training loss: 0.2759341299533844\n",
      "Training loss: 0.3594347834587097\n",
      "Training loss: 0.3490443825721741\n",
      "Training loss: 0.33705633878707886\n",
      "Training loss: 0.3011983633041382\n",
      "Training loss: 0.31889158487319946\n",
      "Training loss: 0.31966787576675415\n",
      "Training loss: 0.41078999638557434\n",
      "Training loss: 0.3564505875110626\n",
      "Training loss: 0.4320157766342163\n",
      "Training loss: 0.3063353896141052\n",
      "Training loss: 0.2596416473388672\n",
      "Training loss: 0.29783499240875244\n",
      "Training loss: 0.3139849603176117\n",
      "Training loss: 0.2741566598415375\n",
      "Training loss: 0.28060442209243774\n",
      "Training loss: 0.2714231014251709\n",
      "Training loss: 0.2603674829006195\n",
      "Training loss: 0.2427804172039032\n",
      "Training loss: 0.19208189845085144\n",
      "Training loss: 0.3135659396648407\n",
      "Training loss: 0.3215123116970062\n",
      "Training loss: 0.3179156482219696\n",
      "Training loss: 0.39850497245788574\n",
      "Training loss: 0.29213836789131165\n",
      "Training loss: 0.290246844291687\n",
      "Training loss: 0.3588370084762573\n",
      "Training loss: 0.2539806365966797\n",
      "Training loss: 0.21065668761730194\n",
      "Training loss: 0.2395581305027008\n",
      "Training loss: 0.30153101682662964\n",
      "Training loss: 0.1819012612104416\n",
      "Training loss: 0.3386903405189514\n",
      "Training loss: 0.28156471252441406\n",
      "Training loss: 0.3058362603187561\n",
      "Training loss: 0.31492429971694946\n",
      "Training loss: 0.36027631163597107\n",
      "Training loss: 0.31727519631385803\n",
      "Training loss: 0.2733549177646637\n",
      "Training loss: 0.3242495656013489\n",
      "Training loss: 0.29110923409461975\n",
      "Training loss: 0.28431886434555054\n",
      "Training loss: 0.3335546553134918\n",
      "Training loss: 0.3235872983932495\n",
      "Training loss: 0.19653548300266266\n",
      "Training loss: 0.4212794601917267\n",
      "Training loss: 0.34469661116600037\n",
      "Training loss: 0.2773209512233734\n",
      "Training loss: 0.40731045603752136\n",
      "Training loss: 0.2719607949256897\n",
      "Training loss: 0.25621163845062256\n",
      "Training loss: 0.3473464548587799\n",
      "Training loss: 0.28950035572052\n",
      "Training loss: 0.4360671043395996\n",
      "Training loss: 0.31873470544815063\n",
      "Training loss: 0.2759312391281128\n",
      "Training loss: 0.35943734645843506\n",
      "Training loss: 0.34904536604881287\n",
      "Training loss: 0.33705705404281616\n",
      "Training loss: 0.30119818449020386\n",
      "Training loss: 0.3188917934894562\n",
      "Training loss: 0.3196680545806885\n",
      "Training loss: 0.41079363226890564\n",
      "Training loss: 0.35645151138305664\n",
      "Training loss: 0.4320182502269745\n",
      "Training loss: 0.3063354194164276\n",
      "Training loss: 0.25964024662971497\n",
      "Training loss: 0.2978341281414032\n",
      "Training loss: 0.3139841556549072\n",
      "Training loss: 0.27415701746940613\n",
      "Training loss: 0.28060609102249146\n",
      "Training loss: 0.271424263715744\n",
      "Training loss: 0.26036936044692993\n",
      "Training loss: 0.2427813708782196\n",
      "Training loss: 0.19207923114299774\n",
      "Training loss: 0.3135646879673004\n",
      "Training loss: 0.32151538133621216\n",
      "Training loss: 0.3179139792919159\n",
      "Training loss: 0.3985053300857544\n",
      "Training loss: 0.29213857650756836\n",
      "Training loss: 0.2902475595474243\n",
      "Training loss: 0.3588385581970215\n",
      "Training loss: 0.25397950410842896\n",
      "Training loss: 0.21065488457679749\n",
      "Training loss: 0.23955637216567993\n",
      "Training loss: 0.301530659198761\n",
      "Training loss: 0.18189837038516998\n",
      "Training loss: 0.3386915922164917\n",
      "Training loss: 0.28156495094299316\n",
      "Training loss: 0.30583471059799194\n",
      "Training loss: 0.3149254322052002\n",
      "Training loss: 0.3602786958217621\n",
      "Training loss: 0.31727662682533264\n",
      "Training loss: 0.2733531892299652\n",
      "Training loss: 0.3242494761943817\n",
      "Training loss: 0.29110899567604065\n",
      "Training loss: 0.2843182682991028\n",
      "Training loss: 0.3335552215576172\n",
      "Training loss: 0.32358625531196594\n",
      "Training loss: 0.19653260707855225\n",
      "Training loss: 0.4212835431098938\n",
      "Training loss: 0.3446981608867645\n",
      "Training loss: 0.27731919288635254\n",
      "Training loss: 0.4073130786418915\n",
      "Training loss: 0.2719600200653076\n",
      "Training loss: 0.2562099099159241\n",
      "Training loss: 0.34734827280044556\n",
      "Training loss: 0.28949853777885437\n",
      "Training loss: 0.4360744059085846\n",
      "Training loss: 0.3187360167503357\n",
      "Training loss: 0.27592840790748596\n",
      "Training loss: 0.3594396710395813\n",
      "Training loss: 0.3490462899208069\n",
      "Training loss: 0.33705776929855347\n",
      "Training loss: 0.3011980354785919\n",
      "Training loss: 0.31889188289642334\n",
      "Training loss: 0.3196680545806885\n",
      "Training loss: 0.41079720854759216\n",
      "Training loss: 0.3564525842666626\n",
      "Training loss: 0.4320204555988312\n",
      "Training loss: 0.3063354194164276\n",
      "Training loss: 0.2596389353275299\n",
      "Training loss: 0.29783332347869873\n",
      "Training loss: 0.3139837384223938\n",
      "Training loss: 0.2741573452949524\n",
      "Training loss: 0.280607670545578\n",
      "Training loss: 0.2714253067970276\n",
      "Training loss: 0.26037120819091797\n",
      "Training loss: 0.24278251826763153\n",
      "Training loss: 0.19207671284675598\n",
      "Training loss: 0.3135634958744049\n",
      "Training loss: 0.32151827216148376\n",
      "Training loss: 0.31791236996650696\n",
      "Training loss: 0.39850541949272156\n",
      "Training loss: 0.2921387553215027\n",
      "Training loss: 0.29024821519851685\n",
      "Training loss: 0.35883989930152893\n",
      "Training loss: 0.2539784014225006\n",
      "Training loss: 0.2106531858444214\n",
      "Training loss: 0.2395547330379486\n",
      "Training loss: 0.30153030157089233\n",
      "Training loss: 0.1818956434726715\n",
      "Training loss: 0.338692843914032\n",
      "Training loss: 0.28156521916389465\n",
      "Training loss: 0.30583319067955017\n",
      "Training loss: 0.31492650508880615\n",
      "Training loss: 0.3602806329727173\n",
      "Training loss: 0.3172779679298401\n",
      "Training loss: 0.27335160970687866\n",
      "Training loss: 0.3242494463920593\n",
      "Training loss: 0.2911086082458496\n",
      "Training loss: 0.2843177020549774\n",
      "Training loss: 0.333555668592453\n",
      "Training loss: 0.3235852122306824\n",
      "Training loss: 0.1965298354625702\n",
      "Training loss: 0.4212873578071594\n",
      "Training loss: 0.3446997404098511\n",
      "Training loss: 0.2773175835609436\n",
      "Training loss: 0.40731555223464966\n",
      "Training loss: 0.27195924520492554\n",
      "Training loss: 0.2562083899974823\n",
      "Training loss: 0.34734994173049927\n",
      "Training loss: 0.2894967198371887\n",
      "Training loss: 0.4360814094543457\n",
      "Training loss: 0.3187372386455536\n",
      "Training loss: 0.27592581510543823\n",
      "Training loss: 0.35944196581840515\n",
      "Training loss: 0.3490472137928009\n",
      "Training loss: 0.33705833554267883\n",
      "Training loss: 0.30119794607162476\n",
      "Training loss: 0.31889212131500244\n",
      "Training loss: 0.31966814398765564\n",
      "Training loss: 0.4108004570007324\n",
      "Training loss: 0.35645344853401184\n",
      "Training loss: 0.4320226013660431\n",
      "Training loss: 0.30633535981178284\n",
      "Training loss: 0.25963765382766724\n",
      "Training loss: 0.29783254861831665\n",
      "Training loss: 0.31398311257362366\n",
      "Training loss: 0.27415770292282104\n",
      "Training loss: 0.2806091904640198\n",
      "Training loss: 0.27142640948295593\n",
      "Training loss: 0.26037290692329407\n",
      "Training loss: 0.24278347194194794\n",
      "Training loss: 0.19207431375980377\n",
      "Training loss: 0.3135623335838318\n",
      "Training loss: 0.3215210735797882\n",
      "Training loss: 0.3179108500480652\n",
      "Training loss: 0.39850565791130066\n",
      "Training loss: 0.2921389043331146\n",
      "Training loss: 0.290248841047287\n",
      "Training loss: 0.3588411808013916\n",
      "Training loss: 0.2539774477481842\n",
      "Training loss: 0.21065154671669006\n",
      "Training loss: 0.23955313861370087\n",
      "Training loss: 0.30152997374534607\n",
      "Training loss: 0.18189311027526855\n",
      "Training loss: 0.3386940062046051\n",
      "Training loss: 0.2815653383731842\n",
      "Training loss: 0.305831640958786\n",
      "Training loss: 0.31492748856544495\n",
      "Training loss: 0.3602827191352844\n",
      "Training loss: 0.31727924942970276\n",
      "Training loss: 0.2733500301837921\n",
      "Training loss: 0.32424941658973694\n",
      "Training loss: 0.2911084294319153\n",
      "Training loss: 0.28431713581085205\n",
      "Training loss: 0.33355608582496643\n",
      "Training loss: 0.32358428835868835\n",
      "Training loss: 0.19652722775936127\n",
      "Training loss: 0.4212910532951355\n",
      "Training loss: 0.3447011113166809\n",
      "Training loss: 0.27731597423553467\n",
      "Training loss: 0.4073178768157959\n",
      "Training loss: 0.27195852994918823\n",
      "Training loss: 0.25620678067207336\n",
      "Training loss: 0.3473515510559082\n",
      "Training loss: 0.289495050907135\n",
      "Training loss: 0.43608787655830383\n",
      "Training loss: 0.3187384605407715\n",
      "Training loss: 0.2759232819080353\n",
      "Training loss: 0.3594440221786499\n",
      "Training loss: 0.34904801845550537\n",
      "Training loss: 0.33705899119377136\n",
      "Training loss: 0.30119776725769043\n",
      "Training loss: 0.3188921809196472\n",
      "Training loss: 0.3196682631969452\n",
      "Training loss: 0.4108033776283264\n",
      "Training loss: 0.35645434260368347\n",
      "Training loss: 0.43202459812164307\n",
      "Training loss: 0.3063353896141052\n",
      "Training loss: 0.2596365213394165\n",
      "Training loss: 0.29783186316490173\n",
      "Training loss: 0.31398242712020874\n",
      "Training loss: 0.2741580009460449\n",
      "Training loss: 0.2806106209754944\n",
      "Training loss: 0.2714274525642395\n",
      "Training loss: 0.26037463545799255\n",
      "Training loss: 0.2427845597267151\n",
      "Training loss: 0.1920720338821411\n",
      "Training loss: 0.31356123089790344\n",
      "Training loss: 0.3215236961841583\n",
      "Training loss: 0.3179093599319458\n",
      "Training loss: 0.39850589632987976\n",
      "Training loss: 0.29213908314704895\n",
      "Training loss: 0.29024943709373474\n",
      "Training loss: 0.3588424324989319\n",
      "Training loss: 0.2539765536785126\n",
      "Training loss: 0.2106500118970871\n",
      "Training loss: 0.23955166339874268\n",
      "Training loss: 0.3015296459197998\n",
      "Training loss: 0.18189062178134918\n",
      "Training loss: 0.33869504928588867\n",
      "Training loss: 0.28156569600105286\n",
      "Training loss: 0.30583029985427856\n",
      "Training loss: 0.31492850184440613\n",
      "Training loss: 0.36028462648391724\n",
      "Training loss: 0.31728050112724304\n",
      "Training loss: 0.27334854006767273\n",
      "Training loss: 0.32424938678741455\n",
      "Training loss: 0.29110807180404663\n",
      "Training loss: 0.28431668877601624\n",
      "Training loss: 0.33355650305747986\n",
      "Training loss: 0.3235832452774048\n",
      "Training loss: 0.1965247392654419\n",
      "Training loss: 0.421294629573822\n",
      "Training loss: 0.3447025716304779\n",
      "Training loss: 0.2773144543170929\n",
      "Training loss: 0.4073200225830078\n",
      "Training loss: 0.2719579041004181\n",
      "Training loss: 0.25620532035827637\n",
      "Training loss: 0.3473530411720276\n",
      "Training loss: 0.2894934415817261\n",
      "Training loss: 0.4360945224761963\n",
      "Training loss: 0.3187396824359894\n",
      "Training loss: 0.27592089772224426\n",
      "Training loss: 0.35944604873657227\n",
      "Training loss: 0.34904879331588745\n",
      "Training loss: 0.3370595872402191\n",
      "Training loss: 0.30119767785072327\n",
      "Training loss: 0.31889232993125916\n",
      "Training loss: 0.3196684420108795\n",
      "Training loss: 0.4108063578605652\n",
      "Training loss: 0.35645511746406555\n",
      "Training loss: 0.4320265054702759\n",
      "Training loss: 0.3063353896141052\n",
      "Training loss: 0.25963538885116577\n",
      "Training loss: 0.29783111810684204\n",
      "Training loss: 0.313981831073761\n",
      "Training loss: 0.2741582989692688\n",
      "Training loss: 0.28061196208000183\n",
      "Training loss: 0.2714284062385559\n",
      "Training loss: 0.2603762447834015\n",
      "Training loss: 0.24278539419174194\n",
      "Training loss: 0.19206996262073517\n",
      "Training loss: 0.3135601878166199\n",
      "Training loss: 0.32152611017227173\n",
      "Training loss: 0.31790804862976074\n",
      "Training loss: 0.3985060453414917\n",
      "Training loss: 0.2921392321586609\n",
      "Training loss: 0.2902500033378601\n",
      "Training loss: 0.358843594789505\n",
      "Training loss: 0.25397568941116333\n",
      "Training loss: 0.21064859628677368\n",
      "Training loss: 0.23955024778842926\n",
      "Training loss: 0.30152931809425354\n",
      "Training loss: 0.1818883866071701\n",
      "Training loss: 0.3386959731578827\n",
      "Training loss: 0.28156590461730957\n",
      "Training loss: 0.3058290183544159\n",
      "Training loss: 0.31492939591407776\n",
      "Training loss: 0.3602864444255829\n",
      "Training loss: 0.31728166341781616\n",
      "Training loss: 0.2733471989631653\n",
      "Training loss: 0.32424938678741455\n",
      "Training loss: 0.2911078631877899\n",
      "Training loss: 0.28431618213653564\n",
      "Training loss: 0.3335569202899933\n",
      "Training loss: 0.3235824704170227\n",
      "Training loss: 0.19652239978313446\n",
      "Training loss: 0.42129790782928467\n",
      "Training loss: 0.3447038233280182\n",
      "Training loss: 0.2773129940032959\n",
      "Training loss: 0.40732207894325256\n",
      "Training loss: 0.27195730805397034\n",
      "Training loss: 0.2562038004398346\n",
      "Training loss: 0.3473544716835022\n",
      "Training loss: 0.2894919216632843\n",
      "Training loss: 0.43610039353370667\n",
      "Training loss: 0.3187406361103058\n",
      "Training loss: 0.27591872215270996\n",
      "Training loss: 0.3594478964805603\n",
      "Training loss: 0.3490495979785919\n",
      "Training loss: 0.3370601236820221\n",
      "Training loss: 0.30119752883911133\n",
      "Training loss: 0.3188924193382263\n",
      "Training loss: 0.31966841220855713\n",
      "Training loss: 0.41080906987190247\n",
      "Training loss: 0.35645586252212524\n",
      "Training loss: 0.43202829360961914\n",
      "Training loss: 0.3063353896141052\n",
      "Training loss: 0.2596343755722046\n",
      "Training loss: 0.2978305220603943\n",
      "Training loss: 0.3139812648296356\n",
      "Training loss: 0.2741585969924927\n",
      "Training loss: 0.2806133031845093\n",
      "Training loss: 0.2714293897151947\n",
      "Training loss: 0.26037779450416565\n",
      "Training loss: 0.2427864372730255\n",
      "Training loss: 0.1920679807662964\n",
      "Training loss: 0.3135591745376587\n",
      "Training loss: 0.32152849435806274\n",
      "Training loss: 0.3179067373275757\n",
      "Training loss: 0.39850613474845886\n",
      "Training loss: 0.2921394407749176\n",
      "Training loss: 0.2902505397796631\n",
      "Training loss: 0.3588447570800781\n",
      "Training loss: 0.2539748549461365\n",
      "Training loss: 0.21064728498458862\n",
      "Training loss: 0.23954898118972778\n",
      "Training loss: 0.30152902007102966\n",
      "Training loss: 0.18188625574111938\n",
      "Training loss: 0.3386969566345215\n",
      "Training loss: 0.28156617283821106\n",
      "Training loss: 0.30582770705223083\n",
      "Training loss: 0.3149302303791046\n",
      "Training loss: 0.36028817296028137\n",
      "Training loss: 0.3172827959060669\n",
      "Training loss: 0.27334585785865784\n",
      "Training loss: 0.32424935698509216\n",
      "Training loss: 0.29110777378082275\n",
      "Training loss: 0.28431573510169983\n",
      "Training loss: 0.3335573375225067\n",
      "Training loss: 0.3235815167427063\n",
      "Training loss: 0.19652017951011658\n",
      "Training loss: 0.421301007270813\n",
      "Training loss: 0.3447050452232361\n",
      "Training loss: 0.27731165289878845\n",
      "Training loss: 0.40732404589653015\n",
      "Training loss: 0.27195674180984497\n",
      "Training loss: 0.25620242953300476\n",
      "Training loss: 0.34735584259033203\n",
      "Training loss: 0.2894904911518097\n",
      "Training loss: 0.4361061751842499\n",
      "Training loss: 0.3187417685985565\n",
      "Training loss: 0.27591657638549805\n",
      "Training loss: 0.35944974422454834\n",
      "Training loss: 0.34905022382736206\n",
      "Training loss: 0.3370606601238251\n",
      "Training loss: 0.30119746923446655\n",
      "Training loss: 0.31889253854751587\n",
      "Training loss: 0.3196685314178467\n",
      "Training loss: 0.4108116030693054\n",
      "Training loss: 0.3564566969871521\n",
      "Training loss: 0.43202996253967285\n",
      "Training loss: 0.30633533000946045\n",
      "Training loss: 0.2596334218978882\n",
      "Training loss: 0.2978300154209137\n",
      "Training loss: 0.3139810562133789\n",
      "Training loss: 0.2741588354110718\n",
      "Training loss: 0.2806145250797272\n",
      "Training loss: 0.2714303135871887\n",
      "Training loss: 0.26037922501564026\n",
      "Training loss: 0.24278736114501953\n",
      "Training loss: 0.19206616282463074\n",
      "Training loss: 0.3135582208633423\n",
      "Training loss: 0.3215307593345642\n",
      "Training loss: 0.3179054856300354\n",
      "Training loss: 0.3985062837600708\n",
      "Training loss: 0.29213958978652954\n",
      "Training loss: 0.29025110602378845\n",
      "Training loss: 0.3588458299636841\n",
      "Training loss: 0.2539741098880768\n",
      "Training loss: 0.21064603328704834\n",
      "Training loss: 0.2395477592945099\n",
      "Training loss: 0.3015287518501282\n",
      "Training loss: 0.1818842887878418\n",
      "Training loss: 0.3386978805065155\n",
      "Training loss: 0.281566321849823\n",
      "Training loss: 0.3058265149593353\n",
      "Training loss: 0.31493112444877625\n",
      "Training loss: 0.36028987169265747\n",
      "Training loss: 0.31728386878967285\n",
      "Training loss: 0.27334457635879517\n",
      "Training loss: 0.32424938678741455\n",
      "Training loss: 0.29110750555992126\n",
      "Training loss: 0.2843153476715088\n",
      "Training loss: 0.333557665348053\n",
      "Training loss: 0.32358065247535706\n",
      "Training loss: 0.19651809334754944\n",
      "Training loss: 0.42130404710769653\n",
      "Training loss: 0.344706267118454\n",
      "Training loss: 0.277310311794281\n",
      "Training loss: 0.4073259234428406\n",
      "Training loss: 0.2719562351703644\n",
      "Training loss: 0.25620123744010925\n",
      "Training loss: 0.3473570644855499\n",
      "Training loss: 0.28948917984962463\n",
      "Training loss: 0.4361112415790558\n",
      "Training loss: 0.3187427222728729\n",
      "Training loss: 0.2759145498275757\n",
      "Training loss: 0.3594514727592468\n",
      "Training loss: 0.34905093908309937\n",
      "Training loss: 0.33706116676330566\n",
      "Training loss: 0.301197350025177\n",
      "Training loss: 0.31889256834983826\n",
      "Training loss: 0.31966862082481384\n",
      "Training loss: 0.4108140766620636\n",
      "Training loss: 0.35645726323127747\n",
      "Training loss: 0.4320315718650818\n",
      "Training loss: 0.3063352704048157\n",
      "Training loss: 0.259632408618927\n",
      "Training loss: 0.29782938957214355\n",
      "Training loss: 0.3139805793762207\n",
      "Training loss: 0.27415913343429565\n",
      "Training loss: 0.28061574697494507\n",
      "Training loss: 0.27143120765686035\n",
      "Training loss: 0.26038074493408203\n",
      "Training loss: 0.24278812110424042\n",
      "Training loss: 0.19206437468528748\n",
      "Training loss: 0.3135572671890259\n",
      "Training loss: 0.3215329349040985\n",
      "Training loss: 0.3179042637348175\n",
      "Training loss: 0.39850640296936035\n",
      "Training loss: 0.29213976860046387\n",
      "Training loss: 0.29025161266326904\n",
      "Training loss: 0.3588469326496124\n",
      "Training loss: 0.2539733648300171\n",
      "Training loss: 0.21064487099647522\n",
      "Training loss: 0.23954662680625916\n",
      "Training loss: 0.30152854323387146\n",
      "Training loss: 0.1818823367357254\n",
      "Training loss: 0.33869868516921997\n",
      "Training loss: 0.28156647086143494\n",
      "Training loss: 0.305825412273407\n",
      "Training loss: 0.31493183970451355\n",
      "Training loss: 0.36029142141342163\n",
      "Training loss: 0.31728485226631165\n",
      "Training loss: 0.27334338426589966\n",
      "Training loss: 0.324249267578125\n",
      "Training loss: 0.29110732674598694\n",
      "Training loss: 0.2843148708343506\n",
      "Training loss: 0.33355802297592163\n",
      "Training loss: 0.323579877614975\n",
      "Training loss: 0.19651608169078827\n",
      "Training loss: 0.421306848526001\n",
      "Training loss: 0.3447073996067047\n",
      "Training loss: 0.2773090600967407\n",
      "Training loss: 0.40732768177986145\n",
      "Training loss: 0.2719557285308838\n",
      "Training loss: 0.2561998963356018\n",
      "Training loss: 0.34735825657844543\n",
      "Training loss: 0.2894878387451172\n",
      "Training loss: 0.43611663579940796\n",
      "Training loss: 0.31874358654022217\n",
      "Training loss: 0.2759125828742981\n",
      "Training loss: 0.35945308208465576\n",
      "Training loss: 0.3490515649318695\n",
      "Training loss: 0.33706167340278625\n",
      "Training loss: 0.30119723081588745\n",
      "Training loss: 0.31889256834983826\n",
      "Training loss: 0.3196685314178467\n",
      "Training loss: 0.410816490650177\n",
      "Training loss: 0.3564578890800476\n",
      "Training loss: 0.43203312158584595\n",
      "Training loss: 0.3063352704048157\n",
      "Training loss: 0.25963154435157776\n",
      "Training loss: 0.29782888293266296\n",
      "Training loss: 0.3139800727367401\n",
      "Training loss: 0.27415940165519714\n",
      "Training loss: 0.2806168496608734\n",
      "Training loss: 0.2714320421218872\n",
      "Training loss: 0.2603820264339447\n",
      "Training loss: 0.24278898537158966\n",
      "Training loss: 0.19206273555755615\n",
      "Training loss: 0.31355634331703186\n",
      "Training loss: 0.32153499126434326\n",
      "Training loss: 0.31790316104888916\n",
      "Training loss: 0.3985064625740051\n",
      "Training loss: 0.2921399474143982\n",
      "Training loss: 0.290252149105072\n",
      "Training loss: 0.35884764790534973\n",
      "Training loss: 0.2539726793766022\n",
      "Training loss: 0.21064376831054688\n",
      "Training loss: 0.2395455241203308\n",
      "Training loss: 0.30152827501296997\n",
      "Training loss: 0.18188051879405975\n",
      "Training loss: 0.3386995196342468\n",
      "Training loss: 0.2815667986869812\n",
      "Training loss: 0.30582430958747864\n",
      "Training loss: 0.31493261456489563\n",
      "Training loss: 0.360292911529541\n",
      "Training loss: 0.31728580594062805\n",
      "Training loss: 0.2733422517776489\n",
      "Training loss: 0.324249267578125\n",
      "Training loss: 0.2911071181297302\n",
      "Training loss: 0.28431451320648193\n",
      "Training loss: 0.33355841040611267\n",
      "Training loss: 0.32357916235923767\n",
      "Training loss: 0.19651418924331665\n",
      "Training loss: 0.42130959033966064\n",
      "Training loss: 0.3447084128856659\n",
      "Training loss: 0.2773078978061676\n",
      "Training loss: 0.40732938051223755\n",
      "Training loss: 0.27195531129837036\n",
      "Training loss: 0.2561987042427063\n",
      "Training loss: 0.34735938906669617\n",
      "Training loss: 0.2894866168498993\n",
      "Training loss: 0.4361213147640228\n",
      "Training loss: 0.318744421005249\n",
      "Training loss: 0.27591079473495483\n",
      "Training loss: 0.35945451259613037\n",
      "Training loss: 0.3490521311759949\n",
      "Training loss: 0.33706212043762207\n",
      "Training loss: 0.3011971414089203\n",
      "Training loss: 0.3188927173614502\n",
      "Training loss: 0.3196687400341034\n",
      "Training loss: 0.410818487405777\n",
      "Training loss: 0.356458455324173\n",
      "Training loss: 0.43203452229499817\n",
      "Training loss: 0.3063352108001709\n",
      "Training loss: 0.2596306800842285\n",
      "Training loss: 0.29782843589782715\n",
      "Training loss: 0.3139796257019043\n",
      "Training loss: 0.27415958046913147\n",
      "Training loss: 0.28061792254447937\n",
      "Training loss: 0.2714328169822693\n",
      "Training loss: 0.2603833079338074\n",
      "Training loss: 0.24278965592384338\n",
      "Training loss: 0.192061185836792\n",
      "Training loss: 0.3135555386543274\n",
      "Training loss: 0.32153692841529846\n",
      "Training loss: 0.3179021179676056\n",
      "Training loss: 0.3985065817832947\n",
      "Training loss: 0.2921401262283325\n",
      "Training loss: 0.29025256633758545\n",
      "Training loss: 0.3588486313819885\n",
      "Training loss: 0.25397202372550964\n",
      "Training loss: 0.2106427103281021\n",
      "Training loss: 0.23954452574253082\n",
      "Training loss: 0.30152806639671326\n",
      "Training loss: 0.18187879025936127\n",
      "Training loss: 0.3387002944946289\n",
      "Training loss: 0.28156694769859314\n",
      "Training loss: 0.30582335591316223\n",
      "Training loss: 0.3149333596229553\n",
      "Training loss: 0.36029431223869324\n",
      "Training loss: 0.31728672981262207\n",
      "Training loss: 0.27334117889404297\n",
      "Training loss: 0.3242492973804474\n",
      "Training loss: 0.2911069095134735\n",
      "Training loss: 0.2843141555786133\n",
      "Training loss: 0.33355867862701416\n",
      "Training loss: 0.323578417301178\n",
      "Training loss: 0.1965124011039734\n",
      "Training loss: 0.4213120639324188\n",
      "Training loss: 0.3447093367576599\n",
      "Training loss: 0.27730676531791687\n",
      "Training loss: 0.4073309898376465\n",
      "Training loss: 0.27195486426353455\n",
      "Training loss: 0.2561975121498108\n",
      "Training loss: 0.3473605513572693\n",
      "Training loss: 0.28948545455932617\n",
      "Training loss: 0.43612584471702576\n",
      "Training loss: 0.31874528527259827\n",
      "Training loss: 0.27590909600257874\n",
      "Training loss: 0.35945606231689453\n",
      "Training loss: 0.34905263781547546\n",
      "Training loss: 0.3370625972747803\n",
      "Training loss: 0.3011970818042755\n",
      "Training loss: 0.31889283657073975\n",
      "Training loss: 0.31966862082481384\n",
      "Training loss: 0.4108206033706665\n",
      "Training loss: 0.3564591407775879\n",
      "Training loss: 0.4320358335971832\n",
      "Training loss: 0.3063351511955261\n",
      "Training loss: 0.25962987542152405\n",
      "Training loss: 0.29782789945602417\n",
      "Training loss: 0.31397923827171326\n",
      "Training loss: 0.27415981888771057\n",
      "Training loss: 0.2806189954280853\n",
      "Training loss: 0.27143362164497375\n",
      "Training loss: 0.26038458943367004\n",
      "Training loss: 0.24279047548770905\n",
      "Training loss: 0.1920596957206726\n",
      "Training loss: 0.3135547637939453\n",
      "Training loss: 0.3215387761592865\n",
      "Training loss: 0.3179011046886444\n",
      "Training loss: 0.3985065817832947\n",
      "Training loss: 0.29214030504226685\n",
      "Training loss: 0.29025304317474365\n",
      "Training loss: 0.35884958505630493\n",
      "Training loss: 0.2539713680744171\n",
      "Training loss: 0.2106417417526245\n",
      "Training loss: 0.23954357206821442\n",
      "Training loss: 0.30152785778045654\n",
      "Training loss: 0.18187712132930756\n",
      "Training loss: 0.33870095014572144\n",
      "Training loss: 0.28156712651252747\n",
      "Training loss: 0.30582237243652344\n",
      "Training loss: 0.31493401527404785\n",
      "Training loss: 0.36029571294784546\n",
      "Training loss: 0.3172875642776489\n",
      "Training loss: 0.2733401656150818\n",
      "Training loss: 0.32424917817115784\n",
      "Training loss: 0.2911067605018616\n",
      "Training loss: 0.284313827753067\n",
      "Training loss: 0.3335590660572052\n",
      "Training loss: 0.32357773184776306\n",
      "Training loss: 0.1965107023715973\n",
      "Training loss: 0.4213145971298218\n",
      "Training loss: 0.3447103202342987\n",
      "Training loss: 0.2773056924343109\n",
      "Training loss: 0.4073324203491211\n",
      "Training loss: 0.2719544470310211\n",
      "Training loss: 0.256196528673172\n",
      "Training loss: 0.34736162424087524\n",
      "Training loss: 0.2894843518733978\n",
      "Training loss: 0.43613025546073914\n",
      "Training loss: 0.31874608993530273\n",
      "Training loss: 0.2759074568748474\n",
      "Training loss: 0.3594573736190796\n",
      "Training loss: 0.3490532636642456\n",
      "Training loss: 0.3370629549026489\n",
      "Training loss: 0.30119699239730835\n",
      "Training loss: 0.3188929259777069\n",
      "Training loss: 0.3196686804294586\n",
      "Training loss: 0.4108225703239441\n",
      "Training loss: 0.35645967721939087\n",
      "Training loss: 0.43203723430633545\n",
      "Training loss: 0.30633512139320374\n",
      "Training loss: 0.25962910056114197\n",
      "Training loss: 0.29782751202583313\n",
      "Training loss: 0.3139788508415222\n",
      "Training loss: 0.27416011691093445\n",
      "Training loss: 0.2806199789047241\n",
      "Training loss: 0.27143439650535583\n",
      "Training loss: 0.2603858411312103\n",
      "Training loss: 0.24279125034809113\n",
      "Training loss: 0.1920582801103592\n",
      "Training loss: 0.31355395913124084\n",
      "Training loss: 0.32154059410095215\n",
      "Training loss: 0.3179001212120056\n",
      "Training loss: 0.3985067307949066\n",
      "Training loss: 0.2921404540538788\n",
      "Training loss: 0.2902534306049347\n",
      "Training loss: 0.3588502109050751\n",
      "Training loss: 0.25397080183029175\n",
      "Training loss: 0.2106407880783081\n",
      "Training loss: 0.23954260349273682\n",
      "Training loss: 0.30152761936187744\n",
      "Training loss: 0.18187560141086578\n",
      "Training loss: 0.33870160579681396\n",
      "Training loss: 0.2815673351287842\n",
      "Training loss: 0.30582141876220703\n",
      "Training loss: 0.3149346709251404\n",
      "Training loss: 0.36029699444770813\n",
      "Training loss: 0.3172883987426758\n",
      "Training loss: 0.2733391523361206\n",
      "Training loss: 0.3242492079734802\n",
      "Training loss: 0.291106641292572\n",
      "Training loss: 0.28431347012519836\n",
      "Training loss: 0.3335593044757843\n",
      "Training loss: 0.32357698678970337\n",
      "Training loss: 0.19650906324386597\n",
      "Training loss: 0.4213169515132904\n",
      "Training loss: 0.34471118450164795\n",
      "Training loss: 0.27730464935302734\n",
      "Training loss: 0.4073338806629181\n",
      "Training loss: 0.27195408940315247\n",
      "Training loss: 0.25619569420814514\n",
      "Training loss: 0.34736260771751404\n",
      "Training loss: 0.28948333859443665\n",
      "Training loss: 0.4361344575881958\n",
      "Training loss: 0.3187469244003296\n",
      "Training loss: 0.27590590715408325\n",
      "Training loss: 0.35945868492126465\n",
      "Training loss: 0.34905368089675903\n",
      "Training loss: 0.33706337213516235\n",
      "Training loss: 0.3011969327926636\n",
      "Training loss: 0.3188929557800293\n",
      "Training loss: 0.3196687698364258\n",
      "Training loss: 0.4108244776725769\n",
      "Training loss: 0.3564601540565491\n",
      "Training loss: 0.43203842639923096\n",
      "Training loss: 0.30633506178855896\n",
      "Training loss: 0.25962838530540466\n",
      "Training loss: 0.29782700538635254\n",
      "Training loss: 0.3139784336090088\n",
      "Training loss: 0.2741602659225464\n",
      "Training loss: 0.28062087297439575\n",
      "Training loss: 0.27143508195877075\n",
      "Training loss: 0.26038694381713867\n",
      "Training loss: 0.24279199540615082\n",
      "Training loss: 0.19205698370933533\n",
      "Training loss: 0.3135532736778259\n",
      "Training loss: 0.32154226303100586\n",
      "Training loss: 0.3178991675376892\n",
      "Training loss: 0.39850670099258423\n",
      "Training loss: 0.2921406328678131\n",
      "Training loss: 0.2902539074420929\n",
      "Training loss: 0.3588511347770691\n",
      "Training loss: 0.2539702355861664\n",
      "Training loss: 0.21063990890979767\n",
      "Training loss: 0.23954176902770996\n",
      "Training loss: 0.3015274703502655\n",
      "Training loss: 0.18187426030635834\n",
      "Training loss: 0.3387022912502289\n",
      "Training loss: 0.2815675139427185\n",
      "Training loss: 0.3058205842971802\n",
      "Training loss: 0.31493520736694336\n",
      "Training loss: 0.360298216342926\n",
      "Training loss: 0.31728920340538025\n",
      "Training loss: 0.273338258266449\n",
      "Training loss: 0.3242492377758026\n",
      "Training loss: 0.29110661149024963\n",
      "Training loss: 0.2843132019042969\n",
      "Training loss: 0.3335595428943634\n",
      "Training loss: 0.323576420545578\n",
      "Training loss: 0.19650758802890778\n",
      "Training loss: 0.42131906747817993\n",
      "Training loss: 0.3447120785713196\n",
      "Training loss: 0.27730366587638855\n",
      "Training loss: 0.40733522176742554\n",
      "Training loss: 0.2719537019729614\n",
      "Training loss: 0.25619474053382874\n",
      "Training loss: 0.3473634719848633\n",
      "Training loss: 0.28948235511779785\n",
      "Training loss: 0.4361380934715271\n",
      "Training loss: 0.3187476396560669\n",
      "Training loss: 0.27590444684028625\n",
      "Training loss: 0.3594599664211273\n",
      "Training loss: 0.3490542471408844\n",
      "Training loss: 0.3370637595653534\n",
      "Training loss: 0.3011968433856964\n",
      "Training loss: 0.3188931345939636\n",
      "Training loss: 0.31966888904571533\n",
      "Training loss: 0.41082632541656494\n",
      "Training loss: 0.3564606010913849\n",
      "Training loss: 0.4320395588874817\n",
      "Training loss: 0.30633506178855896\n",
      "Training loss: 0.25962769985198975\n",
      "Training loss: 0.2978266179561615\n",
      "Training loss: 0.31397804617881775\n",
      "Training loss: 0.2741605341434479\n",
      "Training loss: 0.28062179684638977\n",
      "Training loss: 0.27143579721450806\n",
      "Training loss: 0.260388046503067\n",
      "Training loss: 0.24279269576072693\n",
      "Training loss: 0.19205573201179504\n",
      "Training loss: 0.31355252861976624\n",
      "Training loss: 0.3215438723564148\n",
      "Training loss: 0.31789830327033997\n",
      "Training loss: 0.3985067903995514\n",
      "Training loss: 0.29214081168174744\n",
      "Training loss: 0.29025429487228394\n",
      "Training loss: 0.35885167121887207\n",
      "Training loss: 0.253969669342041\n",
      "Training loss: 0.210639089345932\n",
      "Training loss: 0.2395409345626831\n",
      "Training loss: 0.3015272617340088\n",
      "Training loss: 0.18187279999256134\n",
      "Training loss: 0.33870288729667664\n",
      "Training loss: 0.28156763315200806\n",
      "Training loss: 0.30581969022750854\n",
      "Training loss: 0.3149358332157135\n",
      "Training loss: 0.36029931902885437\n",
      "Training loss: 0.31728994846343994\n",
      "Training loss: 0.27333739399909973\n",
      "Training loss: 0.3242490887641907\n",
      "Training loss: 0.2911064028739929\n",
      "Training loss: 0.284312903881073\n",
      "Training loss: 0.33355990052223206\n",
      "Training loss: 0.3235757648944855\n",
      "Training loss: 0.19650612771511078\n",
      "Training loss: 0.42132115364074707\n",
      "Training loss: 0.34471288323402405\n",
      "Training loss: 0.27730274200439453\n",
      "Training loss: 0.4073364734649658\n",
      "Training loss: 0.2719533443450928\n",
      "Training loss: 0.2561938166618347\n",
      "Training loss: 0.3473643660545349\n",
      "Training loss: 0.28948140144348145\n",
      "Training loss: 0.43614205718040466\n",
      "Training loss: 0.3187482953071594\n",
      "Training loss: 0.27590301632881165\n",
      "Training loss: 0.3594612181186676\n",
      "Training loss: 0.3490546643733978\n",
      "Training loss: 0.33706414699554443\n",
      "Training loss: 0.30119675397872925\n",
      "Training loss: 0.3188931345939636\n",
      "Training loss: 0.3196687698364258\n",
      "Training loss: 0.4108280539512634\n",
      "Training loss: 0.3564610183238983\n",
      "Training loss: 0.43204060196876526\n",
      "Training loss: 0.3063349723815918\n",
      "Training loss: 0.25962701439857483\n",
      "Training loss: 0.29782629013061523\n",
      "Training loss: 0.3139777183532715\n",
      "Training loss: 0.2741606831550598\n",
      "Training loss: 0.28062260150909424\n",
      "Training loss: 0.2714364528656006\n",
      "Training loss: 0.2603890597820282\n",
      "Training loss: 0.24279333651065826\n",
      "Training loss: 0.19205451011657715\n",
      "Training loss: 0.3135518431663513\n",
      "Training loss: 0.32154545187950134\n",
      "Training loss: 0.3178974986076355\n",
      "Training loss: 0.3985068202018738\n",
      "Training loss: 0.2921409606933594\n",
      "Training loss: 0.2902546525001526\n",
      "Training loss: 0.3588525056838989\n",
      "Training loss: 0.2539691925048828\n",
      "Training loss: 0.21063831448554993\n",
      "Training loss: 0.2395401895046234\n",
      "Training loss: 0.30152711272239685\n",
      "Training loss: 0.18187150359153748\n"
     ]
    }
   ],
   "source": [
    "from valuation.models.pytorch_model import PyTorchSupervisedModel, PyTorchOptimizer\n",
    "from valuation.models.binary_logistic_regression import BinaryLogisticRegressionTorchModel\n",
    "import torch.nn.functional as F\n",
    "\n",
    "flipped_model = PyTorchSupervisedModel(\n",
    "    model=BinaryLogisticRegressionTorchModel(num_features),\n",
    "    objective=F.binary_cross_entropy,\n",
    "    num_epochs=100,\n",
    "    batch_size=128,\n",
    "    optimizer=PyTorchOptimizer.ADAM_W,\n",
    "    optimizer_kwargs={\n",
    "        \"lr\": 0.005,\n",
    "        \"weight_decay\": 0.005\n",
    "    },\n",
    ")\n",
    "flipped_model.fit(\n",
    "    flipped_dataset.x_train,\n",
    "    flipped_dataset.y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053be6b9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "a new model is fitted with the pyDVL library. The newly obtained model is then used along with the flipped dataset to obtain the influences. Recall the aforementioned influence metrics and more specifically the mean absolute influence. For each training sample this metric is calculated and the 5% data points with the highest associated metric are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e745b2d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Around 79.43% could be identified. But there are 20.57% remaining samples'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipped_train_test_influences = influences(\n",
    "    flipped_model,\n",
    "    flipped_dataset.x_train,\n",
    "    flipped_dataset.y_train,\n",
    "    flipped_dataset.x_test,\n",
    "    flipped_dataset.y_test,\n",
    "    influence_type=InfluenceTypes.Up\n",
    ")\n",
    "mean_flipped_train_test_influences = mean_influences(flipped_train_test_influences)\n",
    "estimated_idx = np.flip(np.argsort(mean_flipped_train_test_influences))[:len(flipped_idx)]\n",
    "found_elements = set(estimated_idx).intersection(set(flipped_idx))\n",
    "remaining_element = set(flipped_idx).difference(set(estimated_idx))\n",
    "f\"Around {100* len(found_elements) / len(flipped_idx):.2f}% could be identified. But there are {100* len(remaining_element) / len(idx):.2f}% remaining samples\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414e4d8d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " Furthermore, the accuracy is evaluated, e.g. the number of same elements in the ground truth and the detected samples. Depending on the dataset a detection of up to 80% percent could be achieved. One might further inspect the selection method for the indices as it only selects the highest influence points as flipped samples. Furthermore, it is unclear how flipping all samples back and retraining the model affects the loss of the initial dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
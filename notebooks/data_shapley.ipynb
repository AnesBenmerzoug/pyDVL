{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PyDVL - Shapley for data valuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook introduces Shapley methods for the computation of data value using pyDVL.\n",
    "\n",
    "In order to show the practical advantages of shapley pydvl implementation, we will predict the popularity of songs in the \"Top Hits Spotify from 2000-2019\" dataset. While doing so, we will highlight how data valuation can help boost the performance of your models.\n",
    "\n",
    "Here, all the library main entry-points will be briefly described. We will also show the advantages of this library compared to vanilla data-Shapley implementations, like runtime optimization for large datasets and models.\n",
    "\n",
    "Let's start with some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To load the dataset, we will use the load_spotify_dataset method. Internally, the method will load data on songs published after 2014, use 30% of data for test, and another 30% of the remaining data for validation. Then, the method will return train, validation and test data as lists of the shape [X_input, Y_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fabio/miniconda3/envs/data_shapley/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>year</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>Friends (with BloodPop®)</td>\n",
       "      <td>189466</td>\n",
       "      <td>False</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.739</td>\n",
       "      <td>8</td>\n",
       "      <td>-5.350</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.00459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3060</td>\n",
       "      <td>0.649</td>\n",
       "      <td>104.990</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>Bryson Tiller</td>\n",
       "      <td>Exchange</td>\n",
       "      <td>194613</td>\n",
       "      <td>True</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.433</td>\n",
       "      <td>6</td>\n",
       "      <td>-10.598</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1850</td>\n",
       "      <td>0.10700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>0.276</td>\n",
       "      <td>160.108</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>The Neighbourhood</td>\n",
       "      <td>Daddy Issues</td>\n",
       "      <td>260173</td>\n",
       "      <td>False</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.521</td>\n",
       "      <td>10</td>\n",
       "      <td>-9.461</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.06780</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>0.337</td>\n",
       "      <td>85.012</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>Drake</td>\n",
       "      <td>Money In The Grave (Drake ft. Rick Ross)</td>\n",
       "      <td>205426</td>\n",
       "      <td>True</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.502</td>\n",
       "      <td>10</td>\n",
       "      <td>-4.045</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.10100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1220</td>\n",
       "      <td>0.101</td>\n",
       "      <td>100.541</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>Fifth Harmony</td>\n",
       "      <td>Work from Home (feat. Ty Dolla $ign)</td>\n",
       "      <td>214480</td>\n",
       "      <td>False</td>\n",
       "      <td>2016</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.585</td>\n",
       "      <td>8</td>\n",
       "      <td>-5.861</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>0.10300</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.0644</td>\n",
       "      <td>0.593</td>\n",
       "      <td>105.017</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 artist                                      song  \\\n",
       "1782      Justin Bieber                  Friends (with BloodPop®)   \n",
       "1693      Bryson Tiller                                  Exchange   \n",
       "1512  The Neighbourhood                              Daddy Issues   \n",
       "1938              Drake  Money In The Grave (Drake ft. Rick Ross)   \n",
       "1665      Fifth Harmony      Work from Home (feat. Ty Dolla $ign)   \n",
       "\n",
       "      duration_ms  explicit  year  danceability  energy  key  loudness  mode  \\\n",
       "1782       189466     False  2017         0.744   0.739    8    -5.350     1   \n",
       "1693       194613      True  2015         0.525   0.433    6   -10.598     1   \n",
       "1512       260173     False  2015         0.588   0.521   10    -9.461     1   \n",
       "1938       205426      True  2019         0.831   0.502   10    -4.045     0   \n",
       "1665       214480     False  2016         0.803   0.585    8    -5.861     1   \n",
       "\n",
       "      speechiness  acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "1782       0.0387       0.00459          0.000000    0.3060    0.649  104.990   \n",
       "1693       0.1850       0.10700          0.000000    0.1350    0.276  160.108   \n",
       "1512       0.0329       0.06780          0.149000    0.1230    0.337   85.012   \n",
       "1938       0.0460       0.10100          0.000000    0.1220    0.101  100.541   \n",
       "1665       0.0432       0.10300          0.000004    0.0644    0.593  105.017   \n",
       "\n",
       "      genre  \n",
       "1782     14  \n",
       "1693     10  \n",
       "1512     26  \n",
       "1938     10  \n",
       "1665     14  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from valuation.utils import load_spotify_dataset\n",
    "train_data, val_data, test_data = load_spotify_dataset(val_size=0.3, test_size=0.3, target_column='popularity')\n",
    "train_data[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The dataset has many high level feature, some quite intuitive ('duration_ms' or 'tempo'), and other a bit more cryptic ('valence'?). If you want more information on each feature, you can find it on [this webpage](https://www.kaggle.com/datasets/paradisejoy/top-hits-spotify-from-20002019?resource=download).\n",
    "In our analysis, we will use all the columns, excluding 'artist' and 'song', to predict the 'popularity' of each song. We will nonetheless keep the information on song and artist in the train set in a separate pandas Series, for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "song_name = train_data[0]['song']\n",
    "artist = train_data[0]['artist']\n",
    "train_data[0] = train_data[0].drop(['song', 'artist'], axis=1)\n",
    "test_data[0] = test_data[0].drop(['song', 'artist'], axis=1)\n",
    "val_data[0] = val_data[0].drop(['song', 'artist'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Input and label data need to be stored in a Dataset object, a pyDVL specific tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from valuation.utils import Dataset\n",
    "dataset = Dataset(*train_data, *val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The calculation of Shapley coefficients is very computationally expensive because it needs to go through several subsets of the original input dataset before converging. For this reason, the PyDVL library implements techniques to speed up the calculation, both caching intermediate results and allowing to group data to calculate Shapley values on groups instead of single-datapoints. From a Dataset object, you can just do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from valuation.utils import GroupedDataset\n",
    "grouped_dataset = GroupedDataset.from_dataset(dataset, artist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And now we can finally calculate the contribution of each datapoint to the modle performance!\n",
    "\n",
    "As model, we will use a GradientBoostingRegressor, but any model from sklearn, xgboost and lightgbm works. More precisely, any model that has a fit and predict method should run without issues.\n",
    "\n",
    "Note: Make sure to restart (or simply start if it is not already running) your memcache. See our [documentation](https://appliedai-initiative.github.io/valuation/install.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m/Users/fabio/Desktop/valuation/notebooks/data_shapley.ipynb Cell 13\u001B[0m in \u001B[0;36m<cell line: 8>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/data_shapley.ipynb#ch0000012?line=2'>3</a>\u001B[0m \u001B[39mfrom\u001B[39;00m \u001B[39msklearn\u001B[39;00m\u001B[39m.\u001B[39;00m\u001B[39mensemble\u001B[39;00m \u001B[39mimport\u001B[39;00m GradientBoostingRegressor\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/data_shapley.ipynb#ch0000012?line=4'>5</a>\u001B[0m utility \u001B[39m=\u001B[39m Utility(\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/data_shapley.ipynb#ch0000012?line=5'>6</a>\u001B[0m         model\u001B[39m=\u001B[39mGradientBoostingRegressor(n_estimators\u001B[39m=\u001B[39m\u001B[39m3\u001B[39m), data\u001B[39m=\u001B[39mgrouped_dataset, scoring\u001B[39m=\u001B[39m\u001B[39m'\u001B[39m\u001B[39mneg_mean_absolute_error\u001B[39m\u001B[39m'\u001B[39m, enable_cache\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m,\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/data_shapley.ipynb#ch0000012?line=6'>7</a>\u001B[0m     )\n\u001B[0;32m----> <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/data_shapley.ipynb#ch0000012?line=7'>8</a>\u001B[0m dvl_df \u001B[39m=\u001B[39m get_shapley_values(utility, iterations_per_job\u001B[39m=\u001B[39;49m\u001B[39m50\u001B[39;49m, num_jobs\u001B[39m=\u001B[39;49m\u001B[39m20\u001B[39;49m)\n",
      "File \u001B[0;32m~/Desktop/valuation/src/valuation/shapley/__init__.py:59\u001B[0m, in \u001B[0;36mget_shapley_values\u001B[0;34m(u, iterations_per_job, num_jobs, use_combinatorial, use_exact)\u001B[0m\n\u001B[1;32m     55\u001B[0m         dval, dval_std \u001B[39m=\u001B[39m combinatorial_montecarlo_shapley(\n\u001B[1;32m     56\u001B[0m             u, iterations_per_job, num_jobs, progress\n\u001B[1;32m     57\u001B[0m         )\n\u001B[1;32m     58\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[0;32m---> 59\u001B[0m         dval, dval_std \u001B[39m=\u001B[39m permutation_montecarlo_shapley(\n\u001B[1;32m     60\u001B[0m             u, iterations_per_job, num_jobs, progress\n\u001B[1;32m     61\u001B[0m         )\n\u001B[1;32m     62\u001B[0m \u001B[39mreturn\u001B[39;00m pd\u001B[39m.\u001B[39mDataFrame(\n\u001B[1;32m     63\u001B[0m     \u001B[39mlist\u001B[39m(\u001B[39mzip\u001B[39m(dval\u001B[39m.\u001B[39mkeys(), dval\u001B[39m.\u001B[39mvalues(), dval_std\u001B[39m.\u001B[39mvalues())),\n\u001B[1;32m     64\u001B[0m     columns\u001B[39m=\u001B[39m[\u001B[39m\"\u001B[39m\u001B[39mdata_key\u001B[39m\u001B[39m\"\u001B[39m, \u001B[39m\"\u001B[39m\u001B[39mshapley_dval\u001B[39m\u001B[39m\"\u001B[39m, \u001B[39m\"\u001B[39m\u001B[39mdval_std\u001B[39m\u001B[39m\"\u001B[39m],\n\u001B[1;32m     65\u001B[0m )\n",
      "File \u001B[0;32m~/Desktop/valuation/src/valuation/shapley/montecarlo.py:310\u001B[0m, in \u001B[0;36mpermutation_montecarlo_shapley\u001B[0;34m(u, max_iterations, num_jobs, progress)\u001B[0m\n\u001B[1;32m    308\u001B[0m \u001B[39m# TODO move to map_reduce as soon as it is fixed\u001B[39;00m\n\u001B[1;32m    309\u001B[0m backend \u001B[39m=\u001B[39m make_nested_backend(\u001B[39m\"\u001B[39m\u001B[39mloky\u001B[39m\u001B[39m\"\u001B[39m)()\n\u001B[0;32m--> 310\u001B[0m results \u001B[39m=\u001B[39m Parallel(n_jobs\u001B[39m=\u001B[39;49mnum_jobs, backend\u001B[39m=\u001B[39;49mbackend)(\n\u001B[1;32m    311\u001B[0m     delayed(fun)(job_id\u001B[39m=\u001B[39;49mj \u001B[39m+\u001B[39;49m \u001B[39m1\u001B[39;49m) \u001B[39mfor\u001B[39;49;00m j \u001B[39min\u001B[39;49;00m \u001B[39mrange\u001B[39;49m(num_jobs)\n\u001B[1;32m    312\u001B[0m )\n\u001B[1;32m    313\u001B[0m full_results \u001B[39m=\u001B[39m np\u001B[39m.\u001B[39mconcatenate(results, axis\u001B[39m=\u001B[39m\u001B[39m0\u001B[39m)\n\u001B[1;32m    314\u001B[0m \u001B[39m# Careful: for some models there might be nans, e.g. for i=0 or i=1!\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/data_shapley/lib/python3.10/site-packages/joblib/parallel.py:1056\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1053\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_iterating \u001B[39m=\u001B[39m \u001B[39mFalse\u001B[39;00m\n\u001B[1;32m   1055\u001B[0m \u001B[39mwith\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backend\u001B[39m.\u001B[39mretrieval_context():\n\u001B[0;32m-> 1056\u001B[0m     \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mretrieve()\n\u001B[1;32m   1057\u001B[0m \u001B[39m# Make sure that we get a last message telling us we are done\u001B[39;00m\n\u001B[1;32m   1058\u001B[0m elapsed_time \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime() \u001B[39m-\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_start_time\n",
      "File \u001B[0;32m~/miniconda3/envs/data_shapley/lib/python3.10/site-packages/joblib/parallel.py:935\u001B[0m, in \u001B[0;36mParallel.retrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    933\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m    934\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mgetattr\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backend, \u001B[39m'\u001B[39m\u001B[39msupports_timeout\u001B[39m\u001B[39m'\u001B[39m, \u001B[39mFalse\u001B[39;00m):\n\u001B[0;32m--> 935\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_output\u001B[39m.\u001B[39mextend(job\u001B[39m.\u001B[39;49mget(timeout\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mtimeout))\n\u001B[1;32m    936\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    937\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_output\u001B[39m.\u001B[39mextend(job\u001B[39m.\u001B[39mget())\n",
      "File \u001B[0;32m~/miniconda3/envs/data_shapley/lib/python3.10/site-packages/joblib/_parallel_backends.py:542\u001B[0m, in \u001B[0;36mLokyBackend.wrap_future_result\u001B[0;34m(future, timeout)\u001B[0m\n\u001B[1;32m    539\u001B[0m \u001B[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001B[39;00m\n\u001B[1;32m    540\u001B[0m \u001B[39mAsyncResults.get from multiprocessing.\"\"\"\u001B[39;00m\n\u001B[1;32m    541\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m--> 542\u001B[0m     \u001B[39mreturn\u001B[39;00m future\u001B[39m.\u001B[39;49mresult(timeout\u001B[39m=\u001B[39;49mtimeout)\n\u001B[1;32m    543\u001B[0m \u001B[39mexcept\u001B[39;00m CfTimeoutError \u001B[39mas\u001B[39;00m e:\n\u001B[1;32m    544\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mTimeoutError\u001B[39;00m \u001B[39mfrom\u001B[39;00m \u001B[39me\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/data_shapley/lib/python3.10/concurrent/futures/_base.py:441\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    438\u001B[0m \u001B[39melif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_state \u001B[39m==\u001B[39m FINISHED:\n\u001B[1;32m    439\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m__get_result()\n\u001B[0;32m--> 441\u001B[0m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_condition\u001B[39m.\u001B[39;49mwait(timeout)\n\u001B[1;32m    443\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_state \u001B[39min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001B[1;32m    444\u001B[0m     \u001B[39mraise\u001B[39;00m CancelledError()\n",
      "File \u001B[0;32m~/miniconda3/envs/data_shapley/lib/python3.10/threading.py:320\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[39mtry\u001B[39;00m:    \u001B[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    319\u001B[0m     \u001B[39mif\u001B[39;00m timeout \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[0;32m--> 320\u001B[0m         waiter\u001B[39m.\u001B[39;49macquire()\n\u001B[1;32m    321\u001B[0m         gotit \u001B[39m=\u001B[39m \u001B[39mTrue\u001B[39;00m\n\u001B[1;32m    322\u001B[0m     \u001B[39melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from valuation.shapley import get_shapley_values\n",
    "from valuation.utils import Utility\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "utility = Utility(\n",
    "    model=GradientBoostingRegressor(n_estimators=3),\n",
    "    data=grouped_dataset,\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    # TODO: This should be set parametrically\n",
    "    enable_cache=False,\n",
    ")\n",
    "dvl_df = get_shapley_values(utility, iterations_per_job=50, num_jobs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see, the first object that we have defined (utility) holds all the information on the dataset, the model and the scoring methods. Within utility, the PyDVL library uses a cache to speed up calculation of the Shapley coefficients.\n",
    "The second method, get_shapley_values, calculates the actual Shapley values and manages parallelization. Let's take a look at the returned dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dvl_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The first thing to notice is that the dvl_df dataframe is sorted in ascending order of shapley values. The data_key columns holds the labels for each data group: in this case, since the groups correspond to artists, it coincides with artist names. The second column corresponds to the shapley data valuation score, and the third to its (approximate) standard deviation.\n",
    "\n",
    "A better way to analyse the results is through a plot. In the next cell we will take the 30 datapoints with lowest score and plot them with errorbars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from valuation.utils import plot_shapley\n",
    "low_dvl = dvl_df.iloc[:30]\n",
    "fig = plot_shapley(low_dvl, figsize=(20, 4), title='Artists with low data valuation scores', xlabel='artist', ylabel='shapley_pydvl')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As you can see, there are a lot of points which give negative shapley score, meaning that they tend to decrease the total score of the model when present in the training set! What happens if we remove it? In the next cell we will create a new training set which excludes the 30 points with lowest scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "low_dvl_artists = dvl_df.iloc[:30].data_key.to_list()\n",
    "artist_filter = ~artist.isin(low_dvl_artists)\n",
    "X_train_good_dvl = train_data[0][artist_filter]\n",
    "y_train_good_dvl = train_data[0][artist_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we will use this cleaned dataset to train a full GradientBoostingRegressor and compare its mean absolute error to the model which uses the full dataset. Notice that the score now is calculated using the test set, while for calculating the shapley values we were using the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "full_model = GradientBoostingRegressor(n_estimators=3).fit(X_train_good_dvl, y_train_good_dvl)\n",
    "mean_absolute_error(full_model.predict(test_data[0]), test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "full_model = GradientBoostingRegressor(n_estimators=3).fit(train_data[0], train_data[1])\n",
    "mean_absolute_error(full_model.predict(test_data[0]), test_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The score has improved by more than 15%! This is quite an important result, as it shows a self-consistent process to improve the performance of a model by excluding datapoints from its training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation on anomalous data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Another interesting test is to corrupt some of the data and monitor how their valuation score changes. To do this, we will take one of the authors with the highest score and set all its popularity to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dval_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m/Users/fabio/Desktop/valuation/notebooks/data_shapley.ipynb Cell 26\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/data_shapley.ipynb#ch0000025?line=0'>1</a>\u001B[0m high_dval \u001B[39m=\u001B[39m dval_df\u001B[39m.\u001B[39miloc[\u001B[39m-\u001B[39m\u001B[39m30\u001B[39m:]\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/data_shapley.ipynb#ch0000025?line=1'>2</a>\u001B[0m fig \u001B[39m=\u001B[39m plot_shapley(high_dval, figsize\u001B[39m=\u001B[39m(\u001B[39m20\u001B[39m, \u001B[39m4\u001B[39m), title\u001B[39m=\u001B[39m\u001B[39m'\u001B[39m\u001B[39mArtists with high data valuation scores\u001B[39m\u001B[39m'\u001B[39m, xlabel\u001B[39m=\u001B[39m\u001B[39m'\u001B[39m\u001B[39martist\u001B[39m\u001B[39m'\u001B[39m, ylabel\u001B[39m=\u001B[39m\u001B[39m'\u001B[39m\u001B[39mshapley_dval\u001B[39m\u001B[39m'\u001B[39m)\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/data_shapley.ipynb#ch0000025?line=2'>3</a>\u001B[0m fig\u001B[39m.\u001B[39mshow()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'dval_df' is not defined"
     ]
    }
   ],
   "source": [
    "high_dvl = dvl_df.iloc[-30:]\n",
    "fig = plot_shapley(high_dvl, figsize=(20, 4), title='Artists with high data valuation scores', xlabel='artist', ylabel='shapley values')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "From the plot above, we can see that Rihanna has one of the highest scores. Let's now take all the train labels related to her, set the score to 0 and re-calculate the data valuation scores. Notice that I must use a new name for the dataset, because I need to create another cache. Reusing the old one would be a mistake since modifying even just one label changes all the shapley values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'|\\x01s\\x04d\\x01S\\x00t\\x00|\\x00j\\x01|\\x00j\\x02\\x83\\x02}\\x02|\\x00j\\x03\\xa0\\x04t\\x05|\\x01\\x83\\x01\\xa1\\x01\\\\\\x02}\\x03}\\x04z\\x15|\\x00j\\x01\\xa0\\x06|\\x03|\\x04\\xa1\\x02\\x01\\x00t\\x07|\\x02|\\x00j\\x01|\\x00j\\x03j\\x08|\\x00j\\x03j\\t\\x83\\x03\\x83\\x01W\\x00S\\x00\\x04\\x00t\\nyK\\x01\\x00}\\x05\\x01\\x00z\\x15|\\x00j\\x0brEt\\x0c\\xa0\\rt\\x0e|\\x05\\x83\\x01\\xa1\\x01\\x01\\x00|\\x00j\\x0fW\\x00\\x06\\x00Y\\x00d\\x02}\\x05~\\x05S\\x00|\\x05\\x82\\x01d\\x02}\\x05~\\x05w\\x01w\\x00', ('Fits the model on a subset of the training data and scores it on the\\n        test data. If the object is constructed with cache_size > 0, results are\\n        memoized to avoid duplicate computation. This is useful in particular\\n        when computing utilities of permutations of indices.\\n\\n        :param indices: a subset of indices from data.x_train.index. The type\\n         must be hashable for the caching to work, e.g. wrap the argument with\\n         `frozenset` (rather than `tuple` since order should not matter)\\n\\n        :return: 0 if no indices are passed, otherwise the value the scorer\\n        on the test data.\\n        ', 0, None))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m/Users/fabio/Desktop/valuation/notebooks/data_shapley.ipynb Cell 28\u001B[0m in \u001B[0;36m<cell line: 7>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/data_shapley.ipynb#ch0000027?line=2'>3</a>\u001B[0m grouped_dataset \u001B[39m=\u001B[39m GroupedDataset\u001B[39m.\u001B[39mfrom_dataset(dirty_dataset, artist)\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/data_shapley.ipynb#ch0000027?line=3'>4</a>\u001B[0m utility \u001B[39m=\u001B[39m Utility(\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/data_shapley.ipynb#ch0000027?line=4'>5</a>\u001B[0m         model\u001B[39m=\u001B[39mGradientBoostingRegressor(n_estimators\u001B[39m=\u001B[39m\u001B[39m3\u001B[39m), data\u001B[39m=\u001B[39mgrouped_dataset, scoring\u001B[39m=\u001B[39m\u001B[39m'\u001B[39m\u001B[39mneg_mean_absolute_error\u001B[39m\u001B[39m'\u001B[39m, enable_cache\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m,\n\u001B[1;32m      <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/data_shapley.ipynb#ch0000027?line=5'>6</a>\u001B[0m     )\n\u001B[0;32m----> <a href='vscode-notebook-cell:/Users/fabio/Desktop/valuation/notebooks/data_shapley.ipynb#ch0000027?line=6'>7</a>\u001B[0m dval_df \u001B[39m=\u001B[39m shapley_pydvl(utility, iterations_per_job\u001B[39m=\u001B[39;49m\u001B[39m50\u001B[39;49m, num_jobs\u001B[39m=\u001B[39;49m\u001B[39m20\u001B[39;49m)\n",
      "File \u001B[0;32m~/Desktop/valuation/src/valuation/shapley/__init__.py:59\u001B[0m, in \u001B[0;36mshapley_pydvl\u001B[0;34m(u, iterations_per_job, num_jobs, use_combinatorial, use_exact)\u001B[0m\n\u001B[1;32m     55\u001B[0m         dval, dval_std \u001B[39m=\u001B[39m combinatorial_montecarlo_shapley(\n\u001B[1;32m     56\u001B[0m             u, iterations_per_job, num_jobs, progress\n\u001B[1;32m     57\u001B[0m         )\n\u001B[1;32m     58\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[0;32m---> 59\u001B[0m         dval, dval_std \u001B[39m=\u001B[39m permutation_montecarlo_shapley(\n\u001B[1;32m     60\u001B[0m             u, iterations_per_job, num_jobs, progress\n\u001B[1;32m     61\u001B[0m         )\n\u001B[1;32m     62\u001B[0m \u001B[39mreturn\u001B[39;00m pd\u001B[39m.\u001B[39mDataFrame(\n\u001B[1;32m     63\u001B[0m     \u001B[39mlist\u001B[39m(\u001B[39mzip\u001B[39m(dval\u001B[39m.\u001B[39mkeys(), dval\u001B[39m.\u001B[39mvalues(), dval_std\u001B[39m.\u001B[39mvalues())),\n\u001B[1;32m     64\u001B[0m     columns\u001B[39m=\u001B[39m[\u001B[39m\"\u001B[39m\u001B[39mdata_key\u001B[39m\u001B[39m\"\u001B[39m, \u001B[39m\"\u001B[39m\u001B[39mshapley_dval\u001B[39m\u001B[39m\"\u001B[39m, \u001B[39m\"\u001B[39m\u001B[39mdval_std\u001B[39m\u001B[39m\"\u001B[39m],\n\u001B[1;32m     65\u001B[0m )\n",
      "File \u001B[0;32m~/Desktop/valuation/src/valuation/shapley/montecarlo.py:310\u001B[0m, in \u001B[0;36mpermutation_montecarlo_shapley\u001B[0;34m(u, max_iterations, num_jobs, progress)\u001B[0m\n\u001B[1;32m    308\u001B[0m \u001B[39m# TODO move to map_reduce as soon as it is fixed\u001B[39;00m\n\u001B[1;32m    309\u001B[0m backend \u001B[39m=\u001B[39m make_nested_backend(\u001B[39m\"\u001B[39m\u001B[39mloky\u001B[39m\u001B[39m\"\u001B[39m)()\n\u001B[0;32m--> 310\u001B[0m results \u001B[39m=\u001B[39m Parallel(n_jobs\u001B[39m=\u001B[39;49mnum_jobs, backend\u001B[39m=\u001B[39;49mbackend)(\n\u001B[1;32m    311\u001B[0m     delayed(fun)(job_id\u001B[39m=\u001B[39;49mj \u001B[39m+\u001B[39;49m \u001B[39m1\u001B[39;49m) \u001B[39mfor\u001B[39;49;00m j \u001B[39min\u001B[39;49;00m \u001B[39mrange\u001B[39;49m(num_jobs)\n\u001B[1;32m    312\u001B[0m )\n\u001B[1;32m    313\u001B[0m full_results \u001B[39m=\u001B[39m np\u001B[39m.\u001B[39mconcatenate(results, axis\u001B[39m=\u001B[39m\u001B[39m0\u001B[39m)\n\u001B[1;32m    314\u001B[0m \u001B[39m# Careful: for some models there might be nans, e.g. for i=0 or i=1!\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/data_shapley/lib/python3.10/site-packages/joblib/parallel.py:1056\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1053\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_iterating \u001B[39m=\u001B[39m \u001B[39mFalse\u001B[39;00m\n\u001B[1;32m   1055\u001B[0m \u001B[39mwith\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backend\u001B[39m.\u001B[39mretrieval_context():\n\u001B[0;32m-> 1056\u001B[0m     \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mretrieve()\n\u001B[1;32m   1057\u001B[0m \u001B[39m# Make sure that we get a last message telling us we are done\u001B[39;00m\n\u001B[1;32m   1058\u001B[0m elapsed_time \u001B[39m=\u001B[39m time\u001B[39m.\u001B[39mtime() \u001B[39m-\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_start_time\n",
      "File \u001B[0;32m~/miniconda3/envs/data_shapley/lib/python3.10/site-packages/joblib/parallel.py:935\u001B[0m, in \u001B[0;36mParallel.retrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    933\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[1;32m    934\u001B[0m     \u001B[39mif\u001B[39;00m \u001B[39mgetattr\u001B[39m(\u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_backend, \u001B[39m'\u001B[39m\u001B[39msupports_timeout\u001B[39m\u001B[39m'\u001B[39m, \u001B[39mFalse\u001B[39;00m):\n\u001B[0;32m--> 935\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_output\u001B[39m.\u001B[39mextend(job\u001B[39m.\u001B[39;49mget(timeout\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mtimeout))\n\u001B[1;32m    936\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[1;32m    937\u001B[0m         \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_output\u001B[39m.\u001B[39mextend(job\u001B[39m.\u001B[39mget())\n",
      "File \u001B[0;32m~/miniconda3/envs/data_shapley/lib/python3.10/site-packages/joblib/_parallel_backends.py:542\u001B[0m, in \u001B[0;36mLokyBackend.wrap_future_result\u001B[0;34m(future, timeout)\u001B[0m\n\u001B[1;32m    539\u001B[0m \u001B[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001B[39;00m\n\u001B[1;32m    540\u001B[0m \u001B[39mAsyncResults.get from multiprocessing.\"\"\"\u001B[39;00m\n\u001B[1;32m    541\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m--> 542\u001B[0m     \u001B[39mreturn\u001B[39;00m future\u001B[39m.\u001B[39;49mresult(timeout\u001B[39m=\u001B[39;49mtimeout)\n\u001B[1;32m    543\u001B[0m \u001B[39mexcept\u001B[39;00m CfTimeoutError \u001B[39mas\u001B[39;00m e:\n\u001B[1;32m    544\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mTimeoutError\u001B[39;00m \u001B[39mfrom\u001B[39;00m \u001B[39me\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/data_shapley/lib/python3.10/concurrent/futures/_base.py:441\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    438\u001B[0m \u001B[39melif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_state \u001B[39m==\u001B[39m FINISHED:\n\u001B[1;32m    439\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m__get_result()\n\u001B[0;32m--> 441\u001B[0m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_condition\u001B[39m.\u001B[39;49mwait(timeout)\n\u001B[1;32m    443\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_state \u001B[39min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001B[1;32m    444\u001B[0m     \u001B[39mraise\u001B[39;00m CancelledError()\n",
      "File \u001B[0;32m~/miniconda3/envs/data_shapley/lib/python3.10/threading.py:320\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    318\u001B[0m \u001B[39mtry\u001B[39;00m:    \u001B[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    319\u001B[0m     \u001B[39mif\u001B[39;00m timeout \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[0;32m--> 320\u001B[0m         waiter\u001B[39m.\u001B[39;49macquire()\n\u001B[1;32m    321\u001B[0m         gotit \u001B[39m=\u001B[39m \u001B[39mTrue\u001B[39;00m\n\u001B[1;32m    322\u001B[0m     \u001B[39melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_data[0].loc[artist == 'Rihanna'] = 0\n",
    "dataset = Dataset(*train_data, *val_data)\n",
    "grouped_dataset = GroupedDataset.from_dataset(dataset, artist)\n",
    "utility = Utility(\n",
    "        model=GradientBoostingRegressor(n_estimators=3), data=grouped_dataset, scoring='neg_mean_absolute_error', enable_cache=True,\n",
    "    )\n",
    "dvl_df = get_shapley_values(utility, iterations_per_job=50, num_jobs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's now take the low scoring artists and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "low_dvl = dvl_df.iloc[:30]\n",
    "fig = plot_shapley(low_dvl, figsize=(20, 4), title='Artists with low data valuation scores', xlabel='artist', ylabel='shapley values')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And Rihanna (our anomalous data group) has moved from top contributor to having negative impact on the performance of the model, as expected!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('data_shapley')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e000971326892723e7f31ded70802f690c31c3620f59a0f99e594aaee3047ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
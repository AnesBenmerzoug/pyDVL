{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dval - Shapley for data valuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces Shapley methods for the computation of data value using pyDVL.\n",
    "\n",
    "In order to show the practical advantages of shapley_dval, we will predict the popularity of songs in the \"Top Hits Spotify from 2000-2019\" dataset. While doing so, we will highlight how data valuation can help boost the performance of your models.\n",
    "\n",
    "Here, all the library main entry-points will be briefly described. We will also show the advantages of this library compared to vanilla data-Shapley implementations, like runtime optimization for large datasets and models.\n",
    "\n",
    "Let's start with some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the dataset, we will use the load_spotify_dataset method. Internally, the method will load data on songs published after 2014, use 30% of data for test, and another 30% of the remaining data for validation. Then, the method will return train, validation and test data as lists of the shape [X_input, Y_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from valuation.utils import load_spotify_dataset\n",
    "train_data, val_data, test_data = load_spotify_dataset(val_size=0.3, test_size=0.3, target_column='popularity')\n",
    "train_data[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has many high level feature, some quite intuitive ('duration_ms' or 'tempo'), and other a bit more cryptic ('valence'?). If you want more information on each feature, you can find it on [this webpage](https://www.kaggle.com/datasets/paradisejoy/top-hits-spotify-from-20002019?resource=download).\n",
    "In our analysis, we will use all the columns, excluding 'artist' and 'song', to predict the 'popularity' of each song. We will nonetheless keep the information on song and artist in the train set in a separate pandas Series, for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_name = train_data[0]['song']\n",
    "artist = train_data[0]['artist']\n",
    "train_data[0] = train_data[0].drop(['song', 'artist'], axis=1)\n",
    "test_data[0] = test_data[0].drop(['song', 'artist'], axis=1)\n",
    "val_data[0] = val_data[0].drop(['song', 'artist'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input and label data need to be stored in a Dataset object, a pyDVL specific tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from valuation.utils import Dataset\n",
    "dataset = Dataset(*train_data, *val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation of Shapley coefficients is very computationally expensive because it needs to go through several subsets of the original input dataset before converging. For this reason, the Dval library implements techniques to speed up the calculation, both caching intermediate results and allowing to group data to calculate Shapley values on groups instead of single-datapoints. From a Dataset object, you can just do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from valuation.utils import GroupedDataset\n",
    "grouped_dataset = GroupedDataset.from_dataset(dataset, artist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can finally calculate the contribution of each datapoint to the modle performance!\n",
    "\n",
    "As model, we will use a GradientBoostingRegressor, but any model from sklearn, xgboost and lightgbm works. More precisely, any model that has a fit and predict method should run without issues.\n",
    "\n",
    "Note: Make sure to restart (or simply start if it is not already running) your memcache. See our [documentation](https://appliedai-initiative.github.io/valuation/install.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from valuation.shapley import shapley_pydvl\n",
    "from valuation.utils import Utility\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "utility = Utility(\n",
    "        model=GradientBoostingRegressor(n_estimators=3), data=grouped_dataset, scoring='neg_mean_absolute_error', enable_cache=True,\n",
    "    )\n",
    "dval_df = shapley_pydvl(utility, iterations_per_job=50, num_jobs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the first object that we have defined (utility) holds all the information on the dataset, the model and the scoring methods. Within utility, the dval library uses a cache to speed up calculation of the Shapley coefficients.\n",
    "The second method, shapley_dval, calculates the actual Shapley values and manages parallelization. Let's take a look at the returned dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to notice is that the dval_df dataframe is sorted in ascending order of shapley_dval. The data_key columns holds the labels for each data group: in this case, since the groups correspond to artists, it coincides with artist names. The second column corresponds to the shapley data valuation score, and the third to its (approximate) standard deviation.\n",
    "\n",
    "A better way to analyse the results is through a plot. In the next cell we will take the 30 datapoints with lowest score and plot them with errorbars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from valuation.utils import plot_shapley_pydvl\n",
    "low_dval = dval_df.iloc[:30]\n",
    "fig = plot_shapley_pydvl(low_dval, figsize=(20, 4), title='Artists with low data valuation scores', xlabel='artist', ylabel='shapley_dval')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are a lot of points which give negative shapley score, meaning that they tend to decrease the total score of the model when present in the training set! What happens if we remove it? In the next cell we will create a new training set which excludes the 30 points with lowest scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dval_artists = dval_df.iloc[:30].data_key.to_list()\n",
    "artist_filter = ~artist.isin(low_dval_artists)\n",
    "X_train_good_dval = train_data[0][artist_filter]\n",
    "y_train_good_dval = train_data[0][artist_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use this cleaned dataset to train a full GradientBoostingRegressor and compare its mean absolute error to the model which uses the full dataset. Notice that the score now is calculated using the test set, while for calculating the shapley values we were using the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "full_model = GradientBoostingRegressor(n_estimators=3).fit(X_train_good_dval, y_train_good_dval)\n",
    "mean_absolute_error(full_model.predict(test_data[0]), test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model = GradientBoostingRegressor(n_estimators=3).fit(train_data[0], train_data[1])\n",
    "mean_absolute_error(full_model.predict(test_data[0]), test_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score has improved by more than 15%! This is quite an important result, as it shows a self-consistent process to improve the performance of a model by excluding datapoints from its training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on anomalous data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting test is to corrupt some of the data and monitor how their valuation score changes. To do this, we will take one of the authors with the highest score and set all its popularity to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_dval = dval_df.iloc[-30:]\n",
    "fig = plot_shapley_pydvl(high_dval, figsize=(20, 4), title='Artists with high data valuation scores', xlabel='artist', ylabel='shapley_dval')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can see that Rihanna has one of the highest scores. Let's now take all the train labels related to her, set the score to 0 and re-calculate the data valuation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0].loc[artist == 'Rihanna'] = 0\n",
    "dataset = Dataset(*train_data, *val_data)\n",
    "grouped_dataset = GroupedDataset.from_dataset(dataset, artist)\n",
    "utility = Utility(\n",
    "        model=GradientBoostingRegressor(n_estimators=3), data=grouped_dataset, scoring='neg_mean_absolute_error', enable_cache=True,\n",
    "    )\n",
    "dval_df = shapley_pydvl(utility, iterations_per_job=50, num_jobs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now take the low scoring artists and plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_dval = dval_df.iloc[:30]\n",
    "fig = plot_shapley_pydvl(low_dval, figsize=(20, 4), title='Artists with low data valuation scores', xlabel='artist', ylabel='shapley_dval')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And Rihanna (our anomalous data group) has moved from top contributor to having negative impact on the performance of the model, as expected!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('data_shapley')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e000971326892723e7f31ded70802f690c31c3620f59a0f99e594aaee3047ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
